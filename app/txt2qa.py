import json
from openai import OpenAI
import re
import os
import glob
from typing import List, Dict
import time
from collections import Counter

os.environ["DEEPSEEK_API_KEY"] = "sk-b6118335f5c34520abffbe6fa324257a"

api_key = os.environ.get("DEEPSEEK_API_KEY")

if not api_key:
    raise ValueError("Please set the environment variable DEEPSEEK_API_KEY")

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

import concurrent.futures
from threading import Lock
import time

# æ·»åŠ å…¨å±€å˜é‡ç”¨äºè¿›åº¦è·Ÿè¸ª
qa_progress = {
    "total_files": 0,
    "processed_files": 0,
    "total_questions": 0,
    "generated_questions": 0,
    "lock": Lock()
}

def update_qa_progress(processed_files=0, generated_questions=0):
    """æ›´æ–°é—®ç­”ç”Ÿæˆè¿›åº¦"""
    with qa_progress["lock"]:
        if processed_files:
            qa_progress["processed_files"] += processed_files
        if generated_questions:
            qa_progress["generated_questions"] += generated_questions

def get_qa_progress():
    """è·å–å½“å‰è¿›åº¦"""
    with qa_progress["lock"]:
        return {
            "total_files": qa_progress["total_files"],
            "processed_files": qa_progress["processed_files"],
            "total_questions": qa_progress["total_questions"],
            "generated_questions": qa_progress["generated_questions"]
        }

def reset_qa_progress():
    """é‡ç½®è¿›åº¦"""
    with qa_progress["lock"]:
        qa_progress.update({
            "total_files": 0,
            "processed_files": 0,
            "total_questions": 0,
            "generated_questions": 0
        })

def read_multiple_txt_files(txt_folder: str = "txt_files") -> Dict[str, str]:
    """
    è¯»å–å¤šä¸ªtxtæ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶åå’Œå†…å®¹çš„å­—å…¸
    """
    txt_files = glob.glob(os.path.join(txt_folder, "*.txt"))
    contents = {}
    
    for txt_file in txt_files:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
                file_name = os.path.basename(txt_file)
                contents[file_name] = content
                print(f"âœ… å·²è¯»å–: {file_name} (é•¿åº¦: {len(content)} å­—ç¬¦)")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {txt_file} æ—¶å‡ºé”™: {e}")
    
    return contents

def process_single_file_parallel(args):
    """å¹¶è¡Œå¤„ç†å•ä¸ªæ–‡ä»¶çš„åŒ…è£…å‡½æ•°"""
    file_name, content, num_questions, language, domain_analysis = args
    try:
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}")
        qa_pairs = generate_single_file_questions(content, file_name, num_questions, language, domain_analysis)
        update_qa_progress(processed_files=1, generated_questions=len(qa_pairs))
        print(f"âœ… å®Œæˆæ–‡ä»¶: {file_name}, ç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®é¢˜")
        return qa_pairs
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
        update_qa_progress(processed_files=1)
        return []

def adaptive_question_generation_parallel(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict, max_workers: int = 3) -> list:
    """
    è‡ªé€‚åº”é—®é¢˜ç”Ÿæˆï¼šå¤šçº¿ç¨‹ç‰ˆæœ¬
    """
    reset_qa_progress()
    all_qa_pairs = []
    
    # æ£€æŸ¥å¹¶è¡Œæ•°æ˜¯å¦åˆç†
    if max_workers > len(contents):
        raise ValueError(f"å¹¶è¡Œå¤„ç†æ•° {max_workers} å¤§äºæ–‡ä»¶æ•° {len(contents)}ï¼Œè¯·å‡å°‘å¹¶è¡Œæ•°")
    
    # è®¾ç½®æ€»è¿›åº¦
    with qa_progress["lock"]:
        qa_progress["total_files"] = len(contents)
        qa_progress["total_questions"] = num_questions
    
    if len(contents) == 1:
        # å•æ–‡ä»¶ï¼šç›´æ¥å¤„ç†
        file_name, content = list(contents.items())[0]
        single_qa_pairs = generate_single_file_questions(content, file_name, num_questions, language, domain_analysis)
        all_qa_pairs.extend(single_qa_pairs)
        update_qa_progress(processed_files=1, generated_questions=len(single_qa_pairs))
    else:
        # å¤šæ–‡ä»¶ï¼šå¹¶è¡Œå¤„ç†
        # å•ä¸ªæ–‡ä»¶é—®é¢˜ï¼ˆæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆè¾ƒå°‘é—®é¢˜ï¼‰
        single_questions_per_file = max(2, num_questions // (len(contents) * 2))
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        tasks = []
        for file_name, content in contents.items():
            tasks.append((file_name, content, single_questions_per_file, language, domain_analysis))
        
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(tasks)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_single_file_parallel, task) for task in tasks]
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                try:
                    qa_pairs = future.result()
                    all_qa_pairs.extend(qa_pairs)
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
        
        # ç»¼åˆæ€§é—®é¢˜ï¼ˆä¸²è¡Œå¤„ç†ï¼Œå› ä¸ºéœ€è¦æ‰€æœ‰æ–‡ä»¶å†…å®¹ï¼‰
        cross_questions = max(3, num_questions - len(all_qa_pairs))
        if cross_questions > 0:
            print(f"ğŸ”— ç”Ÿæˆ {cross_questions} ä¸ªç»¼åˆæ€§é—®é¢˜")
            cross_qa_pairs = generate_cross_file_questions(contents, cross_questions, language, domain_analysis)
            all_qa_pairs.extend(cross_qa_pairs)
            update_qa_progress(generated_questions=len(cross_qa_pairs))
    
    return all_qa_pairs[:num_questions]
    
def debug_gpt_output(gpt_output: str) -> str:
    """
    è°ƒè¯•GPTè¾“å‡ºï¼Œè¯¦ç»†åˆ†æé—®é¢˜
    """
    print(f"=== DEBUG INFO ===")
    print(f"åŸå§‹è¾“å‡ºé•¿åº¦: {len(gpt_output)}")
    print(f"å‰200å­—ç¬¦: {gpt_output[:200]}")
    
    # æ£€æŸ¥å¸¸è§é—®é¢˜
    if "error" in gpt_output.lower():
        print("âš ï¸ åŒ…å«é”™è¯¯ä¿¡æ¯")
    if "rate limit" in gpt_output.lower():
        print("âš ï¸ å¯èƒ½è¾¾åˆ°é¢‘ç‡é™åˆ¶")
    if "```" in gpt_output:
        print("âš ï¸ åŒ…å«ä»£ç å—æ ‡è®°")
    
    return gpt_output

def robust_clean_gpt_output(gpt_output: str) -> str:
    """
    æ›´å¥å£®çš„GPTè¾“å‡ºæ¸…ç†
    """
    if not gpt_output:
        print("âŒ è¾“å‡ºä¸ºç©º")
        return ""
    
    original_output = gpt_output
    gpt_output = gpt_output.strip()
    
    print(f"æ¸…ç†å‰é•¿åº¦: {len(gpt_output)}")
    
    # å¤šå±‚æ¸…ç†
    cleaning_steps = [
        # ç§»é™¤ä»£ç å—
        (r'```json\s*', ''),
        (r'```\s*', ''),
        (r'\s*```', ''),
        # ç§»é™¤é¢å¤–çš„ç©ºç™½
        (r'\n+', ' '),
        (r'\s+', ' '),
    ]
    
    for pattern, replacement in cleaning_steps:
        gpt_output = re.sub(pattern, replacement, gpt_output, flags=re.DOTALL | re.IGNORECASE)
    
    gpt_output = gpt_output.strip()
    print(f"æ¸…ç†åé•¿åº¦: {len(gpt_output)}")
    
    return gpt_output

def extract_qa_pairs_from_response(parsed_data) -> list:
    """
    ä»APIå“åº”ä¸­æå–é—®ç­”å¯¹ï¼Œå¤„ç†å¤šç§å¯èƒ½çš„æ ¼å¼
    """
    qa_pairs = []
    
    if isinstance(parsed_data, list):
        # æ ¼å¼1: [{"question": "...", "answer": "..."}]
        for item in parsed_data:
            if isinstance(item, dict):
                if "question" in item and "answer" in item:
                    qa_pairs.append({
                        "question": item["question"],
                        "answer": item["answer"]
                    })
                elif "question_en" in item and "answer_en" in item:
                    qa_pairs.append({
                        "question_en": item["question_en"],
                        "answer_en": item["answer_en"],
                        "question_zh": item.get("question_zh", ""),
                        "answer_zh": item.get("answer_zh", "")
                    })
    
    elif isinstance(parsed_data, dict):
        # æ ¼å¼2: {"questions": [...], "answers": [...]}
        if "questions" in parsed_data and "answers" in parsed_data:
            questions = parsed_data["questions"]
            answers = parsed_data["answers"]
            if isinstance(questions, list) and isinstance(answers, list):
                for i, (q, a) in enumerate(zip(questions, answers)):
                    if isinstance(q, dict) and "question" in q:
                        q_text = q["question"]
                        a_text = a["answer"] if isinstance(a, dict) and "answer" in a else a
                    else:
                        q_text = str(q)
                        a_text = str(a)
                    qa_pairs.append({"question": q_text, "answer": a_text})
        
        # æ ¼å¼3: {"qa_pairs": [{"q": "...", "a": "..."}]}
        elif "qa_pairs" in parsed_data:
            for item in parsed_data["qa_pairs"]:
                if isinstance(item, dict):
                    question = item.get("q") or item.get("question")
                    answer = item.get("a") or item.get("answer")
                    if question and answer:
                        qa_pairs.append({"question": question, "answer": answer})
        
        # æ ¼å¼4: ç›´æ¥åŒ…å«questionå’Œanswerçš„å­—å…¸
        elif "question" in parsed_data and "answer" in parsed_data:
            qa_pairs.append({
                "question": parsed_data["question"],
                "answer": parsed_data["answer"]
            })
    
    return qa_pairs

def _get_system_message(language: str) -> str:
    """è·å–ç³»ç»Ÿæ¶ˆæ¯ - æ›´ä¸¥æ ¼çš„æŒ‡ä»¤"""
    if language == "chinese":
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶äººå‘˜ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºé—®ç­”å¯¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
[{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}, {"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}]
å¿…é¡»è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«questionå’Œanswerå­—æ®µã€‚"""
    elif language == "english":
        return """You are a professional researcher. Please output Q&A pairs in strict JSON format only:
[{"question": "question1", "answer": "answer1"}, {"question": "question2", "answer": "answer2"}]
Return ONLY a JSON array with question and answer fields."""
    else:
        return """You are a bilingual research expert. Output in strict JSON format only:
[{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}]
Return ONLY a JSON array."""

def _call_deepseek_api_with_retry(prompt: str, language: str, max_retries: int = 3) -> list:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨ - ä¿®å¤ç‰ˆ
    """
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
            
            system_message = _get_system_message(language)
            
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt},
                ],
                stream=False,
                temperature=0.3,
                max_tokens=8192,
                response_format={"type": "json_object"}  # å¼ºåˆ¶JSONæ ¼å¼
            )

            gpt_output = response.choices[0].message.content
            
            # è°ƒè¯•è¾“å‡º
            debug_gpt_output(gpt_output)
            
            # æ¸…ç†è¾“å‡º
            cleaned_output = robust_clean_gpt_output(gpt_output)
            
            try:
                parsed_data = json.loads(cleaned_output)
                print(f"âœ… JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                
                # æå–é—®ç­”å¯¹
                qa_pairs = extract_qa_pairs_from_response(parsed_data)
                
                if qa_pairs:
                    print(f"âœ… æˆåŠŸæå– {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                    return qa_pairs
                else:
                    print(f"âŒ æœªæå–åˆ°æœ‰æ•ˆé—®ç­”å¯¹")
                    print(f"è§£æçš„æ•°æ®ç»“æ„: {parsed_data}")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        return []
                        
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"æ¸…ç†åçš„å†…å®¹: {cleaned_output[:500]}...")
                
                if attempt < max_retries - 1:
                    print("ç­‰å¾…åé‡è¯•...")
                    time.sleep(2)
                    continue
                else:
                    return []
                    
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                time.sleep(5)
                continue
            else:
                return []
    
    return []

def extract_paper_keywords(content: str, file_name: str, max_keywords: int = 8) -> List[str]:
    """
    ä»è®ºæ–‡å†…å®¹å’Œæ ‡é¢˜ä¸­æå–æœ‰æ„ä¹‰çš„å­¦æœ¯å…³é”®è¯
    """
    # ä»æ–‡ä»¶åæå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼ˆå»é™¤æ‰©å±•åå’Œå¸¸è§æ— æ„ä¹‰è¯ï¼‰
    file_base = os.path.splitext(file_name)[0]  # å»é™¤.txtæ‰©å±•å
    file_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]{2,}', file_base)
    
    # ä»å†…å®¹æå–é«˜é¢‘è¯ï¼Œä½†è¿›è¡Œæ›´ä¸¥æ ¼çš„è¿‡æ»¤
    content_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]{3,}', content.lower()[:5000])  # åªåˆ†æå‰5000å­—ç¬¦
    
    # æ‰©å±•åœç”¨è¯åˆ—è¡¨
    stop_words = {
        # è‹±æ–‡åœç”¨è¯
        'the', 'and', 'for', 'with', 'this', 'that', 'from', 'have', 'has', 'were', 'are', 'was', 'been', 'being',
        'which', 'what', 'when', 'where', 'why', 'how', 'who', 'whom', 'whose', 'will', 'would', 'could', 'should',
        'may', 'might', 'must', 'can', 'cannot', 'able', 'about', 'above', 'after', 'again', 'against', 'all', 'any',
        'because', 'before', 'below', 'between', 'both', 'but', 'by', 'during', 'each', 'few', 'more', 'most', 'other',
        'some', 'such', 'than', 'then', 'there', 'these', 'they', 'those', 'through', 'until', 'very', 'while', 'within',
        'without', 'based', 'using', 'study', 'research', 'paper', 'article', 'analysis', 'method', 'result', 'conclusion',
        'introduction', 'abstract', 'background', 'objective', 'purpose', 'aim', 'goal', 'find', 'found', 'show', 'showed',
        'demonstrate', 'demonstrated', 'indicate', 'indicated', 'suggest', 'suggested', 'reveal', 'revealed', 'provide',
        'provided', 'present', 'presented', 'discuss', 'discussed', 'explain', 'explained', 'describe', 'described',
        'examine', 'examined', 'investigate', 'investigated', 'explore', 'explored', 'assess', 'assessed', 'evaluate',
        'evaluated', 'measure', 'measured', 'test', 'tested', 'model', 'models', 'data', 'dataset', 'sample', 'samples',
        
        # ä¸­æ–‡åœç”¨è¯
        'çš„', 'åœ¨', 'æ˜¯', 'äº†', 'å’Œ', 'ä¸', 'åŠ', 'ç­‰', 'å…¶ä¸­', 'é€šè¿‡', 'åŸºäº', 'ä½¿ç”¨', 'é‡‡ç”¨', 'è¿›è¡Œ', 'å…·æœ‰', 'åŒ…æ‹¬',
        'åŒ…å«', 'æ¶‰åŠ', 'å…³äº', 'å¯¹äº', 'å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'è™½ç„¶', 'å°½ç®¡', 'å¦‚æœ', 'é‚£ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥',
        'æœ¬æ–‡', 'æœ¬ç ”ç©¶', 'æˆ‘ä»¬', 'ä½œè€…', 'è®ºæ–‡', 'æ–‡ç« ', 'ç ”ç©¶', 'åˆ†æ', 'æ–¹æ³•', 'ç»“æœ', 'ç»“è®º', 'å¼•è¨€', 'æ‘˜è¦', 'èƒŒæ™¯',
        'ç›®çš„', 'ç›®æ ‡', 'å‘ç°', 'è¡¨æ˜', 'è¯æ˜', 'æ˜¾ç¤º', 'æ­ç¤º', 'æä¾›', 'æå‡º', 'è®¨è®º', 'è§£é‡Š', 'æè¿°', 'è€ƒå¯Ÿ', 'è°ƒæŸ¥',
        'æ¢è®¨', 'è¯„ä¼°', 'æµ‹é‡', 'æµ‹è¯•', 'æ¨¡å‹', 'æ•°æ®', 'æ ·æœ¬', 'èµ„æ–™'
    }
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    meaningful_words = [
        word for word in content_words 
        if (word not in stop_words and 
            len(word) > 2 and 
            not word.isdigit() and  # æ’é™¤çº¯æ•°å­—
            not re.match(r'^[0-9\.]+$', word))  # æ’é™¤æ•°å­—å’Œç‚¹ç»„æˆçš„å­—ç¬¦ä¸²
    ]
    
    # ç»Ÿè®¡è¯é¢‘ï¼Œä½†ç»™äºˆæ ‡é¢˜è¯æ›´é«˜æƒé‡
    word_freq = Counter(meaningful_words)
    
    # ç»™æ–‡ä»¶åä¸­çš„è¯å¢åŠ æƒé‡
    for file_word in file_words:
        if file_word.lower() not in stop_words and len(file_word) > 2:
            word_freq[file_word.lower()] += 5  # ç»™æ ‡é¢˜è¯æ›´é«˜æƒé‡
    
    # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„å…³é”®è¯
    top_keywords = []
    for word, freq in word_freq.most_common(20):  # å…ˆå–å‰20ä¸ª
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šæ’é™¤å¤ªå¸¸è§çš„å­¦æœ¯è¯æ±‡
        common_academic_words = {
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact',
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact',
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact'
        }
        if word not in common_academic_words:
            top_keywords.append(word)
    
    return top_keywords[:max_keywords]

def analyze_research_domains(contents: Dict[str, str]) -> Dict:
    """
    ç»¼åˆåˆ†ææ‰€æœ‰æ–‡ä»¶çš„ç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜
    """
    print("ğŸ” æ­£åœ¨åˆ†æç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜...")
    
    all_domains = []
    all_themes = []
    all_keywords = []
    
    for file_name, content in contents.items():
        content_lower = content.lower()
        
        # é¢†åŸŸæ˜ å°„ - æ›´ç²¾ç¡®çš„åŒ¹é…
        domain_mapping = {
            'ç»æµå­¦': ['economic', 'economy', 'gdp', 'market', 'financial', 'investment', 'price', 'cost', 'income', 'revenue', 'profit', 'trade'],
            'æ°”å€™ç§‘å­¦': ['climate', 'temperature', 'weather', 'precipitation', 'emission', 'carbon', 'warming', 'greenhouse', 'atmospheric'],
            'å†œä¸š': ['agriculture', 'crop', 'farm', 'food', 'yield', 'harvest', 'rural', 'farming', 'irrigation', 'fertilizer'],
            'ç¯å¢ƒç§‘å­¦': ['environment', 'pollution', 'sustainability', 'ecology', 'conservation', 'ecosystem', 'biodiversity', 'environmental'],
            'é£é™©ç®¡ç†': ['risk', 'management', 'mitigation', 'uncertainty', 'vulnerability', 'resilience', 'exposure', 'hazard'],
            'æ”¿ç­–åˆ†æ': ['policy', 'regulation', 'governance', 'intervention', 'strategy', 'measure', 'implementation', 'enforcement']
        }
        
        # ä¸»é¢˜æ˜ å°„ - æ›´ç²¾ç¡®çš„åŒ¹é…
        theme_mapping = {
            'å½±å“è¯„ä¼°': ['impact', 'effect', 'evaluation', 'assessment', 'consequence', 'outcome', 'result'],
            'æœºåˆ¶åˆ†æ': ['mechanism', 'pathway', 'channel', 'transmission', 'causal', 'causality', 'mediation'],
            'å®è¯ç ”ç©¶': ['empirical', 'evidence', 'data analysis', 'statistical', 'regression', 'estimation', 'empirically'],
            'æ”¿ç­–å»ºè®®': ['policy', 'recommendation', 'suggestion', 'implication', 'application', 'recommend', 'suggest'],
            'æ¨¡å‹æ„å»º': ['model', 'framework', 'theoretical', 'conceptual', 'simulation', 'modeling', 'theoretical']
        }
        
        # è¯†åˆ«é¢†åŸŸ - è¦æ±‚è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
        for domain, keywords in domain_mapping.items():
            match_count = sum(1 for keyword in keywords if keyword in content_lower)
            if match_count >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯æ‰è®¤ä¸ºæ˜¯è¯¥é¢†åŸŸ
                all_domains.append(domain)
        
        # è¯†åˆ«ä¸»é¢˜ - è¦æ±‚è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
        for theme, keywords in theme_mapping.items():
            match_count = sum(1 for keyword in keywords if keyword in content_lower)
            if match_count >= 2:
                all_themes.append(theme)
        
        # æ”¶é›†å…³é”®è¯
        keywords = extract_paper_keywords(content, file_name)
        all_keywords.extend(keywords)
    
    # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°é¢†åŸŸï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not all_domains:
        all_domains = ['å­¦æœ¯ç ”ç©¶']
    
    # ç¡®å®šä¸»è¦é¢†åŸŸå’Œä¸»é¢˜
    domain_counter = Counter(all_domains)
    theme_counter = Counter(all_themes)
    keyword_counter = Counter(all_keywords)
    
    primary_domains = [domain for domain, count in domain_counter.most_common(2)]
    primary_themes = [theme for theme, count in theme_counter.most_common(3)] if theme_counter else ['ç»¼åˆç ”ç©¶']
    top_keywords = [keyword for keyword, count in keyword_counter.most_common(10)]
    
    return {
        'primary_domains': primary_domains,
        'primary_themes': primary_themes,
        'top_keywords': top_keywords,
        'file_count': len(contents)
    }

def generate_single_domain_prompt(content: str, file_name: str, num_questions: int, language: str, domain_analysis: Dict) -> str:
    """
    ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆé¢†åŸŸæ·±åº¦é—®é¢˜çš„æç¤ºè¯ - ä¿®å¤ç‰ˆ
    """
    domains_text = 'ã€'.join(domain_analysis['primary_domains'])
    themes_text = 'ã€'.join(domain_analysis['primary_themes'])
    
    if language == "chinese":
        prompt_content = f"""
è¯·åŸºäºä»¥ä¸‹{domains_text}é¢†åŸŸçš„ç ”ç©¶å†…å®¹ï¼Œç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰å­¦æœ¯æ·±åº¦çš„ä¸“ä¸šé—®é¢˜ã€‚

ç ”ç©¶é¢†åŸŸï¼š{domains_text}
æ ¸å¿ƒä¸»é¢˜ï¼š{themes_text}
å…³é”®è¯ï¼š{', '.join(domain_analysis['top_keywords'][:5])}

ç ”ç©¶å†…å®¹æ‘˜è¦ï¼š
{content[:4000]}

è¯·ç”Ÿæˆæ¶µç›–ä»¥ä¸‹æ·±åº¦å­¦æœ¯ç»´åº¦çš„é—®é¢˜ï¼š
1. ç†è®ºæœºåˆ¶æ¢è®¨ï¼šåˆ†ææ ¸å¿ƒç†è®ºæ¡†æ¶å’Œå› æœæœºåˆ¶
2. æ–¹æ³•è®ºæ‰¹åˆ¤ï¼šè¯„ä»·ç ”ç©¶æ–¹æ³•çš„ç§‘å­¦æ€§å’Œåˆ›æ–°æ€§
3. å®è¯å‘ç°è§£è¯»ï¼šæ·±å…¥è§£è¯»æ•°æ®å‘ç°çš„ç†è®ºæ„ä¹‰
4. æ”¿ç­–å®è·µå…³è”ï¼šæ¢è®¨ç ”ç©¶æˆæœçš„æ”¿ç­–å«ä¹‰å’Œå®è·µä»·å€¼
5. å­¦ç§‘å‰æ²¿å®šä½ï¼šåˆ†æè¯¥ç ”ç©¶åœ¨é¢†åŸŸå‘å±•ä¸­çš„ä½ç½®
6. æœªæ¥ç ”ç©¶æ–¹å‘ï¼šæå‡ºæœ‰è§åœ°çš„åç»­ç ”ç©¶å»ºè®®

é‡è¦è¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šå­¦æœ¯è¯­è¨€ï¼Œé¿å…æåŠå…·ä½“ä½œè€…å§“å
- é¿å…ä½¿ç”¨"æœ¬æ–‡"ã€"æœ¬ç ”ç©¶"ã€"åœ¨XXXçš„ç ”ç©¶ä¸­"ç­‰è¡¨è¿°
- é—®é¢˜è¦ä½“ç°{domains_text}é¢†åŸŸçš„ä¸“ä¸šæ·±åº¦å’Œæ™®é€‚æ€§
- å¼ºè°ƒç†è®ºåˆ†æã€æœºåˆ¶æ¢è®¨å’Œæ‰¹åˆ¤æ€§æ€è€ƒ
- ç­”æ¡ˆåº”å±•ç¤ºä¸¥è°¨çš„å­¦æœ¯æ¨ç†è¿‡ç¨‹ï¼Œä¸ä¾èµ–ç‰¹å®šç ”ç©¶
- é¿å…ç®€å•çš„äº‹å®é™ˆè¿°ï¼Œæ³¨é‡è§£é‡Šå’Œåˆ†æ

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
[{{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}}, {{"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}}]

ç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰æ™®é€‚æ€§çš„æ·±åº¦å­¦æœ¯é—®ç­”å¯¹ï¼š
"""
    elif language == "english":
        prompt_content = f"""
Please generate {num_questions} academically profound questions based on the following research content in the {domains_text} field.

Research Domains: {domains_text}
Core Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:5])}

Content Summary:
{content[:4000]}

Generate questions covering these deep academic dimensions:
1. Theoretical mechanisms: Analyze core theoretical frameworks and causal pathways
2. Methodological critique: Evaluate scientific rigor and innovation of research approaches
3. Empirical findings interpretation: Deeply interpret theoretical significance of data discoveries
4. Policy-practice connections: Discuss policy implications and practical applications
5. Disciplinary positioning: Analyze the research's contribution to field development
6. Future research directions: Propose insightful suggestions for subsequent investigations

CRITICAL REQUIREMENTS:
- Use professional academic discourse, avoid mentioning specific author names
- Refrain from using expressions like "this paper", "this study", "in XXX's research"
- Questions should demonstrate both professional depth in {domains_text} and universal applicability
- Emphasize theoretical analysis, mechanism exploration, and critical thinking
- Answers should exhibit rigorous academic reasoning processes, not relying on specific studies
- Avoid simple factual statements, focus on explanatory and analytical depth

Output in strict JSON format:
[{{"question": "question1", "answer": "answer1"}}, {{"question": "question2", "answer": "answer2"}}]

Generate {num_questions} universally applicable deep academic Q&A pairs:
"""
    else:
        prompt_content = f"""
Please generate {num_questions} bilingual deep academic questions based on this {domains_text} research.

Domains: {domains_text}
Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:5])}

Content:
{content[:4000]}

Generate profound questions covering:
- Theoretical frameworks and causal mechanisms
- Methodological rigor and innovation
- Interpretation of empirical findings
- Policy implications and practical applications
- Positioning in disciplinary development
- Future research trajectories

CRITICAL REQUIREMENTS:
- Generate both English and Chinese versions
- Use professional academic discourse, avoid author-specific references
- Refrain from "this paper/study" expressions in both languages
- Demonstrate deep domain expertise with universal applicability
- Show analytical reasoning in answers, not study-specific facts
- Focus on explanatory depth rather than simple statements

Output in strict JSON format:
[{{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}}]

Generate {num_questions} bilingual universally applicable academic pairs:
"""
    
    return prompt_content

def generate_cross_domain_prompt(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> str:
    """
    ä¸ºå¤šä¸ªæ–‡ä»¶ç”Ÿæˆè·¨é¢†åŸŸæ·±åº¦é—®é¢˜çš„æç¤ºè¯ - è‹±æ–‡ç‰ˆ
    """
    domains_text = 'ã€'.join(domain_analysis['primary_domains'])
    themes_text = 'ã€'.join(domain_analysis['primary_themes'])
    
    # æ„å»ºåˆå¹¶å†…å®¹
    combined_content = ""
    for i, (file_name, content) in enumerate(contents.items(), 1):
        combined_content += f"Research Material {i}:\n{content[:2000]}\n\n"
    
    if language == "chinese":
        prompt_content = f"""
åŸºäºä»¥ä¸‹å¤šä»½{domains_text}é¢†åŸŸçš„ç ”ç©¶èµ„æ–™ï¼Œç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰å­¦æœ¯æ·±åº¦çš„ç»¼åˆæ€§é—®é¢˜ã€‚

ç ”ç©¶é¢†åŸŸï¼š{domains_text}
æ ¸å¿ƒä¸»é¢˜ï¼š{themes_text}
å…³é”®è¯ï¼š{', '.join(domain_analysis['top_keywords'][:8])}
ç ”ç©¶èµ„æ–™æ•°é‡ï¼š{domain_analysis['file_count']}ä»½

ç ”ç©¶èµ„æ–™æ‘˜è¦ï¼š
{combined_content[:6000]}

è¯·ç”Ÿæˆæ¶µç›–ä»¥ä¸‹ç»¼åˆæ€§å­¦æœ¯ç»´åº¦çš„é—®é¢˜ï¼š
1. ç†è®ºæ•´åˆæ¢è®¨ï¼šåˆ†æä¸åŒç ”ç©¶åœ¨ç†è®ºæ¡†æ¶ä¸Šçš„å…±æ€§ä¸å·®å¼‚
2. æ–¹æ³•è®ºæ¯”è¾ƒåæ€ï¼šç»¼åˆè¯„ä»·å„ç§ç ”ç©¶æ–¹æ³•çš„ä¼˜åŠ£å’Œé€‚ç”¨æ€§
3. å®è¯è¯æ®æ±‡èšï¼šæ•´åˆå¤šä¸ªç ”ç©¶çš„å‘ç°ï¼Œæ¢è®¨å…¶ç†è®ºå«ä¹‰
4. æ”¿ç­–å®è·µç»¼åˆï¼šåŸºäºå¤šæºè¯æ®æå‡ºç»¼åˆæ€§çš„æ”¿ç­–å»ºè®®
5. å­¦ç§‘å‘å±•ç ”åˆ¤ï¼šåˆ†æè¯¥é¢†åŸŸç ”ç©¶çš„ç°çŠ¶ã€æŒ‘æˆ˜å’Œå‰æ²¿æ–¹å‘
6. è·¨é¢†åŸŸå¯ç¤ºï¼šæ¢è®¨ç ”ç©¶æˆæœå¯¹å…¶ä»–ç›¸å…³é¢†åŸŸçš„å€Ÿé‰´æ„ä¹‰

é‡è¦è¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šå­¦æœ¯è¯­è¨€ï¼Œé¿å…æåŠå…·ä½“çš„ç ”ç©¶èµ„æ–™ç¼–å·å’Œä½œè€…å§“å
- é—®é¢˜è¦ä½“ç°{domains_text}é¢†åŸŸçš„ç»¼åˆæ€§å’Œæ·±åº¦
- å¼ºè°ƒç†è®ºæ•´åˆã€æ–¹æ³•åæ€å’Œå‰æ²¿å±•æœ›
- ç­”æ¡ˆåº”åŸºäºå¤šä»½ç ”ç©¶èµ„æ–™è¿›è¡Œç»¼åˆæ¨ç†
- é¿å…ç®€å•çš„æ¯”è¾ƒé™ˆè¿°ï¼Œæ³¨é‡æ·±åº¦åˆ†æå’Œç»¼åˆåˆ¤æ–­

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
[{{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}}, {{"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}}]

ç”Ÿæˆ{num_questions}ä¸ªç»¼åˆæ€§æ·±åº¦å­¦æœ¯é—®ç­”å¯¹ï¼š
"""
    elif language == "english":
        prompt_content = f"""
Based on the following multiple research materials in the {domains_text} field, generate {num_questions} comprehensive and profound academic questions.

Research Domains: {domains_text}
Core Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:8])}
Number of Research Materials: {domain_analysis['file_count']}

Research Materials Summary:
{combined_content[:6000]}

Generate questions covering these comprehensive academic dimensions:
1. Theoretical integration: Analyze commonalities and differences in theoretical frameworks across studies
2. Methodological reflection: Comprehensively evaluate strengths and applicability of various research approaches
3. Empirical evidence synthesis: Integrate findings from multiple studies to explore theoretical implications
4. Policy-practice integration: Propose comprehensive policy recommendations based on multiple evidence sources
5. Disciplinary development assessment: Analyze current status, challenges, and frontiers in the field
6. Cross-domain implications: Discuss referential significance of research findings to other related fields

CRITICAL REQUIREMENTS:
- Use professional academic discourse, avoid mentioning specific material numbers or author names
- Questions should demonstrate both comprehensiveness and depth in {domains_text}
- Emphasize theoretical integration, methodological reflection, and frontier outlook
- Answers should be based on comprehensive reasoning from multiple research materials
- Avoid simple comparative statements, focus on deep analysis and synthetic judgment
- Maintain universal applicability while demonstrating domain-specific expertise

Output in strict JSON format:
[{{"question": "question1", "answer": "answer1"}}, {{"question": "question2", "answer": "answer2"}}]

Generate {num_questions} comprehensive deep academic Q&A pairs with universal relevance:
"""
    else:
        prompt_content = f"""
Based on multiple research materials in {domains_text}, generate {num_questions} bilingual comprehensive academic questions.

Domains: {domains_text}
Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:8])}
Materials: {domain_analysis['file_count']}

Content Summary:
{combined_content[:6000]}

Generate comprehensive questions covering:
- Theoretical integration across studies
- Methodological evaluation and reflection
- Synthesis of empirical evidence
- Integrated policy recommendations
- Assessment of disciplinary development
- Cross-domain implications and insights

CRITICAL REQUIREMENTS:
- Generate both English and Chinese versions
- Use professional integrated academic discourse
- Avoid specific references to materials or authors
- Demonstrate comprehensive domain expertise with universal relevance
- Show synthetic reasoning in answers across multiple sources
- Focus on analytical depth rather than descriptive comparisons

Output in strict JSON format:
[{{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}}]

Generate {num_questions} bilingual comprehensive academic pairs with cross-study relevance:
"""
    
    return prompt_content

def generate_single_file_questions(content: str, file_name: str, num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆæ·±åº¦å­¦æœ¯é—®é¢˜
    """
    print(f"ğŸ“– ä¸º {file_name} ç”Ÿæˆ {num_questions} ä¸ªæ·±åº¦å­¦æœ¯é—®é¢˜")
    
    # ç”Ÿæˆå•ä¸ªæ–‡ä»¶çš„æç¤ºè¯
    prompt = generate_single_domain_prompt(content, file_name, num_questions, language, domain_analysis)
    
    qa_pairs = _call_deepseek_api_with_retry(prompt, language)
    
    if qa_pairs:
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªæ·±åº¦å­¦æœ¯é—®ç­”å¯¹")
        return qa_pairs[:num_questions]
    else:
        print("âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥")
        return []

def generate_cross_file_questions(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    ä¸ºå¤šä¸ªæ–‡ä»¶ç”Ÿæˆç»¼åˆæ€§æ·±åº¦é—®é¢˜
    """
    print(f"ğŸ”— åŸºäº {len(contents)} ä¸ªæ–‡ä»¶ç”Ÿæˆ {num_questions} ä¸ªç»¼åˆæ€§æ·±åº¦é—®é¢˜")
    
    # ç”Ÿæˆè·¨æ–‡ä»¶æç¤ºè¯
    prompt = generate_cross_domain_prompt(contents, num_questions, language, domain_analysis)
    
    qa_pairs = _call_deepseek_api_with_retry(prompt, language)
    
    if qa_pairs:
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªç»¼åˆæ€§æ·±åº¦é—®ç­”å¯¹")
        return qa_pairs[:num_questions]
    else:
        print("âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥")
        return []

def adaptive_question_generation(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    è‡ªé€‚åº”é—®é¢˜ç”Ÿæˆï¼šæ ¹æ®æ–‡ä»¶æ•°é‡é€‰æ‹©ç”Ÿæˆç­–ç•¥
    """
    all_qa_pairs = []
    
    if len(contents) == 1:
        # å•æ–‡ä»¶ï¼šåªç”Ÿæˆæ·±åº¦é—®é¢˜
        file_name, content = list(contents.items())[0]
        single_qa_pairs = generate_single_file_questions(content, file_name, num_questions, language, domain_analysis)
        all_qa_pairs.extend(single_qa_pairs)
    else:
        # å¤šæ–‡ä»¶ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ·±åº¦é—®é¢˜ + ç»¼åˆæ€§é—®é¢˜
        # å•ä¸ªæ–‡ä»¶é—®é¢˜ï¼ˆæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆè¾ƒå°‘é—®é¢˜ï¼‰
        single_questions_per_file = max(2, num_questions // (len(contents) * 2))
        for file_name, content in contents.items():
            single_qa_pairs = generate_single_file_questions(content, file_name, single_questions_per_file, language, domain_analysis)
            all_qa_pairs.extend(single_qa_pairs)
        
        # ç»¼åˆæ€§é—®é¢˜
        cross_questions = max(3, num_questions - len(all_qa_pairs))
        if cross_questions > 0:
            cross_qa_pairs = generate_cross_file_questions(contents, cross_questions, language, domain_analysis)
            all_qa_pairs.extend(cross_qa_pairs)
    
    return all_qa_pairs[:num_questions]

def save_qa_dataset(qa_pairs: list, output_file: str, instruction: str, language: str = "both"):
    """
    ä¿å­˜é—®ç­”æ•°æ®é›†
    """
    total_records = 0
    with open(output_file, "w", encoding="utf-8") as f_out:
        for pair in qa_pairs:
            metadata = pair.get("metadata", {})
            
            if language == "both":
                # åŒè¯­ç‰ˆæœ¬
                if all(key in pair for key in ["question_en", "answer_en", "question_zh", "answer_zh"]):
                    # è‹±æ–‡ç‰ˆæœ¬
                    output_record_en = {
                        "instruction": instruction,
                        "input": pair["question_en"],
                        "output": pair["answer_en"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record_en, ensure_ascii=False) + "\n")
                    
                    # ä¸­æ–‡ç‰ˆæœ¬
                    output_record_zh = {
                        "instruction": instruction,
                        "input": pair["question_zh"],
                        "output": pair["answer_zh"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record_zh, ensure_ascii=False) + "\n")
                    total_records += 2
            
            elif language in ["english", "chinese"]:
                if "question" in pair and "answer" in pair:
                    output_record = {
                        "instruction": instruction,
                        "input": pair["question"],
                        "output": pair["answer"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record, ensure_ascii=False) + "\n")
                    total_records += 1
    
    return total_records

def main():
    """
    ä¸»ç¨‹åº - æ·±åº¦å­¦æœ¯é—®é¢˜ç”Ÿæˆå™¨
    """
    print("=== æ·±åº¦å­¦æœ¯é—®é¢˜ç”Ÿæˆå™¨ ===")
    print("âœ¨ æ ¸å¿ƒç‰¹ç‚¹ï¼š")
    print("   - ç”¨æˆ·è‡ªå®šä¹‰ä¸“å®¶è§’è‰²")
    print("   - æ·±åº¦é¢†åŸŸä¸“ä¸šé—®é¢˜")
    print("   - ç»¼åˆæ€§å­¦æœ¯æ¨ç†")
    print("   - ä¸“ä¸šå­¦æœ¯è¯­è¨€è¡¨è¾¾")
    print("   - âš¡ å¹¶è¡Œå¤„ç†åŠ é€Ÿ")
    
    # é…ç½®
    txt_folder = input("è¯·è¾“å…¥txtæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: txt_files): ").strip() or "txt_files"
    
    # ç”¨æˆ·è¾“å…¥ä¸“å®¶è§’è‰²
    instruction = input("è¯·è¾“å…¥instructionå†…å®¹ (ä¾‹å¦‚: 'ä½ æ˜¯ä¸€ä½ç»æµå­¦ä¸“å®¶'): ").strip()
    if not instruction:
        instruction = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶äººå‘˜"
    
    language_choice = input("é€‰æ‹©è¯­è¨€ç‰ˆæœ¬ (1=è‹±æ–‡, 2=ä¸­æ–‡, 3=ä¸­è‹±åŒè¯­): ").strip()
    if language_choice == "1":
        language = "english"
        output_file = "qa_academic_english.jsonl"
    elif language_choice == "2":
        language = "chinese"
        output_file = "qa_academic_chinese.jsonl"
    else:
        language = "both"
        output_file = "qa_academic_bilingual.jsonl"
    
    num_questions = input("è¦ç”Ÿæˆå¤šå°‘ä¸ªé—®é¢˜ï¼Ÿ (é»˜è®¤20): ").strip()
    if not num_questions:
        num_questions = 20
    else:
        num_questions = int(num_questions)
    
    # æ–°å¢ï¼šå¹¶è¡Œå¤„ç†æ•°
    max_workers_input = input("å¹¶è¡Œå¤„ç†æ•° (é»˜è®¤3ï¼Œå»ºè®®ä¸è¶…è¿‡5): ").strip()
    if not max_workers_input:
        max_workers = 3
    else:
        max_workers = int(max_workers_input)
    
    # è¯»å–æ–‡ä»¶
    print("\næ­£åœ¨è¯»å–æ–‡ä»¶...")
    contents = read_multiple_txt_files(txt_folder)
    
    if not contents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å¤¹è·¯å¾„")
        return
    
    # æ£€æŸ¥å¹¶è¡Œæ•°æ˜¯å¦åˆç†
    if max_workers > len(contents):
        print(f"âŒ é”™è¯¯ï¼šå¹¶è¡Œå¤„ç†æ•° {max_workers} å¤§äºæ–‡ä»¶æ•° {len(contents)}")
        print("è¯·å‡å°‘å¹¶è¡Œæ•°æˆ–å¢åŠ æ–‡ä»¶æ•°é‡")
        return
    
    # åˆ†æç ”ç©¶é¢†åŸŸ
    domain_analysis = analyze_research_domains(contents)
    
    print(f"\nğŸ“Š ç»¼åˆåˆ†æç»“æœ:")
    print(f"   ä¸»è¦é¢†åŸŸ: {', '.join(domain_analysis['primary_domains'])}")
    print(f"   æ ¸å¿ƒä¸»é¢˜: {', '.join(domain_analysis['primary_themes'])}")
    print(f"   å…³é”®è¯: {', '.join(domain_analysis['top_keywords'][:8])}")
    print(f"   æ–‡ä»¶æ•°é‡: {domain_analysis['file_count']}")
    print(f"   å¹¶è¡Œå¤„ç†æ•°: {max_workers}")
    
    # ç”Ÿæˆé—®é¢˜ï¼ˆä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬ï¼‰
    print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œç”Ÿæˆæ·±åº¦å­¦æœ¯é—®é¢˜...")
    qa_pairs = adaptive_question_generation_parallel(contents, num_questions, language, domain_analysis, max_workers)
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆ!")
    print(f"   æˆåŠŸç”Ÿæˆ: {len(qa_pairs)} ä¸ªæ·±åº¦å­¦æœ¯é—®ç­”å¯¹")
    
    # ä¿å­˜ç»“æœ
    total_records = save_qa_dataset(qa_pairs, output_file, instruction, language)
    print(f"   ä¿å­˜åˆ°: {output_file}")
    print(f"   æ€»è®°å½•æ•°: {total_records} æ¡")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if qa_pairs:
        print(f"\nğŸ“ ç¤ºä¾‹æ·±åº¦å­¦æœ¯é—®é¢˜:")
        print("-" * 60)
        for i, pair in enumerate(qa_pairs[:3]):
            if language == "both":
                print(f"{i+1}. [EN] {pair.get('question_en', 'N/A')}")
                print(f"    [ZH] {pair.get('question_zh', 'N/A')}")
            else:
                print(f"{i+1}. {pair.get('question', 'N/A')}")
            print()

if __name__ == "__main__":
    main()