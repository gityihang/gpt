import gradio as gr
from pdf2txt import converter
import os
from pathlib import Path
import time
import uuid
import shutil
import json
import subprocess
import requests
from txt2qa import (
    main as qa_main, 
    read_multiple_txt_files, 
    analyze_research_domains, 
    adaptive_question_generation, 
    save_qa_dataset,
    adaptive_question_generation_parallel,
    get_qa_progress,
    reset_qa_progress
)

# å®šä¹‰ç»Ÿä¸€çš„è·¯å¾„å¸¸é‡
BASE_DIR = Path("app")  # appç›®å½•
APPDATA_PATH = BASE_DIR
APPDATA_PATH_JSON = BASE_DIR / "train" / "data"
PDF_FOLDER = BASE_DIR / "pdf"
TXT_FOLDER = BASE_DIR / "txt"
JSONL_FOLDER = APPDATA_PATH_JSON / "jsonl"
JSONL_TRAIN_PATH = JSONL_FOLDER / "train.jsonl"
TRAIN_SCRIPT_PATH = BASE_DIR / "train" / "run.sh"

# æ¨ç†ç›¸å…³å¸¸é‡
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
MODEL_NAME = "deepseek-chat"
API_URL = "http://localhost:11434/v1/chat/completions"

def ensure_directories():
    """ç¡®ä¿æ‰€æœ‰æ‰€éœ€ç›®å½•å­˜åœ¨"""
    try:
        PDF_FOLDER.mkdir(parents=True, exist_ok=True)
        TXT_FOLDER.mkdir(parents=True, exist_ok=True)
        JSONL_FOLDER.mkdir(parents=True, exist_ok=True)
        TRAIN_SCRIPT_PATH.parent.mkdir(parents=True, exist_ok=True)
        print(f"æ‰€æœ‰ç›®å½•å·²åˆ›å»ºåœ¨: {APPDATA_PATH}")
        return True
    except Exception as e:
        print(f"åˆ›å»ºç›®å½•å¤±è´¥: {e}")
        return False

def cleanup_folders():
    """æ¸…ç†æ‰€æœ‰æ–‡ä»¶å¤¹"""
    try:
        # æ¸…ç†pdfæ–‡ä»¶å¤¹
        if PDF_FOLDER.exists():
            for file in PDF_FOLDER.glob("*"):
                try:
                    if file.is_file():
                        file.unlink()
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶ {file} å¤±è´¥: {e}")
        
        # æ¸…ç†txtæ–‡ä»¶å¤¹
        if TXT_FOLDER.exists():
            for file in TXT_FOLDER.glob("*"):
                try:
                    if file.is_file():
                        file.unlink()
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶ {file} å¤±è´¥: {e}")
        
        print("æ–‡ä»¶å¤¹æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"æ¸…ç†æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")

def cleanup_jsonl_folder():
    """æ¸…ç†jsonlæ–‡ä»¶å¤¹"""
    try:
        if JSONL_FOLDER.exists():
            for file in JSONL_FOLDER.glob("*"):
                try:
                    if file.is_file():
                        file.unlink()
                except Exception as e:
                    print(f"åˆ é™¤æ–‡ä»¶ {file} å¤±è´¥: {e}")
        print("JSONLæ–‡ä»¶å¤¹æ¸…ç†å®Œæˆ")
        return True
    except Exception as e:
        print(f"æ¸…ç†JSONLæ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
        return False

def create_pdf_converter_tab():
    """åˆ›å»ºPDFè½¬æ¢é¡µé¢"""
    
    with gr.Blocks() as tab:
        gr.Markdown("""
        # ğŸ“„ GPTä½ç§©è®­ç»ƒ&æ¨ç†
        
        ä½¿ç”¨AIæŠ€æœ¯æ™ºèƒ½å¤„ç†PDFæ–‡æ¡£ï¼Œæå–è®ºæ–‡æ ‡é¢˜ã€æ¸…ç†æ ¼å¼å¹¶è½¬æ¢æ•°å­¦å…¬å¼ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. ä¸Šä¼ PDF")
                pdf_input = gr.File(
                    label="é€‰æ‹©PDFæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
                    file_types=[".pdf"],
                    file_count="multiple",
                    height=100
                )
                
                gr.Markdown("### 2. å¤„ç†é€‰é¡¹")
                use_ai_processing = gr.Checkbox(
                    label="ä½¿ç”¨AIå¢å¼ºå¤„ç†",
                    value=True,
                    info="å¯ç”¨åä¼šè‡ªåŠ¨æ¸…ç†æ ¼å¼ã€æå–æ ‡é¢˜ã€è½¬æ¢æ•°å­¦å…¬å¼"
                )
                
                max_workers = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    label="æœ€å¤§å¹¶è¡Œå¤„ç†æ•°",
                    info="åŒæ—¶å¤„ç†çš„PDFæ–‡ä»¶æ•°é‡"
                )
                
                convert_btn = gr.Button(
                    "ğŸš€ å¼€å§‹æ‰¹é‡è½¬æ¢",
                    variant="primary",
                    size="lg"
                )
                
                cleanup_btn = gr.Button(
                    "ğŸ§¹ æ¸…ç†æ–‡ä»¶å¤¹",
                    variant="secondary"
                )
                cleanup_output = gr.Textbox(
                    label="æ¸…ç†ç»“æœ",
                    visible=False
                )
                
                gr.Markdown("""
                ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
                - **AIå¢å¼ºå¤„ç†**: è‡ªåŠ¨æå–è®ºæ–‡æ ‡é¢˜ã€æ¸…ç†æ ¼å¼ã€è½¬æ¢æ•°å­¦å…¬å¼ä¸ºLaTeX
                - **åŸºç¡€å¤„ç†**: ä»…æå–åŸå§‹æ–‡æœ¬å†…å®¹
                - **å¹¶è¡Œå¤„ç†**: å¯åŒæ—¶å¤„ç†å¤šä¸ªPDFæ–‡ä»¶ï¼Œæé«˜æ•ˆç‡
                - æ–‡ä»¶è‡ªåŠ¨ä¿å­˜åˆ°: `%APPDATA%/app/pdf` å’Œ `%APPDATA%/app/txt` æ–‡ä»¶å¤¹
                - æ”¯æŒä¸­è‹±æ–‡æ–‡æ¡£å’Œæ•°å­¦å…¬å¼å¤„ç†
                """)
            
            with gr.Column(scale=2):
                gr.Markdown("### 3. æ‰¹é‡å¤„ç†ç»“æœ")
                
                task_status = gr.Markdown(
                    value="**ç­‰å¾…å¤„ç†ä»»åŠ¡...**",
                    label="ğŸ“Š ä»»åŠ¡çŠ¶æ€"
                )
                
                progress_display = gr.Textbox(
                    label="ğŸ”„ å¤„ç†è¿›åº¦",
                    lines=6,
                    max_lines=10,
                    show_copy_button=True,
                    interactive=False,
                    placeholder="å¤„ç†è¿›åº¦å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                download_section = gr.Group(visible=False)
                with download_section:
                    gr.Markdown("### ğŸ“¥ ä¸‹è½½è½¬æ¢ç»“æœ")
                    txt_download = gr.File(
                        label="ä¸‹è½½æ‰€æœ‰TXTæ–‡ä»¶",
                        file_count="multiple",
                        interactive=False
                    )
        
        with gr.Accordion("ğŸ”§ å®æ—¶å¤„ç†æ—¥å¿—", open=True):
            terminal_output = gr.Textbox(
                label="å®æ—¶æ—¥å¿—",
                lines=10,
                max_lines=15,
                show_copy_button=True,
                interactive=False,
                placeholder="å®æ—¶å¤„ç†æ—¥å¿—å°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                elem_id="terminal-output",
                autoscroll=True
            )
        
        return {
            "component": tab,
            "inputs": {
                "pdf_input": pdf_input,
                "use_ai_processing": use_ai_processing,
                "max_workers": max_workers,
                "convert_btn": convert_btn,
                "cleanup_btn": cleanup_btn
            },
            "outputs": {
                "task_status": task_status,
                "progress_display": progress_display,
                "download_section": download_section,
                "txt_download": txt_download,
                "terminal_output": terminal_output,
                "cleanup_output": cleanup_output
            }
        }

def create_qa_generator_tab():
    """åˆ›å»ºæ™ºèƒ½é—®ç­”ç”Ÿæˆé¡µé¢"""
    
    with gr.Blocks() as tab:
        gr.Markdown("""
        # ğŸ¤– æ™ºèƒ½é—®ç­”ç”Ÿæˆå™¨
        
        åŸºäºè½¬æ¢åçš„TXTæ–‡ä»¶ï¼Œä½¿ç”¨AIæŠ€æœ¯ç”Ÿæˆæ·±åº¦å­¦æœ¯é—®ç­”å¯¹ï¼Œé€‚ç”¨äºæ¨¡å‹è®­ç»ƒå’Œç ”ç©¶åˆ†æã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. è¾“å…¥é…ç½®")
                
                txt_folder_input = gr.Textbox(
                    label="TXTæ–‡ä»¶æ–‡ä»¶å¤¹è·¯å¾„",
                    value=str(TXT_FOLDER),
                    placeholder="è¯·è¾“å…¥åŒ…å«TXTæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„",
                    info=f"é»˜è®¤è¯»å–PDFè½¬æ¢é¡µé¢ç”Ÿæˆçš„TXTæ–‡ä»¶"
                )
                
                instruction_input = gr.Textbox(
                    label="ä¸“å®¶è§’è‰²æŒ‡ä»¤ (Instruction)",
                    value="ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶äººå‘˜",
                    placeholder="ä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä½ç»æµå­¦ä¸“å®¶",
                    info="å®šä¹‰AIçš„ä¸“å®¶è§’è‰²èº«ä»½"
                )
                
                language_radio = gr.Radio(
                    choices=[
                        ("è‹±æ–‡", "english"),
                        ("ä¸­æ–‡", "chinese"), 
                        ("ä¸­è‹±åŒè¯­", "both")
                    ],
                    label="è¾“å‡ºè¯­è¨€",
                    value="both",
                    info="é€‰æ‹©é—®ç­”å¯¹çš„è¯­è¨€ç‰ˆæœ¬"
                )
                
                num_questions_slider = gr.Slider(
                    minimum=5,
                    maximum=100,
                    value=20,
                    step=5,
                    label="ç”Ÿæˆé—®é¢˜æ•°é‡",
                    info="å»ºè®®20-50ä¸ªé—®é¢˜ä»¥è·å¾—æœ€ä½³æ•ˆæœ"
                )
                
                qa_max_workers_slider = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    label="å¹¶è¡Œå¤„ç†æ•°",
                    info="åŒæ—¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼Œå»ºè®®1-5ä¸ª"
                )
                
                gr.Markdown("### 2. è¾“å‡ºé€‰é¡¹")
                clear_jsonl_checkbox = gr.Checkbox(
                    label="ç”Ÿæˆå‰æ¸…ç©ºJSONLæ–‡ä»¶å¤¹",
                    value=True,
                    info="å¯ç”¨åä¼šåœ¨ç”Ÿæˆæ–°æ•°æ®å‰æ¸…ç©ºjsonlæ–‡ä»¶å¤¹"
                )
                
                generate_btn = gr.Button(
                    "ğŸš€ å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹",
                    variant="primary",
                    size="lg"
                )
                
                check_folder_btn = gr.Button(
                    "ğŸ“ æ£€æŸ¥TXTæ–‡ä»¶å¤¹",
                    variant="secondary"
                )
                
                gr.Markdown("""
                ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
                
                **åŠŸèƒ½ç‰¹ç‚¹ï¼š**
                - ğŸ¯ æ·±åº¦å­¦æœ¯é—®é¢˜ç”Ÿæˆ
                - ğŸ” æ™ºèƒ½é¢†åŸŸåˆ†æ  
                - ğŸŒ å¤šè¯­è¨€æ”¯æŒ
                - ğŸ“š ä¸“ä¸šå­¦æœ¯è¯­è¨€
                
                **å¤„ç†æµç¨‹ï¼š**
                1. è¯»å–TXTæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
                2. åˆ†æç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜
                3. ç”Ÿæˆæ·±åº¦å­¦æœ¯é—®ç­”å¯¹
                4. ä¿å­˜ä¸ºJSONLæ ¼å¼åˆ° `%APPDATA%/app/jsonl` æ–‡ä»¶å¤¹
                """)
            
            with gr.Column(scale=2):
                gr.Markdown("### 3. å¤„ç†ç»“æœ")
                
                folder_status = gr.Markdown(
                    value="**è¯·å…ˆæ£€æŸ¥TXTæ–‡ä»¶å¤¹çŠ¶æ€**",
                    label="ğŸ“ æ–‡ä»¶å¤¹çŠ¶æ€"
                )
                
                processing_status = gr.Markdown(
                    value="**ç­‰å¾…å¼€å§‹å¤„ç†...**",
                    label="ğŸ“ˆ å¤„ç†çŠ¶æ€"
                )
                
                progress_display = gr.Textbox(
                    label="ğŸ”„ å¤„ç†è¿›åº¦",
                    lines=8,
                    max_lines=12,
                    show_copy_button=True,
                    interactive=False,
                    placeholder="å¤„ç†è¿›åº¦å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                download_section = gr.Group(visible=False)
                with download_section:
                    gr.Markdown("### ğŸ“¥ ä¸‹è½½ç»“æœ")
                    output_download = gr.File(
                        label="ä¸‹è½½é—®ç­”æ•°æ®é›†",
                        file_count="single",
                        interactive=False
                    )
        
        with gr.Accordion("ğŸ”§ å®æ—¶å¤„ç†æ—¥å¿—", open=True):
            terminal_output = gr.Textbox(
                label="å®æ—¶æ—¥å¿—",
                lines=10,
                max_lines=15,
                interactive=False,
                placeholder="å®æ—¶å¤„ç†æ—¥å¿—å°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                autoscroll=True
            )
        
        return {
            "component": tab,
            "inputs": {
                "txt_folder": txt_folder_input,
                "instruction": instruction_input,
                "language": language_radio,
                "num_questions": num_questions_slider,
                "qa_max_workers": qa_max_workers_slider,
                "clear_jsonl_checkbox": clear_jsonl_checkbox,
                "generate_btn": generate_btn,
                "check_folder_btn": check_folder_btn
            },
            "outputs": {
                "folder_status": folder_status,
                "processing_status": processing_status,
                "progress_display": progress_display,
                "download_section": download_section,
                "output_download": output_download,
                "terminal_output": terminal_output
            }
        }

def create_deepseek_training_tab():
    """åˆ›å»ºDeepSeekè®­ç»ƒé¡µé¢"""
    with gr.Blocks() as tab:
        gr.Markdown("""
        # ğŸš€ DeepSeekæ¨¡å‹è®­ç»ƒ
        
        åŸºäºé—®ç­”æ•°æ®ï¼Œå¯åŠ¨DeepSeekæ¨¡å‹è®­ç»ƒä»»åŠ¡ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. è®­ç»ƒæ•°æ®")
                
                data_source_radio = gr.Radio(
                    choices=[
                        ("ä½¿ç”¨ç”Ÿæˆçš„JSONLæ•°æ®", "generated"),
                        ("ä¸Šä¼ JSON/JSONLæ–‡ä»¶", "upload")
                    ],
                    label="æ•°æ®æ¥æº",
                    value="generated",
                    info="é€‰æ‹©è®­ç»ƒæ•°æ®çš„æ¥æº"
                )
                
                json_upload_input = gr.File(
                    label="ä¸Šä¼ JSON/JSONLæ–‡ä»¶",
                    file_types=[".json", ".jsonl"],
                    file_count="single",
                    interactive=True,
                    visible=False
                )
                
                gr.Markdown("### 2. å¯åŠ¨è®­ç»ƒ")
                
                start_training_btn = gr.Button(
                    "ğŸ¯ å¼€å§‹è®­ç»ƒ",
                    variant="primary",
                    size="lg"
                )
                
                gr.Markdown("""
                ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
                
                **æ•°æ®æ¥æºé€‰é¡¹ï¼š**
                - **ç”Ÿæˆçš„JSONLæ•°æ®**: ä½¿ç”¨æ™ºèƒ½é—®ç­”ç”Ÿæˆé¡µé¢ç”Ÿæˆçš„æ•°æ®
                - **æ–‡ä»¶ä¸Šä¼ **: ä¸Šä¼ è‡ªå®šä¹‰çš„JSON/JSONLæ–‡ä»¶è¿›è¡Œè®­ç»ƒ
                
                **å¤„ç†æµç¨‹ï¼š**
                1. é€‰æ‹©æ•°æ®æ¥æº
                2. ç‚¹å‡»å¼€å§‹è®­ç»ƒ
                3. ç³»ç»Ÿä¼šè‡ªåŠ¨æ‰§è¡Œè®­ç»ƒè„šæœ¬
                
                **æ–‡ä»¶ä½ç½®ï¼š**
                - è®­ç»ƒæ•°æ®: `%APPDATA%/app/jsonl/train.jsonl`
                - è®­ç»ƒè„šæœ¬: `%APPDATA%/app/train/run.sh`
                """)
            
            with gr.Column(scale=2):
                gr.Markdown("### 3. è®­ç»ƒçŠ¶æ€")
                
                data_status = gr.Markdown(
                    value="**è¯·é€‰æ‹©æ•°æ®æ¥æº**",
                    label="ğŸ“Š æ•°æ®çŠ¶æ€"
                )
                
                training_status = gr.Markdown(
                    value="**ç­‰å¾…å¼€å§‹è®­ç»ƒ...**",
                    label="ğŸ“ˆ è®­ç»ƒçŠ¶æ€"
                )
                
                training_progress = gr.Textbox(
                    label="ğŸ”„ è®­ç»ƒè¿›åº¦",
                    lines=12,
                    show_copy_button=True,
                    interactive=False,
                    placeholder="è®­ç»ƒè¿›åº¦å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                model_download_section = gr.Group(visible=False)
                with model_download_section:
                    gr.Markdown("### ğŸ“¥ ä¸‹è½½è®­ç»ƒç»“æœ")
                    model_download = gr.File(
                        label="ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹",
                        file_count="single",
                        interactive=False
                    )
        
        def on_data_source_change(source):
            if source == "generated":
                return [
                    gr.File(visible=False),
                    "**å·²é€‰æ‹©ç”Ÿæˆçš„JSONLæ•°æ®**\n\nå°†ä½¿ç”¨æ™ºèƒ½é—®ç­”ç”Ÿæˆé¡µé¢ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚"
                ]
            else:
                return [
                    gr.File(visible=True),
                    "**å·²é€‰æ‹©æ–‡ä»¶ä¸Šä¼ **\n\nè¯·ä¸Šä¼ JSONæˆ–JSONLæ–‡ä»¶è¿›è¡Œè®­ç»ƒã€‚"
                ]
        
        data_source_radio.change(
            fn=on_data_source_change,
            inputs=data_source_radio,
            outputs=[json_upload_input, data_status]
        )
        
        return {
            "component": tab,
            "inputs": {
                "data_source_radio": data_source_radio,
                "json_upload_input": json_upload_input,
                "start_training_btn": start_training_btn
            },
            "outputs": {
                "data_status": data_status,
                "training_status": training_status,
                "training_progress": training_progress,
                "model_download_section": model_download_section,
                "model_download": model_download
            }
        }

def create_inference_tab():
    """åˆ›å»ºæ¨¡å‹æ¨ç†é¡µé¢"""
    with gr.Blocks() as tab:
        gr.Markdown("""
        # ğŸ”® æ¨¡å‹æ¨ç†æµ‹è¯•
        
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†æµ‹è¯•ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. æ¨¡å‹é…ç½®")
                
                model_path = gr.Textbox(
                    label="æ¨¡å‹è·¯å¾„",
                    value="model/merge",
                    placeholder="æ¨¡å‹ç›®å½•è·¯å¾„",
                    info="è¯·è¾“å…¥è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„"
                )
                
                load_model_btn = gr.Button(
                    "ğŸ“¥ åŠ è½½æ¨¡å‹",
                    variant="primary",
                    size="lg"
                )
                
                model_status = gr.Markdown(
                    value="**æ¨¡å‹æœªåŠ è½½**",
                    label="ğŸ“Š æ¨¡å‹çŠ¶æ€"
                )
                
                gr.Markdown("### 2. æ¨ç†è®¾ç½®")
                
                question_input = gr.Textbox(
                    label="è¾“å…¥é—®é¢˜",
                    lines=3,
                    placeholder="è¯·è¾“å…¥é—®é¢˜...",
                    info="è¾“å…¥æ‚¨æƒ³è¦è¯¢é—®çš„é—®é¢˜"
                )
                
                temperature_slider = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.7,
                    step=0.1,
                    label="Temperature",
                    info="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§"
                )
                
                max_tokens_slider = gr.Slider(
                    minimum=100,
                    maximum=4000,
                    value=2048,
                    step=100,
                    label="æœ€å¤§ç”Ÿæˆé•¿åº¦",
                    info="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦"
                )
                
                run_inference_btn = gr.Button(
                    "ğŸ” å¼€å§‹æ¨ç†",
                    variant="primary",
                    size="lg"
                )
                
                gr.Markdown("""
                ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
                
                **æ“ä½œæµç¨‹ï¼š**
                1. è¾“å…¥æ¨¡å‹è·¯å¾„
                2. ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
                3. è¾“å…¥é—®é¢˜å¹¶è®¾ç½®å‚æ•°
                4. ç‚¹å‡»"å¼€å§‹æ¨ç†"è·å–ç»“æœ
                
                **å‚æ•°è¯´æ˜ï¼š**
                - **Temperature**: å€¼è¶Šé«˜ç»“æœè¶Šéšæœºï¼Œå€¼è¶Šä½ç»“æœè¶Šç¡®å®š
                - **æœ€å¤§ç”Ÿæˆé•¿åº¦**: é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦
                """)
                
            with gr.Column(scale=2):
                gr.Markdown("### 3. æ¨ç†ç»“æœ")
                
                inference_progress = gr.Markdown(
                    value="**ç­‰å¾…æ¨ç†...**",
                    label="ğŸ”„ æ¨ç†çŠ¶æ€"
                )
                
                inference_result = gr.Textbox(
                    label="æ¨ç†ç»“æœ",
                    lines=12,
                    show_copy_button=True,
                    interactive=False,
                    placeholder="æ¨ç†ç»“æœå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
                
                gr.Markdown("### 4. æ¨ç†ä¿¡æ¯")
                
                inference_info = gr.Textbox(
                    label="æ¨ç†è¯¦æƒ…",
                    lines=4,
                    show_copy_button=True,
                    interactive=False,
                    placeholder="æ¨ç†çš„è¯¦ç»†ä¿¡æ¯å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
        
        return {
            "component": tab,
            "inputs": {
                "model_path": model_path,
                "question_input": question_input,
                "temperature_slider": temperature_slider,
                "max_tokens_slider": max_tokens_slider,
                "load_model_btn": load_model_btn,
                "run_inference_btn": run_inference_btn
            },
            "outputs": {
                "model_status": model_status,
                "inference_progress": inference_progress,
                "inference_result": inference_result,
                "inference_info": inference_info
            }
        }

def start_training_ui(data_source, uploaded_file):
    """å¯åŠ¨è®­ç»ƒçš„UIå‡½æ•° - å®æ—¶è¾“å‡ºç‰ˆæœ¬"""
    try:
        if not ensure_directories():
            return (
                "<div class='error-msg'>âŒ åˆ›å»ºç›®å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æƒé™</div>",
                "é”™è¯¯ï¼šæ— æ³•åˆ›å»ºæ‰€éœ€ç›®å½•",
                gr.Group(visible=False),
                gr.File(value=None)
            )
        
        training_logs = []
        
        if data_source == "upload":
            if not uploaded_file:
                return (
                    "<div class='error-msg'>âŒ è¯·å…ˆä¸Šä¼ JSON/JSONLæ–‡ä»¶</div>",
                    "é”™è¯¯ï¼šæœªé€‰æ‹©æ–‡ä»¶",
                    gr.Group(visible=False),
                    gr.File(value=None)
                )
            
            try:
                uploaded_path = uploaded_file.name
                # ç›´æ¥å¤åˆ¶ä¸Šä¼ çš„æ–‡ä»¶åˆ°è®­ç»ƒè·¯å¾„
                shutil.copy2(uploaded_path, JSONL_TRAIN_PATH)
                training_logs.append(f"âœ… å·²ä¿å­˜è®­ç»ƒæ•°æ®åˆ°: {JSONL_TRAIN_PATH}")
            except Exception as e:
                return (
                    f"<div class='error-msg'>âŒ ä¿å­˜è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}</div>",
                    f"é”™è¯¯ï¼šä¿å­˜æ–‡ä»¶å¤±è´¥ - {str(e)}",
                    gr.Group(visible=False),
                    gr.File(value=None)
                )
        
        else:  # ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®
            jsonl_files = list(JSONL_FOLDER.glob("*.jsonl"))
            if not jsonl_files:
                return (
                    "<div class='error-msg'>âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„JSONLæ•°æ®ï¼Œè¯·å…ˆåœ¨æ™ºèƒ½é—®ç­”ç”Ÿæˆé¡µé¢ç”Ÿæˆæ•°æ®</div>",
                    "é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒæ•°æ®",
                    gr.Group(visible=False),
                    gr.File(value=None)
                )
            
            latest_jsonl = max(jsonl_files, key=lambda x: x.stat().st_mtime)
            
            # æ£€æŸ¥æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶æ˜¯å¦ç›¸åŒ
            if latest_jsonl.resolve() == JSONL_TRAIN_PATH.resolve():
                training_logs.append(f"âœ… è®­ç»ƒæ•°æ®å·²åœ¨æ­£ç¡®ä½ç½®: {JSONL_TRAIN_PATH}")
            else:
                try:
                    shutil.copy2(latest_jsonl, JSONL_TRAIN_PATH)
                    training_logs.append(f"âœ… å·²å¤åˆ¶è®­ç»ƒæ•°æ®: {latest_jsonl.name} -> {JSONL_TRAIN_PATH}")
                except Exception as e:
                    return (
                        f"<div class='error-msg'>âŒ å¤åˆ¶è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}</div>",
                        f"é”™è¯¯ï¼šå¤åˆ¶æ–‡ä»¶å¤±è´¥ - {str(e)}",
                        gr.Group(visible=False),
                        gr.File(value=None)
                    )
        
        # æ£€æŸ¥è®­ç»ƒè„šæœ¬æ˜¯å¦å­˜åœ¨
        if not TRAIN_SCRIPT_PATH.exists():
            training_logs.append(f"âš ï¸ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {TRAIN_SCRIPT_PATH}")
            return (
                "<div class='error-msg'>âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨</div>",
                "\n".join(training_logs),
                gr.Group(visible=False),
                gr.File(value=None)
            )
        
        training_logs.append("ğŸš€ å¼€å§‹æ‰§è¡Œè®­ç»ƒè„šæœ¬...")
        training_logs.append(f"è„šæœ¬è·¯å¾„: {TRAIN_SCRIPT_PATH}")
        training_logs.append(f"è®­ç»ƒæ•°æ®: {JSONL_TRAIN_PATH}")
        training_logs.append("-" * 50)
        
        try:
            # åˆ‡æ¢åˆ°trainç›®å½•
            train_dir = TRAIN_SCRIPT_PATH.parent
            current_dir = os.getcwd()
            os.chdir(train_dir)
            
            # æ‰§è¡Œè®­ç»ƒå‘½ä»¤
            cmd = ["llamafactory-cli", "train", "llama3_lora_sft.yaml"]
            
            training_logs.append(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            training_logs.append(f"å·¥ä½œç›®å½•: {train_dir}")
            training_logs.append("=" * 50)
            
            # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # åˆå¹¶æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            training_logs.append("âœ… è®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨")
            training_logs.append("ğŸ“Š å®æ—¶è¾“å‡º:")
            
            # å®æ—¶è¯»å–è¾“å‡º
            output_lines = []
            while True:
                line = process.stdout.readline()
                if not line and process.poll() is not None:
                    break
                if line:
                    cleaned_line = line.strip()
                    output_lines.append(cleaned_line)
                    training_logs.append(cleaned_line)
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            return_code = process.wait()
            
            # åˆ‡æ¢å›åŸç›®å½•
            os.chdir(current_dir)
            
            training_logs.append("=" * 50)
            
            # æ£€æŸ¥è®­ç»ƒç»“æœ
            if return_code == 0:
                training_logs.append("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
                training_logs.append(f"é€€å‡ºç : {return_code}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹è¾“å‡º
                output_dir = train_dir / "output"
                if output_dir.exists():
                    model_files = list(output_dir.glob("**/*.bin")) + list(output_dir.glob("**/*.safetensors"))
                    if model_files:
                        training_logs.append(f"âœ… æ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶: {len(model_files)} ä¸ª")
                        for model_file in model_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                            training_logs.append(f"  - {model_file.name}")
                    else:
                        training_logs.append("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œå¯èƒ½è®­ç»ƒé…ç½®æœ‰é—®é¢˜")
                
                success_msg = f"""
                <div class='success-msg'>
                âœ… è®­ç»ƒä»»åŠ¡æˆåŠŸå®Œæˆ!
                - é€€å‡ºç : {return_code}
                - è®­ç»ƒæ—¥å¿—å·²ä¿å­˜
                </div>
                """
                
                return (
                    success_msg,
                    "\n".join(training_logs),
                    gr.Group(visible=True),
                    gr.File(value=None)
                )
            else:
                training_logs.append(f"âŒ è®­ç»ƒå¤±è´¥!")
                training_logs.append(f"é€€å‡ºç : {return_code}")
                training_logs.append("è¯·æ£€æŸ¥è®­ç»ƒé…ç½®å’Œæ•°æ®æ ¼å¼")
                
                # åˆ†æå¯èƒ½çš„é”™è¯¯åŸå› 
                error_output = "\n".join(output_lines[-10:])  # æ˜¾ç¤ºæœ€å10è¡Œè¾“å‡º
                training_logs.append("æœ€è¿‘è¾“å‡º:")
                training_logs.append(error_output)
                
                error_msg = f"""
                <div class='error-msg'>
                âŒ è®­ç»ƒä»»åŠ¡å¤±è´¥!
                - é€€å‡ºç : {return_code}
                - è¯·æ£€æŸ¥è®­ç»ƒé…ç½®ã€æ•°æ®æ ¼å¼å’Œä¾èµ–åŒ…
                </div>
                """
                
                return (
                    error_msg,
                    "\n".join(training_logs),
                    gr.Group(visible=False),
                    gr.File(value=None)
                )
            
        except Exception as e:
            # ç¡®ä¿åˆ‡æ¢å›åŸç›®å½•
            try:
                os.chdir(current_dir)
            except:
                pass
            
            training_logs.append(f"âŒ æ‰§è¡Œè®­ç»ƒè„šæœ¬å¤±è´¥: {str(e)}")
            training_logs.append(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return (
                f"<div class='error-msg'>âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {str(e)}</div>",
                "\n".join(training_logs),
                gr.Group(visible=False),
                gr.File(value=None)
            )
            
    except Exception as e:
        return (
            f"<div class='error-msg'>âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {str(e)}</div>",
            f"é”™è¯¯: {str(e)}",
            gr.Group(visible=False),
            gr.File(value=None)
        )

def load_instructions_from_jsonl(jsonl_path, max_instructions=5):
    """
    ä»JSONLæ–‡ä»¶ä¸­è¯»å–instructionä½œä¸ºå†å²å¯¹è¯
    """
    instructions = []
    try:
        if jsonl_path.exists():
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= max_instructions:
                        break
                    try:
                        data = json.loads(line.strip())
                        if 'instruction' in data and data['instruction']:
                            instructions.append(data['instruction'])
                    except json.JSONDecodeError:
                        continue
        else:
            print(f"è­¦å‘Š: JSONLæ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
    except Exception as e:
        print(f"è¯»å–JSONLæ–‡ä»¶å‡ºé”™: {e}")
    
    return instructions

def load_model_handler(model_path):
    """å¤„ç†æ¨¡å‹åŠ è½½"""
    try:
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        model_dir = Path(model_path)
        if not model_dir.exists():
            return (
                "<div class='error-msg'>âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨</div>",
                f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {model_path}"
            )
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_files = list(model_dir.glob("**/*.bin")) + list(model_dir.glob("**/*.safetensors"))
        if not model_files:
            return (
                "<div class='error-msg'>âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶</div>",
                f"åœ¨ {model_path} ä¸­æœªæ‰¾åˆ° .bin æˆ– .safetensors æ–‡ä»¶"
            )
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„æ¨¡å‹åŠ è½½é€»è¾‘
        # ä¾‹å¦‚è°ƒç”¨ç›¸åº”çš„æ¨¡å‹åŠ è½½å‡½æ•°
        
        return (
            f"<div class='success-msg'>âœ… æ¨¡å‹åŠ è½½æˆåŠŸ</div>",
            f"æ¨¡å‹ä¿¡æ¯:\n"
            f"- è·¯å¾„: {model_path}\n"
            f"- æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶\n"
            f"- ç¤ºä¾‹æ–‡ä»¶: {model_files[0].name if model_files else 'æ— '}\n"
            f"- åŠ è½½æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
        )
        
    except Exception as e:
        return (
            f"<div class='error-msg'>âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}</div>",
            f"é”™è¯¯è¯¦æƒ…: {str(e)}"
        )

def run_inference_handler(model_path, question, temperature, max_tokens):
    """å¤„ç†æ¨ç†è¯·æ±‚"""
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        model_dir = Path(model_path)
        if not model_dir.exists():
            return (
                "<div class='error-msg'>âŒ è¯·å…ˆåŠ è½½æ¨¡å‹</div>",
                "é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·å…ˆç‚¹å‡»'åŠ è½½æ¨¡å‹'",
                f"æ¨¡å‹è·¯å¾„: {model_path}\né—®é¢˜: {question}\nçŠ¶æ€: æ¨¡å‹æœªåŠ è½½"
            )
        
        if not question.strip():
            return (
                "<div class='error-msg'>âŒ è¯·è¾“å…¥é—®é¢˜</div>",
                "é”™è¯¯ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©º",
                "è¯·å…ˆè¾“å…¥æ‚¨æƒ³è¦è¯¢é—®çš„é—®é¢˜"
            )
        
        # æ˜¾ç¤ºæ¨ç†å¼€å§‹ä¿¡æ¯
        inference_info = (
            f"æ¨ç†å‚æ•°:\n"
            f"- æ¨¡å‹è·¯å¾„: {model_path}\n"
            f"- é—®é¢˜: {question}\n"
            f"- Temperature: {temperature}\n"
            f"- æœ€å¤§é•¿åº¦: {max_tokens}\n"
            f"- å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}"
        )
        
        # è°ƒç”¨æ¨ç†å‡½æ•°
        result = run_inference_ui_with_params(model_path, question, temperature, max_tokens)
        
        return (
            "<div class='success-msg'>âœ… æ¨ç†å®Œæˆ</div>",
            result,
            inference_info + f"\n- å®Œæˆæ—¶é—´: {time.strftime('%H:%M:%S')}"
        )
        
    except Exception as e:
        return (
            f"<div class='error-msg'>âŒ æ¨ç†å¤±è´¥: {str(e)}</div>",
            f"é”™è¯¯ï¼š{str(e)}",
            f"æ¨¡å‹è·¯å¾„: {model_path}\né—®é¢˜: {question}\né”™è¯¯: {str(e)}"
        )

def run_inference_ui_with_params(model_path, question, temperature=0.7, max_tokens=2048):
    """
    å¸¦å‚æ•°çš„æ¨ç†å‡½æ•°
    """
    try:
        # 0. å…ˆæ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™å¯åŠ¨
        if not is_service_running():
            start_result = start_inference_service()
            if not start_result:
                return "âŒ å¯åŠ¨æ¨ç†æœåŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ infer.sh æ–‡ä»¶"
            time.sleep(5)  # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
        
        # 1. ä»JSONLæ–‡ä»¶åŠ è½½instructionsä½œä¸ºå†å²
        instructions = load_instructions_from_jsonl(JSONL_TRAIN_PATH)
        
        # 2. æ„é€  messages åˆ—è¡¨ï¼Œä»ç³»ç»Ÿæç¤ºè¯å¼€å§‹
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # 3. å°†instructionsä½œä¸ºå†å²å¯¹è¯æ·»åŠ åˆ°messagesä¸­
        for instruction in instructions:
            messages.append({"role": "user", "content": instruction})

        # 4. å°†å½“å‰ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯æ·»åŠ åˆ°åˆ—è¡¨
        messages.append({"role": "user", "content": question})

        # 5. æ„é€ è¯·æ±‚çš„ payload
        payload = {
            "model": MODEL_NAME,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }

        # 6. å‘é€è¯·æ±‚
        response = requests.post(
            API_URL, 
            headers={"Content-Type": "application/json"}, 
            json=payload, 
            timeout=60
        )
        response.raise_for_status()

        # 7. è§£æå“åº”
        data = response.json()
        full_response = data['choices'][0]['message']['content']
        
        return f"æ¨¡å‹è·¯å¾„: {model_path}\né—®é¢˜: {question}\nå›ç­”: {full_response}"

    except requests.exceptions.RequestException as e:
        return f"æ¨¡å‹è·¯å¾„: {model_path}\né—®é¢˜: {question}\né”™è¯¯: APIè¯·æ±‚å‡ºé”™ - {e}"
    except Exception as e:
        return f"æ¨¡å‹è·¯å¾„: {model_path}\né—®é¢˜: {question}\né”™è¯¯: å‘ç”ŸæœªçŸ¥é”™è¯¯ - {e}"

def is_service_running():
    """æ£€æŸ¥æ¨ç†æœåŠ¡æ˜¯å¦å·²ç»åœ¨è¿è¡Œ"""
    try:
        response = requests.get(API_URL.replace("/v1/chat/completions", "/api/tags"), timeout=5)
        return response.status_code == 200
    except:
        return False

def start_inference_service():
    """å¯åŠ¨æ¨ç†æœåŠ¡"""
    try:
        # è¿è¡Œ infer.sh è„šæœ¬
        process = subprocess.Popen(
            ["bash", "app/infer/infer.sh"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ
        time.sleep(3)
        return process.poll() is None  # å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œè¯´æ˜å¯åŠ¨æˆåŠŸ
    except Exception as e:
        print(f"å¯åŠ¨æœåŠ¡å¤±è´¥: {e}")
        return False

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨å®æ—¶æ—¥å¿—
real_time_logs = []
qa_processing_logs = []

def add_log_message(message):
    timestamp = time.strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    real_time_logs.append(log_entry)
    if len(real_time_logs) > 100:
        real_time_logs.pop(0)

def add_qa_log(message):
    timestamp = time.strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    qa_processing_logs.append(log_entry)
    if len(qa_processing_logs) > 100:
        qa_processing_logs.pop(0)

def get_real_time_logs():
    return "\n".join(real_time_logs)

def get_qa_logs():
    return "\n".join(qa_processing_logs)

def process_multiple_pdfs(pdf_files, use_ai_processing, max_workers):
    real_time_logs.clear()
    
    if not pdf_files:
        add_log_message("âŒ è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶")
        return (
            "<div class='error-msg'>âŒ è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶</div>",
            "ç­‰å¾…å¤„ç†ä»»åŠ¡...",
            gr.Group(visible=False),
            None,
            get_real_time_logs()
        )
    
    converter.max_workers = max_workers
    
    try:
        add_log_message(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        add_log_message(f"ğŸ“Š æœ€å¤§å¹¶è¡Œæ•°: {max_workers}")
        add_log_message(f"ğŸ”§ AIå¢å¼ºå¤„ç†: {'å¯ç”¨' if use_ai_processing else 'ç¦ç”¨'}")
        
        task_ids = converter.convert_multiple_pdfs(pdf_files, use_ai_processing)
        
        status_msg = f"""
        <div class='info-box'>
        ğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶
        - æœ€å¤§å¹¶è¡Œæ•°: {max_workers}
        - ä»»åŠ¡ID: {', '.join(task_ids)}
        - çŠ¶æ€: å¤„ç†ä¸­...
        </div>
        """
        
        progress_text = f"å¼€å§‹å¤„ç† {len(pdf_files)} ä¸ªæ–‡ä»¶...\n"
        progress_text += f"å¹¶è¡Œå¤„ç†æ•°: {max_workers}\n"
        progress_text += "=" * 50 + "\n"
        
        add_log_message("ğŸ“‹ ä»»åŠ¡å·²æäº¤ï¼Œå¼€å§‹å¤„ç†...")
        
        return (
            status_msg,
            progress_text,
            gr.Group(visible=False),
            None,
            get_real_time_logs()
        )
        
    except Exception as e:
        error_msg = f"å¯åŠ¨å¹¶è¡Œå¤„ç†æ—¶å‡ºé”™: {str(e)}"
        add_log_message(f"âŒ {error_msg}")
        return (
            f"<div class='error-msg'>âŒ {error_msg}</div>",
            f"é”™è¯¯: {error_msg}",
            gr.Group(visible=False),
            None,
            get_real_time_logs()
        )

def update_progress():
    progress_updates = converter.get_progress()
    
    download_files = []
    all_completed = True
    completed_count = 0
    total_count = len(converter.current_tasks)
    
    new_logs = []
    for update in progress_updates:
        if update['type'] == 'progress':
            new_logs.append(f"[{update['task_id']}] {update['message']}")
        elif update['type'] == 'result':
            if update['result']['success']:
                download_files.append(update['result']['txt_path'])
                completed_count += 1
                new_logs.append(f"âœ… {update['task_id']} å¤„ç†å®Œæˆ: {update['result']['filename']}")
            else:
                new_logs.append(f"âŒ {update['task_id']} å¤„ç†å¤±è´¥: {update['result'].get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            task_status = converter.get_task_status(update['task_id'])
            if task_status and task_status['result'] is None:
                all_completed = False
    
    for log in new_logs:
        add_log_message(log)
    
    progress_text = f"å¤„ç†è¿›åº¦: {completed_count}/{total_count} ä¸ªä»»åŠ¡å®Œæˆ\n"
    progress_text += "=" * 50 + "\n"
    
    recent_tasks = list(converter.current_tasks.keys())[-5:]
    for task_id in recent_tasks:
        task = converter.current_tasks.get(task_id)
        if task:
            status = "âœ… å®Œæˆ" if task['result'] else "ğŸ”„ å¤„ç†ä¸­"
            progress_text += f"- {task_id}: {status}\n"
            if task['result'] and task['result']['success']:
                progress_text += f"  è¾“å‡º: {task['result']['filename']}\n"
    
    if all_completed and total_count > 0:
        status_msg = f"<div class='success-msg'>âœ… æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼å…±å®Œæˆ {completed_count} ä¸ªæ–‡ä»¶</div>"
        progress_text += f"\nâœ… æ‰€æœ‰ {completed_count} ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆï¼"
        add_log_message(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼å…±å®Œæˆ {completed_count} ä¸ªæ–‡ä»¶")
        return (
            status_msg,
            progress_text,
            gr.Group(visible=True),
            download_files,
            get_real_time_logs()
        )
    else:
        status_msg = f"<div class='info-box'>ğŸ”„ å¤„ç†ä¸­... ({completed_count}/{total_count} å®Œæˆ)</div>"
        return (
            status_msg,
            progress_text,
            gr.Group(visible=False),
            None,
            get_real_time_logs()
        )

def clear_tasks():
    converter.current_tasks.clear()
    while not converter.progress_queue.empty():
        converter.progress_queue.get()
    real_time_logs.clear()
    return "ä»»åŠ¡å†å²å·²æ¸…ç©º"

def cleanup_folders_ui():
    try:
        cleanup_folders()
        return "âœ… æ–‡ä»¶å¤¹æ¸…ç†å®Œæˆ"
    except Exception as e:
        return f"âŒ æ¸…ç†å¤±è´¥: {str(e)}"

def get_monitoring_info():
    total_tasks = len(converter.current_tasks)
    completed = sum(1 for task in converter.current_tasks.values() if task['result'])
    
    log_text = f"ç³»ç»Ÿæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
    log_text += f"æ€»ä»»åŠ¡æ•°: {total_tasks}\n"
    log_text += f"å·²å®Œæˆ: {completed}\n"
    log_text += f"å¤„ç†ä¸­: {total_tasks - completed}\n"
    log_text += "=" * 30 + "\n"
    
    for task_id, task in list(converter.current_tasks.items())[-10:]:
        status = "âœ… å®Œæˆ" if task['result'] else "ğŸ”„ å¤„ç†ä¸­"
        log_text += f"{task_id}: {status}\n"
    
    return total_tasks, completed, log_text

def check_txt_folder(txt_folder):
    if not os.path.exists(txt_folder):
        add_qa_log(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {txt_folder}")
        return (
            "<div class='error-msg'>âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨</div>",
            f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {txt_folder}",
            get_qa_logs()
        )
    
    txt_files = list(Path(txt_folder).glob("*.txt"))
    if not txt_files:
        add_qa_log(f"âŒ æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶: {txt_folder}")
        return (
            "<div class='error-msg'>âŒ æœªæ‰¾åˆ°TXTæ–‡ä»¶</div>",
            f"âŒ æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶: {txt_folder}",
            get_qa_logs()
        )
    
    file_info = "\n".join([f"- {f.name} ({os.path.getsize(f)} bytes)" for f in txt_files[:10]])
    if len(txt_files) > 10:
        file_info += f"\n- ... è¿˜æœ‰ {len(txt_files) - 10} ä¸ªæ–‡ä»¶"
    
    add_qa_log(f"âœ… æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶")
    return (
        f"<div class='success-msg'>âœ… æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶</div>",
        f"âœ… æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶:\n{file_info}",
        get_qa_logs()
    )

def run_qa_generation(txt_folder, instruction, language, num_questions, clear_jsonl, qa_max_workers):
    """è¿è¡Œæ™ºèƒ½é—®ç­”ç”Ÿæˆ"""
    # æ¸…ç©ºæ—¥å¿—å’Œè¿›åº¦
    qa_processing_logs.clear()
    reset_qa_progress()
    
    add_qa_log("ğŸš€ å¼€å§‹æ™ºèƒ½é—®ç­”ç”Ÿæˆæµç¨‹")
    add_qa_log(f"ğŸ“ TXTæ–‡ä»¶å¤¹: {txt_folder}")
    add_qa_log(f"ğŸ¯ ä¸“å®¶æŒ‡ä»¤: {instruction}")
    add_qa_log(f"ğŸŒ è¾“å‡ºè¯­è¨€: {language}")
    add_qa_log(f"ğŸ“Š é—®é¢˜æ•°é‡: {num_questions}")
    add_qa_log(f"ğŸ§¹ æ¸…ç©ºJSONL: {'æ˜¯' if clear_jsonl else 'å¦'}")
    add_qa_log(f"âš¡ å¹¶è¡Œå¤„ç†æ•°: {qa_max_workers}")
    
    try:
        # 0. å¦‚æœéœ€è¦ï¼Œæ¸…ç©ºJSONLæ–‡ä»¶å¤¹
        if clear_jsonl:
            add_qa_log("ğŸ§¹ æ­£åœ¨æ¸…ç©ºJSONLæ–‡ä»¶å¤¹...")
            if cleanup_jsonl_folder():
                add_qa_log("âœ… JSONLæ–‡ä»¶å¤¹æ¸…ç©ºå®Œæˆ")
            else:
                add_qa_log("âš ï¸ JSONLæ–‡ä»¶å¤¹æ¸…ç©ºå¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†")
        
        # 1. è¯»å–TXTæ–‡ä»¶
        add_qa_log("ğŸ“– æ­£åœ¨è¯»å–TXTæ–‡ä»¶...")
        contents = read_multiple_txt_files(txt_folder)
        
        if not contents:
            error_msg = f"âŒ åœ¨æ–‡ä»¶å¤¹ {txt_folder} ä¸­æœªæ‰¾åˆ°TXTæ–‡ä»¶"
            add_qa_log(error_msg)
            return (
                f"<div class='error-msg'>{error_msg}</div>",
                error_msg,
                gr.Group(visible=False),
                gr.File(value=None),
                get_qa_logs()
            )
        
        # æ£€æŸ¥å¹¶è¡Œæ•°æ˜¯å¦åˆç†
        if qa_max_workers > len(contents):
            error_msg = f"âŒ å¹¶è¡Œå¤„ç†æ•° {qa_max_workers} å¤§äºæ–‡ä»¶æ•° {len(contents)}ï¼Œè¯·å‡å°‘å¹¶è¡Œæ•°"
            add_qa_log(error_msg)
            return (
                f"<div class='error-msg'>{error_msg}</div>",
                error_msg,
                gr.Group(visible=False),
                gr.File(value=None),
                get_qa_logs()
            )
        
        add_qa_log(f"âœ… æˆåŠŸè¯»å– {len(contents)} ä¸ªTXTæ–‡ä»¶")
        
        # 2. åˆ†æç ”ç©¶é¢†åŸŸ
        add_qa_log("ğŸ” æ­£åœ¨åˆ†æç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜...")
        domain_analysis = analyze_research_domains(contents)
        
        progress_text = f"å¤„ç†è¿›åº¦:\n"
        progress_text += f"- å·²è¯»å– {len(contents)} ä¸ªTXTæ–‡ä»¶\n"
        progress_text += f"- ä¸»è¦é¢†åŸŸ: {', '.join(domain_analysis['primary_domains'])}\n"
        progress_text += f"- æ ¸å¿ƒä¸»é¢˜: {', '.join(domain_analysis['primary_themes'])}\n"
        progress_text += f"- å…³é”®è¯: {', '.join(domain_analysis['top_keywords'][:5])}\n"
        progress_text += f"- å¹¶è¡Œå¤„ç†æ•°: {qa_max_workers}\n"
        progress_text += "-" * 50 + "\n"
        progress_text += "æ­£åœ¨å¹¶è¡Œç”Ÿæˆæ·±åº¦å­¦æœ¯é—®é¢˜...\n"
        
        # 3. ç”Ÿæˆé—®ç­”å¯¹ï¼ˆä½¿ç”¨å¹¶è¡Œç‰ˆæœ¬ï¼‰
        add_qa_log("ğŸ¤– å¼€å§‹å¹¶è¡Œç”Ÿæˆæ·±åº¦å­¦æœ¯é—®ç­”å¯¹...")
        qa_pairs = adaptive_question_generation_parallel(contents, num_questions, language, domain_analysis, qa_max_workers)
        
        if not qa_pairs:
            error_msg = "âŒ é—®ç­”å¯¹ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®æˆ–é‡è¯•"
            add_qa_log(error_msg)
            return (
                f"<div class='error-msg'>{error_msg}</div>",
                progress_text + f"\n{error_msg}",
                gr.Group(visible=False),
                gr.File(value=None),
                get_qa_logs()
            )
        
        add_qa_log(f"âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªæ·±åº¦å­¦æœ¯é—®ç­”å¯¹")
        
        # 4. ä¿å­˜ç»“æœåˆ°æœ¬åœ°jsonlæ–‡ä»¶å¤¹å’Œè®­ç»ƒè·¯å¾„
        add_qa_log("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        ensure_directories()
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        local_output_file = JSONL_FOLDER / f"qa_academic_{timestamp}.jsonl"
        
        # ä¿å­˜åˆ°æœ¬åœ°
        total_records = save_qa_dataset(qa_pairs, str(local_output_file), instruction, language)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤åˆ¶åˆ°è®­ç»ƒè·¯å¾„ï¼ˆå¦‚æœæ–‡ä»¶ä¸åŒï¼‰
        if local_output_file.resolve() != JSONL_TRAIN_PATH.resolve():
            try:
                shutil.copy2(local_output_file, JSONL_TRAIN_PATH)
                add_qa_log(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {JSONL_TRAIN_PATH}")
            except Exception as e:
                add_qa_log(f"âš ï¸ ä¿å­˜åˆ°è®­ç»ƒè·¯å¾„å¤±è´¥: {e}")
        else:
            add_qa_log(f"âœ… è®­ç»ƒæ•°æ®å·²åœ¨æ­£ç¡®ä½ç½®: {JSONL_TRAIN_PATH}")
        
        progress_text = f"å¤„ç†è¿›åº¦:\n"
        progress_text += f"- å·²è¯»å– {len(contents)} ä¸ªTXTæ–‡ä»¶\n"
        progress_text += f"- æˆåŠŸç”Ÿæˆ: {len(qa_pairs)} ä¸ªé—®ç­”å¯¹\n"
        progress_text += f"- æœ¬åœ°ä¿å­˜: {local_output_file}\n"
        progress_text += f"- è®­ç»ƒè·¯å¾„: {JSONL_TRAIN_PATH}\n"
        progress_text += f"- æ€»è®°å½•: {total_records} æ¡"
        
        success_msg = f"""
        <div class='success-msg'>
        âœ… æ™ºèƒ½é—®ç­”ç”Ÿæˆå®Œæˆ!
        - ç”Ÿæˆé—®ç­”å¯¹: {len(qa_pairs)} ä¸ª
        - æœ¬åœ°æ–‡ä»¶: {local_output_file.name}
        - è®­ç»ƒæ–‡ä»¶: {JSONL_TRAIN_PATH}
        - æ€»è®°å½•æ•°: {total_records} æ¡
        </div>
        """
        
        add_qa_log("ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆ!")
        
        return (
            success_msg,
            progress_text,
            gr.Group(visible=True),
            gr.File(value=str(local_output_file)),
            get_qa_logs()
        )
        
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        add_qa_log(f"âŒ {error_msg}")
        return (
            f"<div class='error-msg'>{error_msg}</div>",
            f"é”™è¯¯: {error_msg}",
            gr.Group(visible=False),
            gr.File(value=None),
            get_qa_logs()
        )

def main():
    ensure_directories()
    
    custom_css = """
    .gradio-container {
        max-width: 1400px !important;
    }
    .success-msg {
        background: #d4edda;
        border: 2px solid #28a745;
        border-radius: 10px;
        padding: 15px;
        margin: 15px 0;
    }
    .error-msg {
        background: #f8d7da;
        border: 2px solid #dc3545;
        border-radius: 10px;
        padding: 15px;
        margin: 15px 0;
    }
    .info-box {
        background: #d1ecf1;
        border: 2px solid #17a2b8;
        border-radius: 10px;
        padding: 15px;
        margin: 15px 0;
    }
    #terminal-output {
        font-family: 'Courier New', monospace;
        font-size: 12px;
        background-color: #1e1e1e;
        color: #00ff00;
        border: 1px solid #444;
        white-space: pre-wrap;
        word-wrap: break-word;
    }
    .progress-bar {
        background: #f8f9fa;
        border: 1px solid #dee2e6;
        border-radius: 5px;
        padding: 10px;
        margin: 10px 0;
    }
    .cleanup-btn {
        background: #ffc107;
        border: 1px solid #ffc107;
    }
    .cleanup-btn:hover {
        background: #e0a800;
        border: 1px solid #e0a800;
    }
    """
    
    with gr.Blocks(
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="purple"
        ),
        css=custom_css,
        title="é«˜çº§PDFå¤„ç†å·¥å…· - å…¨æµç¨‹AIè§£å†³æ–¹æ¡ˆ"
    ) as demo:
        
        gr.Markdown("# ğŸ“„ é«˜çº§PDFå¤„ç†å·¥å…·")
        gr.Markdown("ä¸€ç«™å¼PDFè½¬TXTã€é—®ç­”ç”Ÿæˆã€æ¨¡å‹è®­ç»ƒä¸æ¨ç†è§£å†³æ–¹æ¡ˆ")
        
        with gr.Tabs() as tabs:
            with gr.Tab("ğŸ”„ æ‰¹é‡PDFè½¬TXT", id=0):
                pdf_tab = create_pdf_converter_tab()
            
            with gr.Tab("ğŸ¤– æ™ºèƒ½é—®ç­”ç”Ÿæˆ", id=1):
                qa_tab = create_qa_generator_tab()
            
            with gr.Tab("ğŸš€ DeepSeekè®­ç»ƒ", id=2):
                training_tab = create_deepseek_training_tab()
            
            with gr.Tab("ğŸ”® æ¨¡å‹æ¨ç†", id=3):
                inference_tab = create_inference_tab()
            
            with gr.Tab("ğŸ“Š ä»»åŠ¡ç›‘æ§", id=4):
                with gr.Blocks() as monitoring_tab:
                    gr.Markdown("# ğŸ“Š ä»»åŠ¡ç›‘æ§")
                    gr.Markdown("å®æ—¶ç›‘æ§æ‰€æœ‰å¤„ç†ä»»åŠ¡çš„çŠ¶æ€å’Œè¿›åº¦")
                    
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### ç³»ç»ŸçŠ¶æ€")
                            active_tasks = gr.Number(
                                label="æ´»è·ƒä»»åŠ¡æ•°",
                                value=0,
                                interactive=False
                            )
                            completed_tasks = gr.Number(
                                label="å·²å®Œæˆä»»åŠ¡",
                                value=0,
                                interactive=False
                            )
                            
                            refresh_monitoring_btn = gr.Button(
                                "ğŸ”„ åˆ·æ–°çŠ¶æ€",
                                variant="secondary"
                            )
                            clear_btn = gr.Button(
                                "ğŸ§¹ æ¸…ç©ºä»»åŠ¡å†å²",
                                variant="secondary"
                            )
                            clear_output = gr.Textbox(
                                label="æ“ä½œç»“æœ",
                                interactive=False,
                                visible=False
                            )
                        
                        with gr.Column():
                            gr.Markdown("### å®æ—¶æ—¥å¿—")
                            monitoring_log = gr.Textbox(
                                label="ç³»ç»Ÿæ—¥å¿—",
                                lines=10,
                                interactive=False,
                                show_copy_button=True
                            )
                    
                    refresh_monitoring_btn.click(
                        fn=get_monitoring_info,
                        outputs=[active_tasks, completed_tasks, monitoring_log]
                    )
                    
                    clear_btn.click(
                        fn=clear_tasks,
                        outputs=clear_output
                    ).then(
                        fn=lambda: (0, 0, "ä»»åŠ¡å†å²å·²æ¸…ç©º"),
                        outputs=[active_tasks, completed_tasks, monitoring_log]
                    )
        
        # PDFè½¬æ¢äº‹ä»¶ç»‘å®š
        pdf_tab["inputs"]["convert_btn"].click(
            fn=process_multiple_pdfs,
            inputs=[
                pdf_tab["inputs"]["pdf_input"],
                pdf_tab["inputs"]["use_ai_processing"],
                pdf_tab["inputs"]["max_workers"]
            ],
            outputs=[
                pdf_tab["outputs"]["task_status"],
                pdf_tab["outputs"]["progress_display"],
                pdf_tab["outputs"]["download_section"],
                pdf_tab["outputs"]["txt_download"],
                pdf_tab["outputs"]["terminal_output"]
            ]
        )
        
        pdf_tab["inputs"]["cleanup_btn"].click(
            fn=cleanup_folders_ui,
            outputs=pdf_tab["outputs"]["cleanup_output"]
        )
        
        # é—®ç­”ç”Ÿæˆäº‹ä»¶ç»‘å®š
        qa_tab["inputs"]["generate_btn"].click(
            fn=run_qa_generation,
            inputs=[
                qa_tab["inputs"]["txt_folder"],
                qa_tab["inputs"]["instruction"], 
                qa_tab["inputs"]["language"],
                qa_tab["inputs"]["num_questions"],
                qa_tab["inputs"]["clear_jsonl_checkbox"],
                qa_tab["inputs"]["qa_max_workers"]
            ],
            outputs=[
                qa_tab["outputs"]["processing_status"],
                qa_tab["outputs"]["progress_display"],
                qa_tab["outputs"]["download_section"],
                qa_tab["outputs"]["output_download"],
                qa_tab["outputs"]["terminal_output"]
            ]
        )
        
        qa_tab["inputs"]["check_folder_btn"].click(
            fn=check_txt_folder,
            inputs=qa_tab["inputs"]["txt_folder"],
            outputs=[
                qa_tab["outputs"]["folder_status"],
                qa_tab["outputs"]["progress_display"],
                qa_tab["outputs"]["terminal_output"]
            ]
        )
        
        # è®­ç»ƒäº‹ä»¶ç»‘å®š
        training_tab["inputs"]["start_training_btn"].click(
            fn=start_training_ui,
            inputs=[
                training_tab["inputs"]["data_source_radio"],
                training_tab["inputs"]["json_upload_input"]
            ],
            outputs=[
                training_tab["outputs"]["training_status"],
                training_tab["outputs"]["training_progress"],
                training_tab["outputs"]["model_download_section"],
                training_tab["outputs"]["model_download"]
            ]
        )
        
        # æ¨ç†äº‹ä»¶ç»‘å®š
        inference_tab["inputs"]["load_model_btn"].click(
            fn=load_model_handler,
            inputs=inference_tab["inputs"]["model_path"],
            outputs=[
                inference_tab["outputs"]["model_status"],
                inference_tab["outputs"]["inference_info"]
            ]
        )
        
        inference_tab["inputs"]["run_inference_btn"].click(
            fn=run_inference_handler,
            inputs=[
                inference_tab["inputs"]["model_path"],
                inference_tab["inputs"]["question_input"],
                inference_tab["inputs"]["temperature_slider"],
                inference_tab["inputs"]["max_tokens_slider"]
            ],
            outputs=[
                inference_tab["outputs"]["inference_progress"],
                inference_tab["outputs"]["inference_result"],
                inference_tab["outputs"]["inference_info"]
            ]
        )
        
        # è¿›åº¦æ§åˆ¶
        with gr.Row():
            gr.Markdown("### è¿›åº¦æ§åˆ¶")
            manual_refresh_btn = gr.Button("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°è¿›åº¦", variant="secondary")
            manual_refresh_btn.click(
                fn=update_progress,
                outputs=[
                    pdf_tab["outputs"]["task_status"],
                    pdf_tab["outputs"]["progress_display"],
                    pdf_tab["outputs"]["download_section"],
                    pdf_tab["outputs"]["txt_download"],
                    pdf_tab["outputs"]["terminal_output"]
                ]
            )
            
    return demo

if __name__ == "__main__":
    app = main()
    app.launch(
        server_name="0.0.0.0",
        server_port=7888,
        share=True,
        show_error=True,
        inbrowser=True
    )
