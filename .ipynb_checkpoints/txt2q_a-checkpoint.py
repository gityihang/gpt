import json
from openai import OpenAI
import re
import os
import glob
from typing import List, Dict
import time
from collections import Counter

os.environ["DEEPSEEK_API_KEY"] = "sk-b6118335f5c34520abffbe6fa324257a"

api_key = os.environ.get("DEEPSEEK_API_KEY")
if not api_key:
    raise ValueError("Please set the environment variable DEEPSEEK_API_KEY")

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def read_multiple_txt_files(txt_folder: str = "txt_files") -> Dict[str, str]:
    """
    è¯»å–å¤šä¸ªtxtæ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶åå’Œå†…å®¹çš„å­—å…¸
    """
    txt_files = glob.glob(os.path.join(txt_folder, "*.txt"))
    contents = {}
    
    for txt_file in txt_files:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
                file_name = os.path.basename(txt_file)
                contents[file_name] = content
                print(f"âœ… å·²è¯»å–: {file_name} (é•¿åº¦: {len(content)} å­—ç¬¦)")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {txt_file} æ—¶å‡ºé”™: {e}")
    
    return contents

def debug_gpt_output(gpt_output: str) -> str:
    """
    è°ƒè¯•GPTè¾“å‡ºï¼Œè¯¦ç»†åˆ†æé—®é¢˜
    """
    print(f"=== DEBUG INFO ===")
    print(f"åŸå§‹è¾“å‡ºé•¿åº¦: {len(gpt_output)}")
    print(f"å‰200å­—ç¬¦: {gpt_output[:200]}")
    
    # æ£€æŸ¥å¸¸è§é—®é¢˜
    if "error" in gpt_output.lower():
        print("âš ï¸ åŒ…å«é”™è¯¯ä¿¡æ¯")
    if "rate limit" in gpt_output.lower():
        print("âš ï¸ å¯èƒ½è¾¾åˆ°é¢‘ç‡é™åˆ¶")
    if "```" in gpt_output:
        print("âš ï¸ åŒ…å«ä»£ç å—æ ‡è®°")
    
    return gpt_output

def robust_clean_gpt_output(gpt_output: str) -> str:
    """
    æ›´å¥å£®çš„GPTè¾“å‡ºæ¸…ç†
    """
    if not gpt_output:
        print("âŒ è¾“å‡ºä¸ºç©º")
        return ""
    
    original_output = gpt_output
    gpt_output = gpt_output.strip()
    
    print(f"æ¸…ç†å‰é•¿åº¦: {len(gpt_output)}")
    
    # å¤šå±‚æ¸…ç†
    cleaning_steps = [
        # ç§»é™¤ä»£ç å—
        (r'```json\s*', ''),
        (r'```\s*', ''),
        (r'\s*```', ''),
        # ç§»é™¤é¢å¤–çš„ç©ºç™½
        (r'\n+', ' '),
        (r'\s+', ' '),
    ]
    
    for pattern, replacement in cleaning_steps:
        gpt_output = re.sub(pattern, replacement, gpt_output, flags=re.DOTALL | re.IGNORECASE)
    
    gpt_output = gpt_output.strip()
    print(f"æ¸…ç†åé•¿åº¦: {len(gpt_output)}")
    
    return gpt_output

def extract_qa_pairs_from_response(parsed_data) -> list:
    """
    ä»APIå“åº”ä¸­æå–é—®ç­”å¯¹ï¼Œå¤„ç†å¤šç§å¯èƒ½çš„æ ¼å¼
    """
    qa_pairs = []
    
    if isinstance(parsed_data, list):
        # æ ¼å¼1: [{"question": "...", "answer": "..."}]
        for item in parsed_data:
            if isinstance(item, dict):
                if "question" in item and "answer" in item:
                    qa_pairs.append({
                        "question": item["question"],
                        "answer": item["answer"]
                    })
                elif "question_en" in item and "answer_en" in item:
                    qa_pairs.append({
                        "question_en": item["question_en"],
                        "answer_en": item["answer_en"],
                        "question_zh": item.get("question_zh", ""),
                        "answer_zh": item.get("answer_zh", "")
                    })
    
    elif isinstance(parsed_data, dict):
        # æ ¼å¼2: {"questions": [...], "answers": [...]}
        if "questions" in parsed_data and "answers" in parsed_data:
            questions = parsed_data["questions"]
            answers = parsed_data["answers"]
            if isinstance(questions, list) and isinstance(answers, list):
                for i, (q, a) in enumerate(zip(questions, answers)):
                    if isinstance(q, dict) and "question" in q:
                        q_text = q["question"]
                        a_text = a["answer"] if isinstance(a, dict) and "answer" in a else a
                    else:
                        q_text = str(q)
                        a_text = str(a)
                    qa_pairs.append({"question": q_text, "answer": a_text})
        
        # æ ¼å¼3: {"qa_pairs": [{"q": "...", "a": "..."}]}
        elif "qa_pairs" in parsed_data:
            for item in parsed_data["qa_pairs"]:
                if isinstance(item, dict):
                    question = item.get("q") or item.get("question")
                    answer = item.get("a") or item.get("answer")
                    if question and answer:
                        qa_pairs.append({"question": question, "answer": answer})
        
        # æ ¼å¼4: ç›´æ¥åŒ…å«questionå’Œanswerçš„å­—å…¸
        elif "question" in parsed_data and "answer" in parsed_data:
            qa_pairs.append({
                "question": parsed_data["question"],
                "answer": parsed_data["answer"]
            })
    
    return qa_pairs

def _get_system_message(language: str) -> str:
    """è·å–ç³»ç»Ÿæ¶ˆæ¯ - æ›´ä¸¥æ ¼çš„æŒ‡ä»¤"""
    if language == "chinese":
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶äººå‘˜ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºé—®ç­”å¯¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
[{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}, {"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}]
å¿…é¡»è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«questionå’Œanswerå­—æ®µã€‚"""
    elif language == "english":
        return """You are a professional researcher. Please output Q&A pairs in strict JSON format only:
[{"question": "question1", "answer": "answer1"}, {"question": "question2", "answer": "answer2"}]
Return ONLY a JSON array with question and answer fields."""
    else:
        return """You are a bilingual research expert. Output in strict JSON format only:
[{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}]
Return ONLY a JSON array."""

def _call_deepseek_api_with_retry(prompt: str, language: str, max_retries: int = 3) -> list:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨ - ä¿®å¤ç‰ˆ
    """
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ APIè°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
            
            system_message = _get_system_message(language)
            
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt},
                ],
                stream=False,
                temperature=0.3,
                max_tokens=4000,
                response_format={"type": "json_object"}  # å¼ºåˆ¶JSONæ ¼å¼
            )

            gpt_output = response.choices[0].message.content
            
            # è°ƒè¯•è¾“å‡º
            debug_gpt_output(gpt_output)
            
            # æ¸…ç†è¾“å‡º
            cleaned_output = robust_clean_gpt_output(gpt_output)
            
            try:
                parsed_data = json.loads(cleaned_output)
                print(f"âœ… JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                
                # æå–é—®ç­”å¯¹
                qa_pairs = extract_qa_pairs_from_response(parsed_data)
                
                if qa_pairs:
                    print(f"âœ… æˆåŠŸæå– {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                    return qa_pairs
                else:
                    print(f"âŒ æœªæå–åˆ°æœ‰æ•ˆé—®ç­”å¯¹")
                    print(f"è§£æçš„æ•°æ®ç»“æ„: {parsed_data}")
                    if attempt < max_retries - 1:
                        continue
                    else:
                        return []
                        
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"æ¸…ç†åçš„å†…å®¹: {cleaned_output[:500]}...")
                
                if attempt < max_retries - 1:
                    print("ç­‰å¾…åé‡è¯•...")
                    time.sleep(2)
                    continue
                else:
                    return []
                    
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                time.sleep(5)
                continue
            else:
                return []
    
    return []

def extract_paper_keywords(content: str, file_name: str, max_keywords: int = 8) -> List[str]:
    """
    ä»è®ºæ–‡å†…å®¹å’Œæ ‡é¢˜ä¸­æå–æœ‰æ„ä¹‰çš„å­¦æœ¯å…³é”®è¯
    """
    # ä»æ–‡ä»¶åæå–æœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼ˆå»é™¤æ‰©å±•åå’Œå¸¸è§æ— æ„ä¹‰è¯ï¼‰
    file_base = os.path.splitext(file_name)[0]  # å»é™¤.txtæ‰©å±•å
    file_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]{2,}', file_base)
    
    # ä»å†…å®¹æå–é«˜é¢‘è¯ï¼Œä½†è¿›è¡Œæ›´ä¸¥æ ¼çš„è¿‡æ»¤
    content_words = re.findall(r'[a-zA-Z\u4e00-\u9fff]{3,}', content.lower()[:5000])  # åªåˆ†æå‰5000å­—ç¬¦
    
    # æ‰©å±•åœç”¨è¯åˆ—è¡¨
    stop_words = {
        # è‹±æ–‡åœç”¨è¯
        'the', 'and', 'for', 'with', 'this', 'that', 'from', 'have', 'has', 'were', 'are', 'was', 'been', 'being',
        'which', 'what', 'when', 'where', 'why', 'how', 'who', 'whom', 'whose', 'will', 'would', 'could', 'should',
        'may', 'might', 'must', 'can', 'cannot', 'able', 'about', 'above', 'after', 'again', 'against', 'all', 'any',
        'because', 'before', 'below', 'between', 'both', 'but', 'by', 'during', 'each', 'few', 'more', 'most', 'other',
        'some', 'such', 'than', 'then', 'there', 'these', 'they', 'those', 'through', 'until', 'very', 'while', 'within',
        'without', 'based', 'using', 'study', 'research', 'paper', 'article', 'analysis', 'method', 'result', 'conclusion',
        'introduction', 'abstract', 'background', 'objective', 'purpose', 'aim', 'goal', 'find', 'found', 'show', 'showed',
        'demonstrate', 'demonstrated', 'indicate', 'indicated', 'suggest', 'suggested', 'reveal', 'revealed', 'provide',
        'provided', 'present', 'presented', 'discuss', 'discussed', 'explain', 'explained', 'describe', 'described',
        'examine', 'examined', 'investigate', 'investigated', 'explore', 'explored', 'assess', 'assessed', 'evaluate',
        'evaluated', 'measure', 'measured', 'test', 'tested', 'model', 'models', 'data', 'dataset', 'sample', 'samples',
        
        # ä¸­æ–‡åœç”¨è¯
        'çš„', 'åœ¨', 'æ˜¯', 'äº†', 'å’Œ', 'ä¸', 'åŠ', 'ç­‰', 'å…¶ä¸­', 'é€šè¿‡', 'åŸºäº', 'ä½¿ç”¨', 'é‡‡ç”¨', 'è¿›è¡Œ', 'å…·æœ‰', 'åŒ…æ‹¬',
        'åŒ…å«', 'æ¶‰åŠ', 'å…³äº', 'å¯¹äº', 'å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'è™½ç„¶', 'å°½ç®¡', 'å¦‚æœ', 'é‚£ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥',
        'æœ¬æ–‡', 'æœ¬ç ”ç©¶', 'æˆ‘ä»¬', 'ä½œè€…', 'è®ºæ–‡', 'æ–‡ç« ', 'ç ”ç©¶', 'åˆ†æ', 'æ–¹æ³•', 'ç»“æœ', 'ç»“è®º', 'å¼•è¨€', 'æ‘˜è¦', 'èƒŒæ™¯',
        'ç›®çš„', 'ç›®æ ‡', 'å‘ç°', 'è¡¨æ˜', 'è¯æ˜', 'æ˜¾ç¤º', 'æ­ç¤º', 'æä¾›', 'æå‡º', 'è®¨è®º', 'è§£é‡Š', 'æè¿°', 'è€ƒå¯Ÿ', 'è°ƒæŸ¥',
        'æ¢è®¨', 'è¯„ä¼°', 'æµ‹é‡', 'æµ‹è¯•', 'æ¨¡å‹', 'æ•°æ®', 'æ ·æœ¬', 'èµ„æ–™'
    }
    
    # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
    meaningful_words = [
        word for word in content_words 
        if (word not in stop_words and 
            len(word) > 2 and 
            not word.isdigit() and  # æ’é™¤çº¯æ•°å­—
            not re.match(r'^[0-9\.]+$', word))  # æ’é™¤æ•°å­—å’Œç‚¹ç»„æˆçš„å­—ç¬¦ä¸²
    ]
    
    # ç»Ÿè®¡è¯é¢‘ï¼Œä½†ç»™äºˆæ ‡é¢˜è¯æ›´é«˜æƒé‡
    word_freq = Counter(meaningful_words)
    
    # ç»™æ–‡ä»¶åä¸­çš„è¯å¢åŠ æƒé‡
    for file_word in file_words:
        if file_word.lower() not in stop_words and len(file_word) > 2:
            word_freq[file_word.lower()] += 5  # ç»™æ ‡é¢˜è¯æ›´é«˜æƒé‡
    
    # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„å…³é”®è¯
    top_keywords = []
    for word, freq in word_freq.most_common(20):  # å…ˆå–å‰20ä¸ª
        # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šæ’é™¤å¤ªå¸¸è§çš„å­¦æœ¯è¯æ±‡
        common_academic_words = {
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact',
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact',
            'analysis', 'method', 'results', 'study', 'research', 'model', 'data', 'effect', 'impact'
        }
        if word not in common_academic_words:
            top_keywords.append(word)
    
    return top_keywords[:max_keywords]

def analyze_research_domains(contents: Dict[str, str]) -> Dict:
    """
    ç»¼åˆåˆ†ææ‰€æœ‰æ–‡ä»¶çš„ç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜
    """
    print("ğŸ” æ­£åœ¨åˆ†æç ”ç©¶é¢†åŸŸå’Œä¸»é¢˜...")
    
    all_domains = []
    all_themes = []
    all_keywords = []
    
    for file_name, content in contents.items():
        content_lower = content.lower()
        
        # é¢†åŸŸæ˜ å°„ - æ›´ç²¾ç¡®çš„åŒ¹é…
        domain_mapping = {
            'ç»æµå­¦': ['economic', 'economy', 'gdp', 'market', 'financial', 'investment', 'price', 'cost', 'income', 'revenue', 'profit', 'trade'],
            'æ°”å€™ç§‘å­¦': ['climate', 'temperature', 'weather', 'precipitation', 'emission', 'carbon', 'warming', 'greenhouse', 'atmospheric'],
            'å†œä¸š': ['agriculture', 'crop', 'farm', 'food', 'yield', 'harvest', 'rural', 'farming', 'irrigation', 'fertilizer'],
            'ç¯å¢ƒç§‘å­¦': ['environment', 'pollution', 'sustainability', 'ecology', 'conservation', 'ecosystem', 'biodiversity', 'environmental'],
            'é£é™©ç®¡ç†': ['risk', 'management', 'mitigation', 'uncertainty', 'vulnerability', 'resilience', 'exposure', 'hazard'],
            'æ”¿ç­–åˆ†æ': ['policy', 'regulation', 'governance', 'intervention', 'strategy', 'measure', 'implementation', 'enforcement']
        }
        
        # ä¸»é¢˜æ˜ å°„ - æ›´ç²¾ç¡®çš„åŒ¹é…
        theme_mapping = {
            'å½±å“è¯„ä¼°': ['impact', 'effect', 'evaluation', 'assessment', 'consequence', 'outcome', 'result'],
            'æœºåˆ¶åˆ†æ': ['mechanism', 'pathway', 'channel', 'transmission', 'causal', 'causality', 'mediation'],
            'å®è¯ç ”ç©¶': ['empirical', 'evidence', 'data analysis', 'statistical', 'regression', 'estimation', 'empirically'],
            'æ”¿ç­–å»ºè®®': ['policy', 'recommendation', 'suggestion', 'implication', 'application', 'recommend', 'suggest'],
            'æ¨¡å‹æ„å»º': ['model', 'framework', 'theoretical', 'conceptual', 'simulation', 'modeling', 'theoretical']
        }
        
        # è¯†åˆ«é¢†åŸŸ - è¦æ±‚è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
        for domain, keywords in domain_mapping.items():
            match_count = sum(1 for keyword in keywords if keyword in content_lower)
            if match_count >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯æ‰è®¤ä¸ºæ˜¯è¯¥é¢†åŸŸ
                all_domains.append(domain)
        
        # è¯†åˆ«ä¸»é¢˜ - è¦æ±‚è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
        for theme, keywords in theme_mapping.items():
            match_count = sum(1 for keyword in keywords if keyword in content_lower)
            if match_count >= 2:
                all_themes.append(theme)
        
        # æ”¶é›†å…³é”®è¯
        keywords = extract_paper_keywords(content, file_name)
        all_keywords.extend(keywords)
    
    # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°é¢†åŸŸï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not all_domains:
        all_domains = ['å­¦æœ¯ç ”ç©¶']
    
    # ç¡®å®šä¸»è¦é¢†åŸŸå’Œä¸»é¢˜
    domain_counter = Counter(all_domains)
    theme_counter = Counter(all_themes)
    keyword_counter = Counter(all_keywords)
    
    primary_domains = [domain for domain, count in domain_counter.most_common(2)]
    primary_themes = [theme for theme, count in theme_counter.most_common(3)] if theme_counter else ['ç»¼åˆç ”ç©¶']
    top_keywords = [keyword for keyword, count in keyword_counter.most_common(10)]
    
    return {
        'primary_domains': primary_domains,
        'primary_themes': primary_themes,
        'top_keywords': top_keywords,
        'file_count': len(contents)
    }
def generate_single_domain_prompt(content: str, file_name: str, num_questions: int, language: str, domain_analysis: Dict) -> str:
    """
    ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆé¢†åŸŸæ·±åº¦é—®é¢˜çš„æç¤ºè¯ - ä¿®å¤ç‰ˆ
    """
    domains_text = 'ã€'.join(domain_analysis['primary_domains'])
    themes_text = 'ã€'.join(domain_analysis['primary_themes'])
    
    if language == "chinese":
        prompt_content = f"""
è¯·åŸºäºä»¥ä¸‹{domains_text}é¢†åŸŸçš„ç ”ç©¶å†…å®¹ï¼Œç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰å­¦æœ¯æ·±åº¦çš„ä¸“ä¸šé—®é¢˜ã€‚

ç ”ç©¶é¢†åŸŸï¼š{domains_text}
æ ¸å¿ƒä¸»é¢˜ï¼š{themes_text}
å…³é”®è¯ï¼š{', '.join(domain_analysis['top_keywords'][:5])}

ç ”ç©¶å†…å®¹æ‘˜è¦ï¼š
{content[:4000]}

è¯·ç”Ÿæˆæ¶µç›–ä»¥ä¸‹æ·±åº¦å­¦æœ¯ç»´åº¦çš„é—®é¢˜ï¼š
1. ç†è®ºæœºåˆ¶æ¢è®¨ï¼šåˆ†ææ ¸å¿ƒç†è®ºæ¡†æ¶å’Œå› æœæœºåˆ¶
2. æ–¹æ³•è®ºæ‰¹åˆ¤ï¼šè¯„ä»·ç ”ç©¶æ–¹æ³•çš„ç§‘å­¦æ€§å’Œåˆ›æ–°æ€§
3. å®è¯å‘ç°è§£è¯»ï¼šæ·±å…¥è§£è¯»æ•°æ®å‘ç°çš„ç†è®ºæ„ä¹‰
4. æ”¿ç­–å®è·µå…³è”ï¼šæ¢è®¨ç ”ç©¶æˆæœçš„æ”¿ç­–å«ä¹‰å’Œå®è·µä»·å€¼
5. å­¦ç§‘å‰æ²¿å®šä½ï¼šåˆ†æè¯¥ç ”ç©¶åœ¨é¢†åŸŸå‘å±•ä¸­çš„ä½ç½®
6. æœªæ¥ç ”ç©¶æ–¹å‘ï¼šæå‡ºæœ‰è§åœ°çš„åç»­ç ”ç©¶å»ºè®®

é‡è¦è¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šå­¦æœ¯è¯­è¨€ï¼Œé¿å…æåŠå…·ä½“ä½œè€…å§“å
- é¿å…ä½¿ç”¨"æœ¬æ–‡"ã€"æœ¬ç ”ç©¶"ã€"åœ¨XXXçš„ç ”ç©¶ä¸­"ç­‰è¡¨è¿°
- é—®é¢˜è¦ä½“ç°{domains_text}é¢†åŸŸçš„ä¸“ä¸šæ·±åº¦å’Œæ™®é€‚æ€§
- å¼ºè°ƒç†è®ºåˆ†æã€æœºåˆ¶æ¢è®¨å’Œæ‰¹åˆ¤æ€§æ€è€ƒ
- ç­”æ¡ˆåº”å±•ç¤ºä¸¥è°¨çš„å­¦æœ¯æ¨ç†è¿‡ç¨‹ï¼Œä¸ä¾èµ–ç‰¹å®šç ”ç©¶
- é¿å…ç®€å•çš„äº‹å®é™ˆè¿°ï¼Œæ³¨é‡è§£é‡Šå’Œåˆ†æ

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
[{{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}}, {{"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}}]

ç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰æ™®é€‚æ€§çš„æ·±åº¦å­¦æœ¯é—®ç­”å¯¹ï¼š
"""
    elif language == "english":
        prompt_content = f"""
Please generate {num_questions} academically profound questions based on the following research content in the {domains_text} field.

Research Domains: {domains_text}
Core Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:5])}

Content Summary:
{content[:4000]}

Generate questions covering these deep academic dimensions:
1. Theoretical mechanisms: Analyze core theoretical frameworks and causal pathways
2. Methodological critique: Evaluate scientific rigor and innovation of research approaches
3. Empirical findings interpretation: Deeply interpret theoretical significance of data discoveries
4. Policy-practice connections: Discuss policy implications and practical applications
5. Disciplinary positioning: Analyze the research's contribution to field development
6. Future research directions: Propose insightful suggestions for subsequent investigations

CRITICAL REQUIREMENTS:
- Use professional academic discourse, avoid mentioning specific author names
- Refrain from using expressions like "this paper", "this study", "in XXX's research"
- Questions should demonstrate both professional depth in {domains_text} and universal applicability
- Emphasize theoretical analysis, mechanism exploration, and critical thinking
- Answers should exhibit rigorous academic reasoning processes, not relying on specific studies
- Avoid simple factual statements, focus on explanatory and analytical depth

Output in strict JSON format:
[{{"question": "question1", "answer": "answer1"}}, {{"question": "question2", "answer": "answer2"}}]

Generate {num_questions} universally applicable deep academic Q&A pairs:
"""
    else:
        prompt_content = f"""
Please generate {num_questions} bilingual deep academic questions based on this {domains_text} research.

Domains: {domains_text}
Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:5])}

Content:
{content[:4000]}

Generate profound questions covering:
- Theoretical frameworks and causal mechanisms
- Methodological rigor and innovation
- Interpretation of empirical findings
- Policy implications and practical applications
- Positioning in disciplinary development
- Future research trajectories

CRITICAL REQUIREMENTS:
- Generate both English and Chinese versions
- Use professional academic discourse, avoid author-specific references
- Refrain from "this paper/study" expressions in both languages
- Demonstrate deep domain expertise with universal applicability
- Show analytical reasoning in answers, not study-specific facts
- Focus on explanatory depth rather than simple statements

Output in strict JSON format:
[{{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}}]

Generate {num_questions} bilingual universally applicable academic pairs:
"""
    
    return prompt_content

def generate_cross_domain_prompt(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> str:
    """
    ä¸ºå¤šä¸ªæ–‡ä»¶ç”Ÿæˆè·¨é¢†åŸŸæ·±åº¦é—®é¢˜çš„æç¤ºè¯ - è‹±æ–‡ç‰ˆ
    """
    domains_text = 'ã€'.join(domain_analysis['primary_domains'])
    themes_text = 'ã€'.join(domain_analysis['primary_themes'])
    
    # æ„å»ºåˆå¹¶å†…å®¹
    combined_content = ""
    for i, (file_name, content) in enumerate(contents.items(), 1):
        combined_content += f"Research Material {i}:\n{content[:2000]}\n\n"
    
    if language == "chinese":
        prompt_content = f"""
åŸºäºä»¥ä¸‹å¤šä»½{domains_text}é¢†åŸŸçš„ç ”ç©¶èµ„æ–™ï¼Œç”Ÿæˆ{num_questions}ä¸ªå…·æœ‰å­¦æœ¯æ·±åº¦çš„ç»¼åˆæ€§é—®é¢˜ã€‚

ç ”ç©¶é¢†åŸŸï¼š{domains_text}
æ ¸å¿ƒä¸»é¢˜ï¼š{themes_text}
å…³é”®è¯ï¼š{', '.join(domain_analysis['top_keywords'][:8])}
ç ”ç©¶èµ„æ–™æ•°é‡ï¼š{domain_analysis['file_count']}ä»½

ç ”ç©¶èµ„æ–™æ‘˜è¦ï¼š
{combined_content[:6000]}

è¯·ç”Ÿæˆæ¶µç›–ä»¥ä¸‹ç»¼åˆæ€§å­¦æœ¯ç»´åº¦çš„é—®é¢˜ï¼š
1. ç†è®ºæ•´åˆæ¢è®¨ï¼šåˆ†æä¸åŒç ”ç©¶åœ¨ç†è®ºæ¡†æ¶ä¸Šçš„å…±æ€§ä¸å·®å¼‚
2. æ–¹æ³•è®ºæ¯”è¾ƒåæ€ï¼šç»¼åˆè¯„ä»·å„ç§ç ”ç©¶æ–¹æ³•çš„ä¼˜åŠ£å’Œé€‚ç”¨æ€§
3. å®è¯è¯æ®æ±‡èšï¼šæ•´åˆå¤šä¸ªç ”ç©¶çš„å‘ç°ï¼Œæ¢è®¨å…¶ç†è®ºå«ä¹‰
4. æ”¿ç­–å®è·µç»¼åˆï¼šåŸºäºå¤šæºè¯æ®æå‡ºç»¼åˆæ€§çš„æ”¿ç­–å»ºè®®
5. å­¦ç§‘å‘å±•ç ”åˆ¤ï¼šåˆ†æè¯¥é¢†åŸŸç ”ç©¶çš„ç°çŠ¶ã€æŒ‘æˆ˜å’Œå‰æ²¿æ–¹å‘
6. è·¨é¢†åŸŸå¯ç¤ºï¼šæ¢è®¨ç ”ç©¶æˆæœå¯¹å…¶ä»–ç›¸å…³é¢†åŸŸçš„å€Ÿé‰´æ„ä¹‰

é‡è¦è¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šå­¦æœ¯è¯­è¨€ï¼Œé¿å…æåŠå…·ä½“çš„ç ”ç©¶èµ„æ–™ç¼–å·å’Œä½œè€…å§“å
- é—®é¢˜è¦ä½“ç°{domains_text}é¢†åŸŸçš„ç»¼åˆæ€§å’Œæ·±åº¦
- å¼ºè°ƒç†è®ºæ•´åˆã€æ–¹æ³•åæ€å’Œå‰æ²¿å±•æœ›
- ç­”æ¡ˆåº”åŸºäºå¤šä»½ç ”ç©¶èµ„æ–™è¿›è¡Œç»¼åˆæ¨ç†
- é¿å…ç®€å•çš„æ¯”è¾ƒé™ˆè¿°ï¼Œæ³¨é‡æ·±åº¦åˆ†æå’Œç»¼åˆåˆ¤æ–­

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
[{{"question": "é—®é¢˜1", "answer": "ç­”æ¡ˆ1"}}, {{"question": "é—®é¢˜2", "answer": "ç­”æ¡ˆ2"}}]

ç”Ÿæˆ{num_questions}ä¸ªç»¼åˆæ€§æ·±åº¦å­¦æœ¯é—®ç­”å¯¹ï¼š
"""
    elif language == "english":
        prompt_content = f"""
Based on the following multiple research materials in the {domains_text} field, generate {num_questions} comprehensive and profound academic questions.

Research Domains: {domains_text}
Core Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:8])}
Number of Research Materials: {domain_analysis['file_count']}

Research Materials Summary:
{combined_content[:6000]}

Generate questions covering these comprehensive academic dimensions:
1. Theoretical integration: Analyze commonalities and differences in theoretical frameworks across studies
2. Methodological reflection: Comprehensively evaluate strengths and applicability of various research approaches
3. Empirical evidence synthesis: Integrate findings from multiple studies to explore theoretical implications
4. Policy-practice integration: Propose comprehensive policy recommendations based on multiple evidence sources
5. Disciplinary development assessment: Analyze current status, challenges, and frontiers in the field
6. Cross-domain implications: Discuss referential significance of research findings to other related fields

CRITICAL REQUIREMENTS:
- Use professional academic discourse, avoid mentioning specific material numbers or author names
- Questions should demonstrate both comprehensiveness and depth in {domains_text}
- Emphasize theoretical integration, methodological reflection, and frontier outlook
- Answers should be based on comprehensive reasoning from multiple research materials
- Avoid simple comparative statements, focus on deep analysis and synthetic judgment
- Maintain universal applicability while demonstrating domain-specific expertise

Output in strict JSON format:
[{{"question": "question1", "answer": "answer1"}}, {{"question": "question2", "answer": "answer2"}}]

Generate {num_questions} comprehensive deep academic Q&A pairs with universal relevance:
"""
    else:
        prompt_content = f"""
Based on multiple research materials in {domains_text}, generate {num_questions} bilingual comprehensive academic questions.

Domains: {domains_text}
Themes: {themes_text}
Keywords: {', '.join(domain_analysis['top_keywords'][:8])}
Materials: {domain_analysis['file_count']}

Content Summary:
{combined_content[:6000]}

Generate comprehensive questions covering:
- Theoretical integration across studies
- Methodological evaluation and reflection
- Synthesis of empirical evidence
- Integrated policy recommendations
- Assessment of disciplinary development
- Cross-domain implications and insights

CRITICAL REQUIREMENTS:
- Generate both English and Chinese versions
- Use professional integrated academic discourse
- Avoid specific references to materials or authors
- Demonstrate comprehensive domain expertise with universal relevance
- Show synthetic reasoning in answers across multiple sources
- Focus on analytical depth rather than descriptive comparisons

Output in strict JSON format:
[{{"question_en": "q1", "answer_en": "a1", "question_zh": "q1ä¸­æ–‡", "answer_zh": "a1ä¸­æ–‡"}}]

Generate {num_questions} bilingual comprehensive academic pairs with cross-study relevance:
"""
    
    return prompt_content

def enhance_academic_language(qa_pairs: list, language: str, domain_info: Dict = None) -> list:
    """
    å¢å¼ºå­¦æœ¯è¯­è¨€ï¼Œç§»é™¤æ‰€æœ‰äººåå¼•ç”¨å’Œæ¨¡ç³ŠæŒ‡ä»£
    """
    enhanced_pairs = []
    
    # è·å–é¢†åŸŸä¿¡æ¯ç”¨äºå…·ä½“åŒ–è¡¨è¿°
    domains_text = 'ã€'.join(domain_info['primary_domains']) if domain_info else 'ç›¸å…³é¢†åŸŸ'
    
    # ä¸­æ–‡äººåæ¨¡å¼ - æ›´å…¨é¢çš„åŒ¹é…
    chinese_name_patterns = [
        # åŒå­—å§“ + åŒå­—å
        r'[èµµé’±å­™æå‘¨å´éƒ‘ç‹å†¯é™ˆè¤šå«è’‹æ²ˆéŸ©æ¨æœ±ç§¦å°¤è®¸ä½•å•æ–½å¼ å­”æ›¹ä¸¥åé‡‘é­é™¶å§œæˆšè°¢é‚¹å–»æŸæ°´çª¦ç« äº‘è‹æ½˜è‘›å¥šèŒƒå½­éƒé²éŸ¦æ˜Œé©¬è‹—å‡¤èŠ±æ–¹ä¿ä»»è¢æŸ³é…†é²å²å”è´¹å»‰å²‘è–›é›·è´ºå€ªæ±¤æ»•æ®·ç½—æ¯•éƒé‚¬å®‰å¸¸ä¹äºæ—¶å‚…çš®åé½åº·ä¼ä½™å…ƒåœé¡¾å­Ÿå¹³é»„å’Œç©†è§å°¹å§šé‚µæ¹›æ±ªç¥æ¯›ç¦¹ç‹„ç±³è´æ˜è‡§è®¡ä¼æˆæˆ´è°ˆå®‹èŒ…åºç†Šçºªèˆ’å±ˆé¡¹ç¥è‘£æ¢æœé˜®è“é—µå¸­å­£éº»å¼ºè´¾è·¯å¨„å±æ±Ÿç«¥é¢œéƒ­æ¢…ç››æ—åˆé’Ÿå¾é‚±éª†é«˜å¤è”¡ç”°æ¨Šèƒ¡å‡Œéœè™ä¸‡æ”¯æŸ¯æ˜ç®¡å¢è«ç»æˆ¿è£˜ç¼ªå¹²è§£åº”å®—ä¸å®£è´²é‚“éƒå•æ­æ´ªåŒ…è¯¸å·¦çŸ³å´”å‰é’®é¾šç¨‹åµ‡é‚¢æ»‘è£´é™†è£ç¿è€ç¾Šæ–¼æƒ ç”„æ›²å®¶å°èŠ®ç¾¿å‚¨é³æ±²é‚´ç³œæ¾äº•æ®µå¯Œå·«ä¹Œç„¦å·´å¼“ç‰§éš—å±±è°·è½¦ä¾¯å®“è“¬å…¨éƒ—ç­ä»°ç§‹ä»²ä¼Šå®«å®ä»‡æ ¾æš´ç”˜é’­å‰æˆç¥–æ­¦ç¬¦åˆ˜æ™¯è©¹æŸé¾™å¶å¹¸å¸éŸ¶éƒœé»è“Ÿè–„å°å®¿ç™½æ€€è’²é‚°ä»é„‚ç´¢å’¸ç±èµ–å“è”ºå± è’™æ± ä¹”é˜´é¬±èƒ¥èƒ½è‹åŒé—»è˜å…šç¿Ÿè°­è´¡åŠ³é€„å§¬ç”³æ‰¶å µå†‰å®°éƒ¦é›å»ç’©æ¡‘æ¡‚æ¿®ç‰›å¯¿é€šè¾¹æ‰ˆç‡•å†€éƒæµ¦å°šå†œæ¸©åˆ«åº„æ™æŸ´ç¿é˜å……æ…•è¿èŒ¹ä¹ å®¦è‰¾é±¼å®¹å‘å¤æ˜“æ…æˆˆå»–åº¾ç»ˆæš¨å±…è¡¡æ­¥éƒ½è€¿æ»¡å¼˜åŒ¡å›½æ–‡å¯‡å¹¿ç¦„é˜™ä¸œæ¬§æ®³æ²ƒåˆ©è”šè¶Šå¤”éš†å¸ˆå·©åè‚æ™å‹¾æ•–èå†·è¨¾è¾›é˜šé‚£ç®€é¥¶ç©ºæ›¾æ¯‹æ²™ä¹œå…»é é¡»ä¸°å·¢å…³è’¯ç›¸æŸ¥åè†çº¢æ¸¸ç«ºæƒé€¯ç›–ç›Šæ¡“å…¬ä¸‡ä¿Ÿå¸é©¬ä¸Šå®˜æ¬§é˜³å¤ä¾¯è¯¸è‘›é—»äººä¸œæ–¹èµ«è¿çš‡ç”«å°‰è¿Ÿå…¬ç¾Šæ¾¹å°å…¬å†¶å®—æ”¿æ¿®é˜³æ·³äºå•äºå¤ªå”ç”³å± å…¬å­™ä»²å­™è½©è¾•ä»¤ç‹é’Ÿç¦»å®‡æ–‡é•¿å­™æ…•å®¹é²œäºé—¾ä¸˜å¸å¾’å¸ç©ºäº“å®˜å¸å¯‡ä»‰ç£å­è½¦é¢›å­™ç«¯æœ¨å·«é©¬å…¬è¥¿æ¼†é›•ä¹æ­£å£¤é©·å…¬è‰¯æ‹“è·‹å¤¹è°·å®°çˆ¶è°·æ¢æ™‹æ¥šé—«æ³•æ±é„¢æ¶‚é’¦æ®µå¹²ç™¾é‡Œä¸œéƒ­å—é—¨å‘¼å»¶å½’æµ·ç¾ŠèˆŒå¾®ç”Ÿå²³å¸…ç¼‘äº¢å†µåæœ‰ç´æ¢ä¸˜å·¦ä¸˜ä¸œé—¨è¥¿é—¨å•†ç‰Ÿä½˜ä½´ä¼¯èµå—å®«å¢¨å“ˆè°¯ç¬ªå¹´çˆ±é˜³ä½Ÿç¬¬äº”è¨€ç¦][\u4e00-\u9fff]{1,3}',
        # å¸¸è§åŒå­—åæ¨¡å¼
        r'[å¼ ç‹æèµµåˆ˜é™ˆæ¨é»„å‘¨å´å¾å­™èƒ¡æœ±é«˜æ—ä½•éƒ­é©¬ç½—æ¢å®‹éƒ‘è°¢éŸ©å”å†¯äºè‘£è§ç¨‹æ›¹è¢é‚“è®¸å‚…æ²ˆæ›¾å½­å•è‹å¢è’‹è”¡è´¾ä¸é­è–›å¶é˜ä½™æ½˜æœæˆ´å¤é’Ÿæ±ªç”°ä»»å§œèŒƒæ–¹çŸ³å§šè°­å»–é‚¹ç†Šé‡‘é™†éƒå­”ç™½å´”åº·æ¯›é‚±ç§¦æ±Ÿå²é¡¾ä¾¯é‚µå­Ÿé¾™ä¸‡æ®µé›·é’±æ±¤å°¹é»æ˜“å¸¸æ­¦ä¹”è´ºèµ–é¾šæ–‡][\u4e00-\u9fff]{1,2}',
        # è‹±æ–‡åä¸­è¯‘
        r'(å²å¯†æ–¯|çº¦ç¿°é€Š|å¨å»‰æ–¯|å¸ƒæœ—|ç¼æ–¯|ç±³å‹’|æˆ´ç»´æ–¯|åŠ è¥¿äºš|ç½—å¾·é‡Œæ ¼æ–¯|å¨å°”é€Š|é©¬ä¸|å®‰å¾·æ£®|æ³°å‹’|æ‰˜é©¬æ–¯|æ°å…‹é€Š|æ€€ç‰¹|å“ˆé‡Œæ–¯|é©¬ä¸å†…æ–¯|æ±¤æ™®æ£®|ç½—å®¾é€Š|å…‹æ‹‰å…‹|åˆ˜æ˜“æ–¯|æ|æ²ƒå…‹|éœå°”|è‰¾ä¼¦|é‡‘|èµ–ç‰¹|æ´›ä½©æ–¯|å¸Œå°”|æ–¯ç§‘ç‰¹|æ ¼æ—|äºšå½“æ–¯|è´å…‹|å†ˆè¨é›·æ–¯|çº³å°”é€Š|å¡ç‰¹|ç±³åˆ‡å°”|ä½©é›·æ–¯|ç½—ä¼¯èŒ¨|ç‰¹çº³|è²åˆ©æ™®æ–¯|åè´å°”|å¸•å…‹|åŸƒæ–‡æ–¯|çˆ±å¾·åå…¹|æŸ¯æ—æ–¯)',
    ]
    
    # è‹±æ–‡äººåæ¨¡å¼
    english_name_patterns = [
        # å¸¸è§è‹±æ–‡å + å§“æ°
        r'\b(James|John|Robert|Michael|William|David|Richard|Charles|Joseph|Thomas|Christopher|Daniel|Paul|Mark|Donald|George|Kenneth|Steven|Edward|Brian|Ronald|Anthony|Kevin|Jason|Matthew|Gary|Timothy|Jose|Larry|Jeffrey|Frank|Scott|Eric|Stephen|Andrew|Raymond|Gregory|Joshua|Jerry|Dennis|Walter|Patrick|Peter|Harold|Douglas|Henry|Carl|Arthur|Ryan|Roger|Joe|Juan|Jack|Albert|Jonathan|Justin|Terry|Gerald|Keith|Samuel|Willie|Ralph|Lawrence|Nicholas|Roy|Benjamin|Bruce|Brandon|Adam|Harry|Fred|Wayne|Billy|Steve|Louis|Jeremy|Aaron|Randy|Howard|Eugene|Carlos|Russell|Bobby|Victor|Martin|Ernest|Phillip|Todd|Jesse|Craig|Alan|Shawn|Clarence|Sean|Philip|Chris|Johnny|Earl|Jimmy|Antonio|Danny|Bryan|Tony|Luis|Mike|Stanley|Leonard|Nathan|Dale|Manuel|Rodney|Curtis|Norman|Allen|Marvin|Vincent|Glenn|Jeffery|Travis|Jeff|Chad|Jacob|Lee|Melvin|Alfred|Kyle|Francis|Bradley|Jesus|Herbert|Frederick|Ray|Joel|Edwin|Don|Eddie|Ricky|Troy|Randall|Barry|Alexander|Bernard|Mario|Leroy|Francisco|Marcus|Micheal|Theodore|Clifford|Miguel|Oscar|Jay|Jim|Tom|Calvin|Alex|Jon|Ronnie|Bill|Lloyd|Tommy|Leon|Derek|Warren|Darrell|Jerome|Floyd|Leo|Alvin|Tim|Wesley|Gordon|Dean|Greg|Jorge|Dustin|Pedro|Derrick|Dan|Lewis|Zachary|Corey|Herman|Maurice|Vernon|Roberto|Clyde|Glen|Hector|Shane|Ricardo|Sam|Rick|Lester|Brent|Ramon|Charlie|Tyler|Gilbert|Gene|Marc|Reginald|Ruben|Brett|Angel|Nathaniel|Rafael|Leslie|Edgar|Milton|Raul|Ben|Chester|Cecil|Duane|Franklin|Andre|Elmer|Brad|Gabriel|Ron|Mitchell|Roland|Arnold|Harvey|Jared|Adrian|Karl|Cory|Claude|Erik|Darryl|Jamie|Neil|Jessie|Christian|Javier|Fernando|Clinton|Ted|Mathew|Tyrone|Darren|Lonnie|Lance|Cody|Julio|Kelly|Kurt|Allan|Nelson|Guy|Clayton|Hugh|Max|Dwayne|Dwight|Armando|Felix|Jimmie|Everett|Jordan|Ian|Wallace|Ken|Bob|Jaime|Casey|Alfredo|Alberto|Dave|Ivan|Johnnie|Sidney|Byron|Julian|Isaac|Morris|Clifton|Willard|Daryl|Ross|Virgil|Andy|Marshall|Salvador|Perry|Kirk|Sergio|Marion|Tracy|Seth|Kent|Terrance|Rene|Eduardo|Terrence|Enrique|Freddie|Wade|Austin|Stuart|Fredrick|Arturo|Alejandro|Jackie|Joey|Nick|Luther|Wendell|Jeremiah|Evan|Julius|Dana|Donnie|Otis|Shannon|Trevor|Oliver|Luke|Homer|Gerard|Doug|Kenny|Hubert|Angelo|Shaun|Lyle|Matt|Lynn|Alfonso|Orlando|Rex|Carlton|Ernesto|Cameron|Neal|Pablo|Lorenzo|Omar|Wilbur|Blake|Grant|Horace|Roderick|Kerry|Abraham|Willis|Rickey|Jean|Ira|Andres|Cesar|Johnathan|Malcolm|Rudolph|Damon|Kelvin|Rudy|Preston|Alton|Archie|Marco|Wm|Pete|Randolph|Garry|Geoffrey|Jonathon|Felipe|Bennie|Gerardo|Ed|Dominic|Robin|Loren|Delbert|Colin|Guillermo|Earnest|Lucas|Benny|Noel|Spencer|Rodolfo|Myron|Edmund|Garrett|Salvatore|Cedric|Lowell|Gregg|Sherman|Wilson|Devin|Sylvester|Kim|Roosevelt|Israel|Jermaine|Forrest|Wilbert|Leland|Simon|Guadalupe|Clark|Irving|Carroll|Bryant|Owen|Rufus|Woodrow|Sammy|Kristopher|Mack|Levi|Marcos|Gustavo|Jake|Lionel|Marty|Taylor|Ellis|Dallas|Gilberto|Clint|Nicolas|Laurence|Ismael|Orville|Drew|Jody|Ervin|Dewey|Al|Wilfred|Josh|Hugo|Ignacio|Caleb|Tomas|Sheldon|Erick|Frankie|Stewart|Doyle|Darrel|Rogelio|Terence|Santiago|Alonzo|Elias|Bert|Elbert|Ramiro|Conrad|Pat|Noah|Grady|Phil|Cornelius|Lamar|Rolando|Clay|Percy|Dexter|Bradford|Merle|Darin|Amos|Terrell|Moses|Irvin|Saul|Roman|Darnell|Randal|Tommie|Timmy|Darrin|Winston|Brendan|Toby|Van|Abel|Dominick|Boyd|Courtney|Jan|Emilio|Elijah|Cary|Domingo|Santos|Aubrey|Emmett|Marlon|Emanuel|Jerald|Edmond|Emil|Dewayne|Will|Otto|Teddy|Reynaldo|Bret|Morgan|Jess|Trent|Humberto|Emmanuel|Stephan|Louie|Vicente|Lamont|Stacy|Garland|Miles|Micah|Efrain|Billie|Logan|Heath|Rodger|Harley|Demetrius|Ethan|Eldon|Rocky|Pierre|Junior|Freddy|Eli|Bryce|Antoine|Robbie|Kendall|Royce|Sterling|Mickey|Chase|Grover|Elton|Cleveland|Dylan|Chuck|Damian|Reuben|Stan|August|Leonardo|Jasper|Russel|Erwin|Benito|Hans|Monte|Blaine|Ernie|Curt|Quentin|Agustin|Murray|Jamal|Devon|Adolfo|Harrison|Tyson|Burton|Brady|Elliott|Wilfredo|Bart|Jarrod|Vance|Denis|Damien|Joaquin|Harlan|Desmond|Elliot|Darwin|Ashley|Gregorio|Buddy|Xavier|Kermit|Roscoe|Esteban|Anton|Solomon|Scotty|Norbert|Elvin|Williams|Nolan|Carey|Rod|Quinton|Hal|Brain|Rob|Elwood|Kendrick|Darius|Moises|Son|Marlin|Fidel|Thaddeus|Cliff|Marcel|Ali|Jackson|Raphael|Bryon|Armand|Alvaro|Jeffry|Dane|Joesph|Thurman|Ned|Sammie|Rusty|Michel|Monty|Rory|Fabian|Reggie|Mason|Graham|Kris|Isaiah|Vaughn|Gus|Avery|Loyd|Diego|Alexis|Adolph|Norris|Millard|Rocco|Gonzalo|Derick|Rodrigo|Gerry|Stacey|Carmen|Wiley|Rigoberto|Alphonso|Ty|Shelby|Rickie|Noe|Vern|Bobbie|Reed|Jefferson|Elvis|Bernardo|Mauricio|Hiram|Donovan|Basil|Riley|Ollie|Nickolas|Maynard|Scot|Vince|Quincy|Eddy|Sebastian|Federico|Ulysses|Heriberto|Donnell|Cole|Denny|Davis|Gavin|Emery|Ward|Romeo|Jayson|Dion|Dante|Clement|Coy|Odell|Maxwell|Jarvis|Bruno|Issac|Mary|Patricia|Linda|Barbara|Elizabeth|Jennifer|Maria|Susan|Margaret|Dorothy|Lisa|Nancy|Karen|Betty|Helen|Sandra|Donna|Carol|Ruth|Sharon|Michelle|Laura|Sarah|Kimberly|Deborah|Jessica|Shirley|Cynthia|Angela|Melissa|Brenda|Amy|Anna|Rebecca|Virginia|Kathleen|Pamela|Martha|Debra|Amanda|Stephanie|Carolyn|Christine|Marie|Janet|Catherine|Frances|Ann|Joyce|Diane|Alice|Julie|Heather|Teresa|Doris|Gloria|Evelyn|Jean|Cheryl|Mildred|Katherine|Joan|Ashley|Judith|Rose|Janice|Kelly|Nicole|Judy|Christina|Kathy|Theresa|Beverly|Denise|Tammy|Irene|Jane|Lori|Rachel|Marilyn|Andrea|Kathryn|Louise|Sara|Anne|Jacqueline|Wanda|Bonnie|Julia|Ruby|Lois|Tina|Phyllis|Norma|Paula|Diana|Annie|Lillian|Emily|Robin|Peggy|Crystal|Gladys|Rita|Dawn|Connie|Florence|Tracy|Edna|Tiffany|Carmen|Rosa|Cindy|Grace|Wendy|Victoria|Michele|Tamara|Jessica|Sherry|Sylvia|Josephine|Thelma|Shannon|Sheila|Ethel|Ellen|Elaine|Marjorie|Carrie|Charlotte|Monica|Esther|Pauline|Emma|Juanita|Anita|Rhonda|Hazel|Amber|Eva|Debbie|April|Leslie|Clara|Lucille|Jamie|Joanne|Eleanor|Valerie|Danielle|Megan|Alicia|Suzanne|Michele|Gail|Bertha|Darlene|Veronica|Jill|Erin|Geraldine|Lauren|Cathy|Joann|Lorraine|Lynn|Sally|Regina|Erica|Beatrice|Dolores|Bernice|Audrey|Yvonne|Annette|June|Samantha|Marion|Dana|Stacy|Ana|Renee|Ida|Vivian|Roberta|Holly|Brittany|Melanie|Loretta|Yolanda|Jeanette|Laurie|Katie|Kristen|Vanessa|Alma|Sue|Elsie|Beth|Jeanne|Vicki|Carla|Tara|Rosemary|Eileen|Terri|Gertrude|Lucy|Tonya|Ella|Stacey|Wilma|Gina|Kristin|Jessie|Natalie|Agnes|Vera|Willie|Charlene|Bessie|Delores|Melinda|Pearl|Arlene|Maureen|Colleen|Allison|Tamara|Joy|Georgia|Constance|Lillie|Claudia|Jackie|Marcia|Tanya|Nellie|Minnie|Marlene|Heidi|Glenda|Lydia|Viola|Courtney|Marian|Stella|Caroline|Dora|Jo|Vickie|Mattie|Terry|Maxine|Irma|Mabel|Marsha|Myrtle|Lena|Christy|Deanna|Patsy|Hilda|Gwendolyn|Jennie|Nora|Margie|Nina|Cassandra|Leah|Penny|Kay|Priscilla|Naomi|Carole|Brandy|Olga|Billie|Dianne|Tracey|Leona|Jenny|Felicia|Sonia|Miriam|Velma|Becky|Bobbie|Violet|Kristina|Toni|Misty|Mae|Shelly|Daisy|Ramona|Sherri|Erika|Katrina|Claire|Lindsey|Lindsay|Geneva|Guadalupe|Belinda|Margarita|Sheryl|Cora|Faye|Ada|Natasha|Sabrina|Isabel|Marguerite|Hattie|Harriet|Molly|Cecilia|Kristi|Brandi|Blanche|Sandy|Rosie|Joanna|Iris|Eunice|Angie|Inez|Lynda|Madeline|Amelia|Alberta|Genevieve|Monique|Jodi|Janie|Maggie|Kayla|Sonya|Jan|Lee|Kristine|Candace|Fannie|Maryann|Opal|Alison|Yvette|Melody|Luz|Susie|Olivia|Flora|Shelley|Kristy|Mamie|Lula|Lola|Verna|Beulah|Antoinette|Candice|Juana|Jeannette|Pam|Kelli|Hannah|Whitney|Bridget|Karla|Celia|Latoya|Patty|Shelia|Gayle|Della|Vicky|Lynne|Sheri|Marianne|Kara|Jacquelyn|Erma|Blanca|Myra|Leticia|Pat|Krista|Roxanne|Angelica|Johnnie|Robyn|Francis|Adrienne|Rosalie|Alexandra|Brooke|Bethany|Sadie|Bernadette|Traci|Jody|Kendra|Jasmine|Nichole|Rachael|Chelsea|Mable|Ernestine|Muriel|Marcella|Elena|Krystal|Angelina|Nadine|Kari|Estelle|Dianna|Paulette|Lora|Mona|Doreen|Rosemarie|Angel|Desiree|Antonia|Hope|Ginger|Janis|Betsy|Christie|Freda|Mercedes|Meredith|Lynette|Teri|Cristina|Eula|Leigh|Meghan|Sophia|Eloise|Rochelle|Gretchen|Cecelia|Raquel|Henrietta|Alyssa|Jana|Kelley|Gwen|Kerry|Jenna|Tricia|Laverne|Olive|Alexis|Tasha|Silvia|Elvira|Casey|Delia|Sophie|Kate|Patti|Lorena|Kellie|Sonja|Lila|Lana|Darla|May|Mindy|Essie|Mandy|Lorene|Elsa|Josefina|Jeannie|Miranda|Dixie|Lucia|Marta|Faith|Lela|Johanna|Shari|Camille|Tami|Shawna|Elisa|Ebony|Melba|Ora|Nettie|Tabitha|Ollie|Jaime|Winifred|Kristie)\s+[A-Z][a-z]+',
        # å§“æ°å•ç‹¬å‡ºç°
        r'\b(Smith|Johnson|Williams|Brown|Jones|Garcia|Miller|Davis|Rodriguez|Martinez|Hernandez|Lopez|Gonzalez|Wilson|Anderson|Thomas|Taylor|Moore|Jackson|Martin|Lee|Perez|Thompson|White|Harris|Sanchez|Clark|Ramirez|Lewis|Robinson|Walker|Young|Allen|King|Wright|Scott|Torres|Nguyen|Hill|Flores|Green|Adams|Nelson|Baker|Hall|Rivera|Campbell|Mitchell|Carter|Roberts|Gomez|Phillips|Evans|Turner|Diaz|Parker|Cruz|Edwards|Collins|Reyes|Stewart|Morris|Morales|Murphy|Cook|Rogers|Gutierrez|Ortiz|Morgan|Cooper|Peterson|Bailey|Reed|Kelly|Howard|Ramos|Kim|Cox|Ward|Richardson|Watson|Brooks|Chavez|Wood|James|Bennett|Gray|Mendoza|Ruiz|Hughes|Price|Alvarez|Castillo|Sanders|Patel|Myers|Long|Ross|Foster|Jimenez|Powell|Jenkins|Perry|Russell|Sullivan|Bell|Coleman|Butler|Henderson|Barnes|Gonzales|Fisher|Vasquez|Simmons|Romero|Jordan|Patterson|Alexander|Hamilton|Graham|Reynolds|Griffin|Wallace|Moreno|West|Cole|Hayes|Bryant|Herrera|Gibson|Ellis|Tran|Medina|Aguilar|Stevens|Murray|Ford|Castro|Marshall|Owens|Harrison|Fernandez|Mcdonald|Woods|Washington|Kennedy|Wells|Vargas|Henry|Chen|Freeman|Webb|Tucker|Guzman|Burns|Crawford|Olson|Simpson|Porter|Hunter|Gordon|Mendez|Silva|Shaw|Snyder|Mason|Dixon|Munoz|Hunt|Hicks|Holmes|Palmer|Wagner|Black|Robertson|Boyd|Rose|Stone|Salazar|Fox|Warren|Mills|Meyer|Rice|Schmidt|Garza|Daniels|Ferguson|Nichols|Stephens|Soto|Weaver|Ryan|Gardner|Payne|Grant|Dunn|Kelley|Spencer|Hawkins|Arnold|Pierce|Austin|Peters|Hawkins|Hoffman|Johnston|Matthews|Wagner|Willis|Ray|Watkins|Olson|Carroll|Duncan|Snyder|Hart|Cunningham|Bradley|Lane|Andrews|Ruiz|Harper|Fox|Riley|Armstrong|Carpenter|Weaver|Greene|Lawrence|Elliott|Chavez|Sims|Austin|Peters|Kelley|Franklin|Lawson|Fields|Gutierrez|Ryan|Schmidt|Carr|Vasquez|Castillo|Wheeler|Chapman|Oliver|Montgomery|Richards|Williamson|Johnston|Banks|Meyer|Bishop|Mccoy|Howell|Alvarez|Morrison|Hansen|Fernandez|Garza|Harvey|Little|Burton|Stanley|Nguyen|George|Jacobs|Reid|Kim|Fuller|Lynch|Dean|Gilbert|Garrett|Romero|Welch|Larson|Frazier|Burke|Hanson|Day|Mendoza|Moreno|Bowman|Medina|Fowler|Brewer|Hoffman|Carlson|Silva|Pearson|Holland|Douglas|Fleming|Jensen|Vargas|Byrd|Davidson|Hopkins|May|Terry|Herrera|Wade|Soto|Walters|Curtis|Neal|Caldwell|Lowe|Jennings|Barnett|Graves|Jimenez|Horton|Shelton|Barrett|Obrien|Castro|Sutton|Gregory|Mckinney|Lucas|Miles|Craig|Rodriquez|Chambers|Holt|Lambert|Fletcher|Watts|Bates|Hale|Rhodes|Pena|Beck|Newman|Haynes|Mcdaniel|Mendez|Bush|Vaughn|Parks|Dawson|Santiago|Norris|Hardy|Love|Steele|Curry|Powers|Schultz|Barker|Guzman|Page|Munoz|Ball|Keller|Chandler|Weber|Leonard|Walsh|Lyons|Ramsey|Wolfe|Schneider|Mullins|Benson|Sharp|Bowen|Daniel|Barber|Cummings|Hines|Baldwin|Griffith|Valdez|Hubbard|Salazar|Reeves|Warner|Stevenson|Burgess|Santos|Tate|Cross|Garner|Mann|Mack|Moss|Thornton|Dennis|Mcgee|Farmer|Delgado|Aguilar|Vega|Glover|Manning|Cohen|Harmon|Rodgers|Robbins|Newton|Todd|Blair|Higgins|Ingram|Reese|Cannon|Strickland|Townsend|Potter|Goodwin|Walton|Rowe|Hampton|Ortega|Patton|Swanson|Joseph|Francis|Goodman|Maldonado|Yates|Becker|Erickson|Hodges|Rios|Conner|Adkins|Webster|Norman|Malone|Hammond|Flowers|Cobb|Moody|Quinn|Blake|Maxwell|Pope|Floyd|Osborne|Paul|Mccarthy|Guerrero|Lindsey|Estrada|Sandoval|Gibbs|Tyler|Gross|Fitzgerald|Stokes|Doyle|Sherman|Saunders|Wise|Colon|Gill|Alvarado|Greer|Padilla|Simon|Waters|Nunez|Ballard|Schwartz|Mcbride|Houston|Christensen|Klein|Pratt|Briggs|Parsons|Mclaughlin|Zimmerman|French|Buchanan|Moran|Copeland|Roy|Pittman|Brady|Mccormick|Holloway|Brock|Poole|Frank|Logan|Owen|Bass|Marsh|Drake|Wong|Jefferson|Park|Morton|Abbott|Sparks|Patrick|Norton|Huff|Clayton|Massey|Lloyd|Figueroa|Carson|Bowers|Roberson|Barton|Tran|Lamb|Harrington|Casey|Boone|Cortez|Clarke|Mathis|Singleton|Wilkins|Cain|Bryan|Underwood|Hogan|Mckenzie|Collier|Luna|Phelps|Mcguire|Allison|Bridges|Wilkerson|Nash|Summers|Atkins|Wilcox|Pitts|Conley|Marquez|Burnett|Richard|Cochran|Chase|Davenport|Hood|Gates|Clay|Ayala|Sawyer|Roman|Vazquez|Dickerson|Hodge|Acosta|Flynn|Espinoza|Nicholson|Monroe|Wolf|Morrow|Kirk|Randall|Anthony|Whitaker|Oconnor|Skinner|Ware|Molina|Kirby|Huffman|Bradford|Charles|Gilmore|Dominguez|Oneal|Bruce|Lang|Combs|Kramer|Heath|Hancock|Gallagher|Gaines|Shaffer|Short|Wiggins|Mathews|Mcclain|Fischer|Wall|Small|Melton|Hensley|Bond|Dyer|Cameron|Grimes|Contreras|Christian|Wyatt|Baxter|Snow|Mosley|Shepherd|Larsen|Hoover|Beasley|Glenn|Petersen|Whitehead|Meyers|Keith|Garrison|Vincent|Shields|Horn|Savage|Olsen|Schroeder|Hartman|Woodard|Mueller|Kemp|Deleon|Booth|Patel|Calhoun|Wiley|Eaton|Cline|Navarro|Harrell|Lester|Humphrey|Parrish|Duran|Hutchinson|Hess|Dorsey|Bullock|Robles|Beard|Dalton|Avila|Vance|Rich|Blackwell|York|Johns|Blankenship|Trevino|Salinas|Campos|Pruitt|Moses|Callahan|Golden|Montoya|Hardin|Guerra|Mcdowell|Carey|Stafford|Gallegos|Henson|Wilkinson|Booker|Merritt|Miranda|Atkinson|Orr|Decker|Hobbs|Preston|Tanner|Knox|Pacheco|Stephenson|Glass|Rojas|Serrano|Marks|Hickman|English|Sweeney|Strong|Prince|Mcclure|Conway|Walter|Roth|Maynard|Farrell|Lowery|Hurst|Nixon|Weiss|Trujillo|Ellison|Sloan|Juarez|Winters|Mclean|Randolph|Leon|Boyer|Villarreal|Mccall|Gentry|Carrillo|Kent|Ayers|Lara|Shannon|Sexton|Pace|Hull|Leblanc|Browning|Velasquez|Leach|Chang|House|Sellers|Herring|Noble|Foley|Bartlett|Mercado|Landry|Durham|Walls|Barr|Mckee|Bauer|Rivers|Everett|Bradshaw|Pugh|Velez|Rush|Estes|Dodson|Morse|Sheppard|Weeks|Camacho|Bean|Barron|Livingston|Middleton|Spears|Branch|Blevins|Chen|Kerr|Mcconnell|Hatfield|Harding|Ashley|Solis|Herman|Frost|Giles|Blackburn|William|Pennington|Woodward|Finley|Mcintosh|Koch|Best|Solomon|Mccullough|Dudley|Nolan|Blanchard|Rivas|Brennan|Mejia|Kane|Benton|Joyce|Buckley|Haley|Valentine|Maddox|Russo|Mcknight|Buck|Moon|Mcmillan|Crosby|Berg|Dotson|Mays|Roach|Cochran|Craft|Cleveland|Clemons|Wynn|Nielsen|Baird|Stanton|Snider|Rosales|Bright|Witt|Stuart|Hays|Holden|Rutledge|Kinney|Clements|Castaneda|Slater|Hahn|Emerson|Conrad|Burks|Delaney|Pate|Lancaster|Sweet|Justice|Tyson|Sharpe|Whitfield|Talley|Macias|Irwin|Burris|Ratliff|Mccray|Madden|Kaufman|Beach|Goff|Cash|Bolton|Mcfadden|Levine|Good|Byers|Kirkland|Kidd|Workman|Carney|Dale|Mcleod|Holcomb|England|Finch|Head|Burt|Hendrix|Sosa|Haney|Franks|Sargent|Nieves|Downs|Rasmussen|Bird|Hewitt|Cardenas|Fulton|Lynn|Lott|Calderon|Rosa|Pollard|Hooper|Burch|Mullen|Fry|Riddle|Levy|David|Duke|Odonnell|Guy|Michael|Britt|Frederick|Daugherty|Berger|Dillard|Alston|Jarvis|Frye|Riggs|Chaney|Odom|Duffy|Fitzpatrick|Valenzuela|Merrill|Mayer|Alford|Mcpherson|Acevedo|Donovan|Barrera|Albert|Cote|Reilly|Compton|Raymond|Mooney|Mcgowan|Craft|Cleveland|Clemons|Wynn|Nielsen|Baird|Stanton|Snider|Rosales|Bright|Witt|Stuart|Hays|Holden|Rutledge|Kinney|Clements|Castaneda|Slater|Hahn|Emerson|Conrad|Burks|Delaney|Pate|Lancaster|Sweet|Justice|Tyson|Sharpe|Whitfield|Talley|Macias|Irwin|Burris|Ratliff|Mccray|Madden|Kaufman|Beach|Goff|Cash|Bolton|Mcfadden|Levine|Good|Byers|Kirkland|Kidd|Workman|Carney|Dale|Mcleod|Holcomb|England|Finch|Head|Burt|Hendrix|Sosa|Haney|Franks|Sargent|Nieves|Downs|Rasmussen|Bird|Hewitt|Cardenas|Fulton|Lynn|Lott|Calderon|Rosa|Pollard|Hooper|Burch|Mullen|Fry|Riddle|Levy|David|Duke|Odonnell|Guy|Michael|Britt|Frederick|Daugherty|Berger|Dillard|Alston|Jarvis|Frye|Riggs|Chaney|Odom|Duffy|Fitzpatrick|Valenzuela|Merrill|Mayer|Alford|Mcpherson|Acevedo|Donovan|Barrera|Albert|Cote|Reilly|Compton|Raymond|Mooney|Mcgowan|Craft|Cleveland|Clemons|Wynn|Nielsen|Baird|Stanton|Snider|Rosales|Bright|Witt|Stuart|Hays|Holden|Rutledge|Kinney|Clements|Castaneda|Slater|Hahn|Emerson|Conrad|Burks|Delaney|Pate|Lancaster|Sweet|Justice|Tyson|Sharpe|Whitfield|Talley|Macias|Irwin|Burris|Ratliff|Mccray|Madden|Kaufman|Beach|Goff|Cash|Bolton|Mcfadden|Levine|Good|Byers|Kirkland|Kidd|Workman|Carney|Dale|Mcleod|Holcomb|England|Finch|Head|Burt|Hendrix|Sosa|Haney|Franks|Sargent|Nieves|Downs|Rasmussen|Bird|Hewitt|Cardenas|Fulton|Lynn|Lott|Calderon|Rosa|Pollard|Hooper|Burch|Mullen|Fry|Riddle|Levy|David|Duke|Odonnell|Guy|Michael|Britt|Frederick|Daugherty|Berger|Dillard|Alston|Jarvis|Frye|Riggs|Chaney|Odom|Duffy|Fitzpatrick|Valenzuela|Merrill|Mayer|Alford|Mcpherson|Acevedo|Donovan|Barrera|Albert|Cote|Reilly|Compton|Raymond|Mooney|Mcgowan)\b',
    ]
    
    for pair in qa_pairs:
        if language == "english":
            question = pair.get('question', '')
            answer = pair.get('answer', '')
            
            # ç§»é™¤è‹±æ–‡äººåå¼•ç”¨
            for pattern in english_name_patterns:
                question = re.sub(pattern, 'research', question, flags=re.IGNORECASE)
                answer = re.sub(pattern, 'studies', answer, flags=re.IGNORECASE)
            
            # ç§»é™¤"in XXX and XXX's research"ç­‰æ¨¡å¼
            question = re.sub(r'\b(in|according to|based on)\s+[A-Z][a-z]+\s+and\s+[A-Z][a-z]+(\'s)?\s+(research|study|work)\b', 
                            'in the research literature', question, flags=re.IGNORECASE)
            question = re.sub(r'\b[A-Z][a-z]+\s+and\s+[A-Z][a-z]+\s+found\b', 'research has found', question, flags=re.IGNORECASE)
            
            answer = re.sub(r'\b(in|according to|based on)\s+[A-Z][a-z]+\s+and\s+[A-Z][a-z]+(\'s)?\s+(research|study|work)\b', 
                          'based on research', answer, flags=re.IGNORECASE)
            answer = re.sub(r'\b[A-Z][a-z]+\s+and\s+[A-Z][a-z]+\s+(found|showed|demonstrated)\b', 'studies have shown', answer, flags=re.IGNORECASE)
            
            # ä½¿ç”¨å…·ä½“é¢†åŸŸæœ¯è¯­æ›¿ä»£æ¨¡ç³ŠæŒ‡ä»£
            question = re.sub(r'\bthis (paper|study|research)\b', f'the {domains_text} literature', question, flags=re.IGNORECASE)
            question = re.sub(r'\bthe (paper|study)\b', f'the {domains_text} research', question, flags=re.IGNORECASE)
            
            answer = re.sub(r'\bthis (paper|study|research)\b', f'the {domains_text} literature', answer, flags=re.IGNORECASE)
            answer = re.sub(r'\bthe (paper|study)\b', f'the {domains_text} research', answer, flags=re.IGNORECASE)
            
            enhanced_pair = {
                "question": question,
                "answer": answer,
                "metadata": pair.get("metadata", {})
            }
            
        elif language == "chinese":
            question = pair.get('question', '')
            answer = pair.get('answer', '')
            
            # ç§»é™¤ä¸­æ–‡äººåå¼•ç”¨
            for pattern in chinese_name_patterns:
                question = re.sub(pattern, 'ç›¸å…³ç ”ç©¶', question)
                answer = re.sub(pattern, 'å®è¯åˆ†æ', answer)
            
            # ç§»é™¤å…·ä½“çš„ä½œè€…å¼•ç”¨æ¨¡å¼
            question = re.sub(r'åœ¨[^ï¼Œã€‚]*?(ç ”ç©¶|åˆ†æ|æ¢è®¨|è€ƒå¯Ÿ)ä¸­', 'åœ¨ç›¸å…³ç ”ç©¶ä¸­', question)
            question = re.sub(r'[^ï¼Œã€‚]*?ç­‰?(?:å‘ç°|è¡¨æ˜|è¯æ˜|æŒ‡å‡º)', 'ç ”ç©¶æ˜¾ç¤º', question)
            question = re.sub(r'[^ï¼Œã€‚]*?çš„(?:ç ”ç©¶|åˆ†æ|æ¢è®¨)', 'ç›¸å…³ç ”ç©¶', question)
            
            answer = re.sub(r'åœ¨[^ï¼Œã€‚]*?(ç ”ç©¶|åˆ†æ|æ¢è®¨|è€ƒå¯Ÿ)ä¸­', 'ç ”ç©¶æ˜¾ç¤º', answer)
            answer = re.sub(r'[^ï¼Œã€‚]*?ç­‰?(?:å‘ç°|è¡¨æ˜|è¯æ˜|æŒ‡å‡º)', 'å®è¯åˆ†æè¡¨æ˜', answer)
            answer = re.sub(r'[^ï¼Œã€‚]*?çš„(?:ç ”ç©¶|åˆ†æ|æ¢è®¨)', 'ç›¸å…³ç ”ç©¶', answer)
            
            # ä½¿ç”¨å…·ä½“é¢†åŸŸæœ¯è¯­æ›¿ä»£æ¨¡ç³ŠæŒ‡ä»£
            question = re.sub(r'æœ¬æ–‡', f'{domains_text}æ–‡çŒ®', question)
            question = re.sub(r'æœ¬ç ”ç©¶', f'{domains_text}ç ”ç©¶', question)
            question = re.sub(r'è¯¥ç ”ç©¶', f'{domains_text}é¢†åŸŸ', question)
            
            answer = re.sub(r'æœ¬æ–‡', f'{domains_text}æ–‡çŒ®', answer)
            answer = re.sub(r'æœ¬ç ”ç©¶', f'{domains_text}ç ”ç©¶', answer)
            answer = re.sub(r'è¯¥ç ”ç©¶', f'{domains_text}é¢†åŸŸ', answer)
            
            enhanced_pair = {
                "question": question,
                "answer": answer,
                "metadata": pair.get("metadata", {})
            }
        else:  # bilingual
            question_en = pair.get('question_en', '')
            question_zh = pair.get('question_zh', '')
            answer_en = pair.get('answer_en', '')
            answer_zh = pair.get('answer_zh', '')
            
            # è‹±æ–‡æ¸…ç†
            for pattern in english_name_patterns:
                question_en = re.sub(pattern, 'research', question_en, flags=re.IGNORECASE)
                answer_en = re.sub(pattern, 'studies', answer_en, flags=re.IGNORECASE)
            
            # ä¸­æ–‡æ¸…ç†
            for pattern in chinese_name_patterns:
                question_zh = re.sub(pattern, 'ç›¸å…³ç ”ç©¶', question_zh)
                answer_zh = re.sub(pattern, 'å®è¯åˆ†æ', answer_zh)
            
            # ç§»é™¤ä¸­è‹±æ–‡æ··åˆçš„äººåå¼•ç”¨
            question_en = re.sub(r'\b(in|according to|based on)\s+[A-Z][a-z]+\s+and\s+[A-Z][a-z]+(\'s)?\s+(research|study|work)\b', 
                               'in the research literature', question_en, flags=re.IGNORECASE)
            answer_en = re.sub(r'\b(in|according to|based on)\s+[A-Z][a-z]+\s+and\s+[A-Z][a-z]+(\'s)?\s+(research|study|work)\b', 
                             'based on research', answer_en, flags=re.IGNORECASE)
            
            question_zh = re.sub(r'åœ¨[^ï¼Œã€‚]*?(ç ”ç©¶|åˆ†æ|æ¢è®¨|è€ƒå¯Ÿ)ä¸­', 'åœ¨ç›¸å…³ç ”ç©¶ä¸­', question_zh)
            answer_zh = re.sub(r'åœ¨[^ï¼Œã€‚]*?(ç ”ç©¶|åˆ†æ|æ¢è®¨|è€ƒå¯Ÿ)ä¸­', 'ç ”ç©¶æ˜¾ç¤º', answer_zh)
            
            enhanced_pair = {
                "question_en": question_en,
                "answer_en": answer_en,
                "question_zh": question_zh,
                "answer_zh": answer_zh,
                "metadata": pair.get("metadata", {})
            }
        
        enhanced_pairs.append(enhanced_pair)
    
    return enhanced_pairs

def generate_single_file_questions(content: str, file_name: str, num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    ä¸ºå•ä¸ªæ–‡ä»¶ç”Ÿæˆæ·±åº¦å­¦æœ¯é—®é¢˜
    """
    print(f"ğŸ“– ä¸º {file_name} ç”Ÿæˆ {num_questions} ä¸ªæ·±åº¦å­¦æœ¯é—®é¢˜")
    
    # ç”Ÿæˆå•ä¸ªæ–‡ä»¶çš„æç¤ºè¯
    prompt = generate_single_domain_prompt(content, file_name, num_questions, language, domain_analysis)
    
    qa_pairs = _call_deepseek_api_with_retry(prompt, language)
    
    if qa_pairs:
        # å¢å¼ºå­¦æœ¯è¯­è¨€ï¼Œä¼ å…¥é¢†åŸŸä¿¡æ¯
        enhanced_pairs = enhance_academic_language(qa_pairs, language, domain_analysis)
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(enhanced_pairs)} ä¸ªæ·±åº¦å­¦æœ¯é—®ç­”å¯¹")
        return enhanced_pairs[:num_questions]
    else:
        print("âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥")
        return []

def generate_cross_file_questions(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    ä¸ºå¤šä¸ªæ–‡ä»¶ç”Ÿæˆç»¼åˆæ€§æ·±åº¦é—®é¢˜
    """
    print(f"ğŸ”— åŸºäº {len(contents)} ä¸ªæ–‡ä»¶ç”Ÿæˆ {num_questions} ä¸ªç»¼åˆæ€§æ·±åº¦é—®é¢˜")
    
    # ç”Ÿæˆè·¨æ–‡ä»¶æç¤ºè¯
    prompt = generate_cross_domain_prompt(contents, num_questions, language, domain_analysis)
    
    qa_pairs = _call_deepseek_api_with_retry(prompt, language)
    
    if qa_pairs:
        # å¢å¼ºå­¦æœ¯è¯­è¨€ï¼Œä¼ å…¥é¢†åŸŸä¿¡æ¯
        enhanced_pairs = enhance_academic_language(qa_pairs, language, domain_analysis)
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(enhanced_pairs)} ä¸ªç»¼åˆæ€§æ·±åº¦é—®ç­”å¯¹")
        return enhanced_pairs[:num_questions]
    else:
        print("âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥")
        return []

def adaptive_question_generation(contents: Dict[str, str], num_questions: int, language: str, domain_analysis: Dict) -> list:
    """
    è‡ªé€‚åº”é—®é¢˜ç”Ÿæˆï¼šæ ¹æ®æ–‡ä»¶æ•°é‡é€‰æ‹©ç”Ÿæˆç­–ç•¥
    """
    all_qa_pairs = []
    
    if len(contents) == 1:
        # å•æ–‡ä»¶ï¼šåªç”Ÿæˆæ·±åº¦é—®é¢˜
        file_name, content = list(contents.items())[0]
        single_qa_pairs = generate_single_file_questions(content, file_name, num_questions, language, domain_analysis)
        all_qa_pairs.extend(single_qa_pairs)
    else:
        # å¤šæ–‡ä»¶ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆæ·±åº¦é—®é¢˜ + ç»¼åˆæ€§é—®é¢˜
        # å•ä¸ªæ–‡ä»¶é—®é¢˜ï¼ˆæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆè¾ƒå°‘é—®é¢˜ï¼‰
        single_questions_per_file = max(2, num_questions // (len(contents) * 2))
        for file_name, content in contents.items():
            single_qa_pairs = generate_single_file_questions(content, file_name, single_questions_per_file, language, domain_analysis)
            all_qa_pairs.extend(single_qa_pairs)
        
        # ç»¼åˆæ€§é—®é¢˜
        cross_questions = max(3, num_questions - len(all_qa_pairs))
        if cross_questions > 0:
            cross_qa_pairs = generate_cross_file_questions(contents, cross_questions, language, domain_analysis)
            all_qa_pairs.extend(cross_qa_pairs)
    
    return all_qa_pairs[:num_questions]

def save_qa_dataset(qa_pairs: list, output_file: str, instruction: str, language: str = "both"):
    """
    ä¿å­˜é—®ç­”æ•°æ®é›†
    """
    total_records = 0
    with open(output_file, "w", encoding="utf-8") as f_out:
        for pair in qa_pairs:
            metadata = pair.get("metadata", {})
            
            if language == "both":
                # åŒè¯­ç‰ˆæœ¬
                if all(key in pair for key in ["question_en", "answer_en", "question_zh", "answer_zh"]):
                    # è‹±æ–‡ç‰ˆæœ¬
                    output_record_en = {
                        "instruction": instruction,
                        "input": pair["question_en"],
                        "output": pair["answer_en"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record_en, ensure_ascii=False) + "\n")
                    
                    # ä¸­æ–‡ç‰ˆæœ¬
                    output_record_zh = {
                        "instruction": instruction,
                        "input": pair["question_zh"],
                        "output": pair["answer_zh"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record_zh, ensure_ascii=False) + "\n")
                    total_records += 2
            
            elif language in ["english", "chinese"]:
                if "question" in pair and "answer" in pair:
                    output_record = {
                        "instruction": instruction,
                        "input": pair["question"],
                        "output": pair["answer"],
                        "metadata": metadata
                    }
                    f_out.write(json.dumps(output_record, ensure_ascii=False) + "\n")
                    total_records += 1
    
    return total_records

def main():
    """
    ä¸»ç¨‹åº - æ·±åº¦å­¦æœ¯é—®é¢˜ç”Ÿæˆå™¨
    """
    print("=== æ·±åº¦å­¦æœ¯é—®é¢˜ç”Ÿæˆå™¨ ===")
    print("âœ¨ æ ¸å¿ƒç‰¹ç‚¹ï¼š")
    print("   - ç”¨æˆ·è‡ªå®šä¹‰ä¸“å®¶è§’è‰²")
    print("   - æ·±åº¦é¢†åŸŸä¸“ä¸šé—®é¢˜")
    print("   - ç»¼åˆæ€§å­¦æœ¯æ¨ç†")
    print("   - ä¸“ä¸šå­¦æœ¯è¯­è¨€è¡¨è¾¾")
    
    # é…ç½®
    txt_folder = input("è¯·è¾“å…¥txtæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: txt_files): ").strip() or "txt_files"
    
    # ç”¨æˆ·è¾“å…¥ä¸“å®¶è§’è‰²
    instruction = input("è¯·è¾“å…¥instructionå†…å®¹ (ä¾‹å¦‚: 'ä½ æ˜¯ä¸€ä½ç»æµå­¦ä¸“å®¶'): ").strip()
    if not instruction:
        instruction = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶äººå‘˜"
    
    language_choice = input("é€‰æ‹©è¯­è¨€ç‰ˆæœ¬ (1=è‹±æ–‡, 2=ä¸­æ–‡, 3=ä¸­è‹±åŒè¯­): ").strip()
    if language_choice == "1":
        language = "english"
        output_file = "qa_academic_english.jsonl"
    elif language_choice == "2":
        language = "chinese"
        output_file = "qa_academic_chinese.jsonl"
    else:
        language = "both"
        output_file = "qa_academic_bilingual.jsonl"
    
    num_questions = input("è¦ç”Ÿæˆå¤šå°‘ä¸ªé—®é¢˜ï¼Ÿ (é»˜è®¤20): ").strip()
    if not num_questions:
        num_questions = 20
    else:
        num_questions = int(num_questions)
    
    # è¯»å–æ–‡ä»¶
    print("\næ­£åœ¨è¯»å–æ–‡ä»¶...")
    contents = read_multiple_txt_files(txt_folder)
    
    if not contents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å¤¹è·¯å¾„")
        return
    
    # åˆ†æç ”ç©¶é¢†åŸŸ
    domain_analysis = analyze_research_domains(contents)
    
    print(f"\nğŸ“Š ç»¼åˆåˆ†æç»“æœ:")
    print(f"   ä¸»è¦é¢†åŸŸ: {', '.join(domain_analysis['primary_domains'])}")
    print(f"   æ ¸å¿ƒä¸»é¢˜: {', '.join(domain_analysis['primary_themes'])}")
    print(f"   å…³é”®è¯: {', '.join(domain_analysis['top_keywords'][:8])}")
    print(f"   æ–‡ä»¶æ•°é‡: {domain_analysis['file_count']}")
    
    # ç”Ÿæˆé—®é¢˜
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆæ·±åº¦å­¦æœ¯é—®é¢˜...")
    qa_pairs = adaptive_question_generation(contents, num_questions, language, domain_analysis)
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆ!")
    print(f"   æˆåŠŸç”Ÿæˆ: {len(qa_pairs)} ä¸ªæ·±åº¦å­¦æœ¯é—®ç­”å¯¹")
    
    # ä¿å­˜ç»“æœ
    total_records = save_qa_dataset(qa_pairs, output_file, instruction, language)
    print(f"   ä¿å­˜åˆ°: {output_file}")
    print(f"   æ€»è®°å½•æ•°: {total_records} æ¡")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if qa_pairs:
        print(f"\nğŸ“ ç¤ºä¾‹æ·±åº¦å­¦æœ¯é—®é¢˜:")
        print("-" * 60)
        for i, pair in enumerate(qa_pairs[:3]):
            if language == "both":
                print(f"{i+1}. [EN] {pair.get('question_en', 'N/A')}")
                print(f"    [ZH] {pair.get('question_zh', 'N/A')}")
            else:
                print(f"{i+1}. {pair.get('question', 'N/A')}")
            print()

if __name__ == "__main__":
    main()