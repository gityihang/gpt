{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef57f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pyzotero in /home/byh/miniconda3/lib/python3.9/site-packages (1.6.17)\n",
      "Collecting openai\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9c/5b/4be258ff072ed8ee15f6bfd8d5a1a4618aa4704b127c0c5959212ad177d6/openai-2.3.0-py3-none-any.whl (999 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.8/999.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: feedparser>=6.0.12 in /home/byh/miniconda3/lib/python3.9/site-packages (from pyzotero) (6.0.12)\n",
      "Requirement already satisfied: bibtexparser<2.0.0,>=1.4.3 in /home/byh/miniconda3/lib/python3.9/site-packages (from pyzotero) (1.4.3)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /home/byh/miniconda3/lib/python3.9/site-packages (from pyzotero) (0.28.1)\n",
      "Requirement already satisfied: whenever>=0.8.8 in /home/byh/miniconda3/lib/python3.9/site-packages (from pyzotero) (0.9.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/byh/miniconda3/lib/python3.9/site-packages (from openai) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/de/91/25e38fbbfc17111d7b70b24290a41d611cc2a27fa6cd0ed84ddae38ec3e6/jiter-0.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (350 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m350.3/350.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f5/69/ce4e60e5e67aa0c339a5dc3391a02b4036545efb6308c54dc4aa9425386f/pydantic-2.12.1-py3-none-any.whl (460 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.5/460.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /home/byh/miniconda3/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/byh/miniconda3/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/byh/miniconda3/lib/python3.9/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/byh/miniconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/byh/miniconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /home/byh/miniconda3/lib/python3.9/site-packages (from bibtexparser<2.0.0,>=1.4.3->pyzotero) (3.2.5)\n",
      "Requirement already satisfied: sgmllib3k in /home/byh/miniconda3/lib/python3.9/site-packages (from feedparser>=6.0.12->pyzotero) (1.0.0)\n",
      "Requirement already satisfied: certifi in /home/byh/miniconda3/lib/python3.9/site-packages (from httpx>=0.28.1->pyzotero) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/byh/miniconda3/lib/python3.9/site-packages (from httpx>=0.28.1->pyzotero) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/byh/miniconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.28.1->pyzotero) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.41.3 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/e7/5aeafab2b7bfde3650e67a7b2a9f98f0ea86caf8cf169e7260e8f624777c/pydantic_core-2.41.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dc/9b/47798a6c91d8bdb567fe2698fe81e0c6b7cb7ef4d13da4114b41d239f65d/typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.11.0 openai-2.3.0 pydantic-2.12.1 pydantic-core-2.41.3 typing-inspection-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyzotero openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d214f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已保存 13 篇文献摘要到 zotero_collection.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pyzotero import zotero\n",
    "import json\n",
    "\n",
    "# --- 1. 连接 Zotero ---\n",
    "library_id = '13400884'\n",
    "library_type = 'user'\n",
    "api_key = 'k2eCXt7ltwQQW7WNnrSiWDYU'\n",
    "\n",
    "zot = zotero.Zotero(library_id, library_type, api_key)\n",
    "\n",
    "# --- 2. 指定 collection ---\n",
    "collection_key = 'XPY9MWBP'\n",
    "items = zot.collection_items(collection_key, limit=None)\n",
    "\n",
    "data_list = []\n",
    "seen_keys = set()  # 保存已经抓过的条目 key\n",
    "\n",
    "for item in items:\n",
    "    d = item['data']\n",
    "    \n",
    "    # 如果是 attachment，先找到 parentItem\n",
    "    if d['itemType'] == 'attachment' and 'parentItem' in d:\n",
    "        parent_key = d['parentItem']\n",
    "        if parent_key in seen_keys:  # 已抓过则跳过\n",
    "            continue\n",
    "        try:\n",
    "            parent_item = zot.item(parent_key)\n",
    "            d = parent_item['data']\n",
    "            seen_keys.add(parent_key)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 无法获取 parentItem {parent_key}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        key = d['key']\n",
    "        if key in seen_keys:  # 已抓过则跳过\n",
    "            continue\n",
    "        seen_keys.add(key)\n",
    "    \n",
    "    # 只处理主要文献类型\n",
    "    if d['itemType'] in ['journalArticle', 'preprint', 'book', 'conferencePaper']:\n",
    "        title = d.get('title', '').strip()\n",
    "        abstract = d.get('abstractNote', '').strip()\n",
    "        if title or abstract:\n",
    "            data_list.append({\n",
    "                'title': title,\n",
    "                'abstract': abstract\n",
    "            })\n",
    "\n",
    "# --- 保存为 JSONL ---\n",
    "with open('zotero_collection.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for entry in data_list:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f'✅ 已保存 {len(data_list)} 篇文献摘要到 zotero_collection.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed267bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96d968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\"question\": \"How does the cross-cycling of Transformer and Mamba blocks in MatIR overcome the individual limitations of each architecture?\", \"answer\": \"The cross-cycling mechanism alternates between Transformer blocks for superior contextual feature learning and Mamba blocks for efficient long-range dependency modeling, creating a synergistic architecture that compensates for Mamba's contextual learning deficiencies while maintaining computational efficiency.\"},\n",
      "  {\"question\": \"What is the functional principle behind the Image Inpainting State Space (IRSS) module in handling long image sequences?\", \"answer\": \"The IRSS module implements four distinct scan paths (likely horizontal, vertical, and diagonal directions) to systematically traverse image data, enabling efficient state space modeling of long-range dependencies while maintaining linear computational complexity relative to sequence length.\"},\n",
      "  {\"question\": \"How does the hybrid attention mechanism in the Transformer module balance local and global feature extraction?\", \"answer\": \"It combines triangular window-based local attention for capturing fine-grained spatial relationships within constrained regions with channel-based global attention to model cross-channel dependencies across the entire image, achieving comprehensive feature activation.\"},\n",
      "  {\"question\": \"What computational efficiency advantages does the Mamba component provide compared to traditional Transformer architectures?\", \"answer\": \"Mamba's selective state space models offer linear scaling with sequence length rather than the quadratic complexity of standard self-attention, significantly reducing computational overhead while maintaining competitive performance on long-range dependencies.\"},\n",
      "  {\"question\": \"What experimental methodology was employed to validate MatIR's performance against state-of-the-art image restoration models?\", \"answer\": \"The paper conducted extensive comparative experiments on benchmark datasets using standard image restoration metrics (PSNR, SSIM), along with comprehensive ablation studies to isolate the contributions of individual architectural components.\"},\n",
      "  {\"question\": \"How does the triangular window attention mechanism differ from conventional local attention approaches?\", \"answer\": \"The triangular window likely implements an asymmetric receptive field that adapts to image structures more effectively than rectangular windows, potentially providing better edge preservation and directional feature extraction in restoration tasks.\"},\n",
      "  {\"question\": \"What were the key findings from the ablation studies regarding the optimal ratio of Transformer to Mamba blocks?\", \"answer\": \"While specific ratios aren't detailed, the ablation studies demonstrated that neither pure Transformer nor pure Mamba architectures achieved the same performance as their hybrid combination, indicating an optimal balance between contextual modeling and computational efficiency.\"},\n",
      "  {\"question\": \"How does the channel-based global attention mechanism operate without the computational burden of spatial global attention?\", \"answer\": \"By applying attention operations along the channel dimension rather than spatial dimensions, it captures global contextual relationships while maintaining manageable computational complexity, complementing the spatial-local attention for comprehensive feature representation.\"},\n",
      "  {\"question\": \"What are the primary limitations of MatIR identified in the research, particularly regarding computational requirements or application scope?\", \"answer\": \"While more efficient than pure Transformers, the hybrid architecture still requires significant memory for high-resolution images, and the optimal scan path configurations in IRSS may be task-dependent, limiting generalization across all restoration scenarios.\"},\n",
      "  {\"question\": \"How does the four-path scanning in IRSS enhance feature extraction compared to single-direction scanning approaches?\", \"answer\": \"The multi-directional scanning captures complementary spatial contexts from different orientations, ensuring comprehensive feature integration and preventing directional bias in the state space modeling, which is crucial for holistic image restoration.\"}\n",
      "]\n",
      "Processed: MatIR: A hybrid mamba-transformer image restoration model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-b6118335f5c34520abffbe6fa324257a\" #my key\n",
    "\n",
    "# --- 设置 DeepSeek API Key ---\n",
    "api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the environment variable DEEPSEEK_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "# --- 输入 JSONL 文件 ---\n",
    "input_file = \"zotero_collection.jsonl\"\n",
    "output_file = \"train.jsonl\"\n",
    "\n",
    "def clean_gpt_output(gpt_output: str) -> str:\n",
    "    \"\"\"\n",
    "    清理 GPT 输出，去掉 ``` 或者非 JSON 的内容\n",
    "    \"\"\"\n",
    "    gpt_output = gpt_output.strip()\n",
    "    # 移除代码块\n",
    "    gpt_output = re.sub(r\"```.*?```\", \"\", gpt_output, flags=re.DOTALL)\n",
    "    # 只保留 [] 中的内容\n",
    "    match = re.search(r\"\\[.*\\]\", gpt_output, flags=re.DOTALL)\n",
    "    if match:\n",
    "        gpt_output = match.group(0)\n",
    "    return gpt_output\n",
    "\n",
    "# --- 处理每条记录 ---\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:\n",
    "        record = json.loads(line)\n",
    "        title = record.get(\"title\", \"\").strip()\n",
    "        abstract = record.get(\"abstract\", \"\").strip()\n",
    "\n",
    "        if not title or not abstract:\n",
    "            print(f\"Skipping empty record: {record}\")\n",
    "            continue\n",
    "\n",
    "        # --- GPT Prompt ---\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert research assistant specialized in computer vision and deep learning.\n",
    "        Based on the following paper title and abstract, generate 10 technical and research-oriented question-answer pairs in English.\n",
    "        - Do NOT ask trivial questions (e.g., \"What is the name of the model?\").\n",
    "        - Each question should probe the methodology, contributions, experiments, or theoretical insights.\n",
    "        - Cover different aspects: architecture, modules, attention mechanisms, efficiency, ablation studies, limitations.\n",
    "        - Return ONLY a JSON array of objects with exactly two fields: \"question\" and \"answer\".\n",
    "\n",
    "        Example output:\n",
    "        [\n",
    "        {{\"question\": \"How does the proposed hybrid architecture combine Transformer and Mamba layers?\", \"answer\": \"It cross-cycles blocks from both layers to leverage contextual learning and computational efficiency.\"}},\n",
    "        {{\"question\": \"What is the function of the IRSS module?\", \"answer\": \"It traverses four scan paths to process long image sequences efficiently.\"}}\n",
    "        ]\n",
    "\n",
    "        Title: {title}\n",
    "        Abstract: {abstract}\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 调用 DeepSeek GPT ---\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful scientific assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "        gpt_output = response.choices[0].message.content\n",
    "        gpt_output = clean_gpt_output(gpt_output)\n",
    "        print(gpt_output)\n",
    "        # --- 解析 JSON ---\n",
    "        try:\n",
    "            qa_pairs = json.loads(gpt_output)\n",
    "            if not isinstance(qa_pairs, list):\n",
    "                raise ValueError(\"Parsed JSON is not a list\")\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"Warning: GPT output is not valid JSON for paper '{title}'. Skipping.\\nError: {e}\\nOutput: {gpt_output}\")\n",
    "            continue\n",
    "\n",
    "        # --- 遍历 qa_pairs 列表，为每个 Q&A 生成一条训练记录 ---\n",
    "        for pair in qa_pairs:\n",
    "            # 假设每个 pair 是一个包含 \"question\" 和 \"answer\" 的字典\n",
    "            if isinstance(pair, dict) and \"question\" in pair and \"answer\" in pair:\n",
    "                output_record = {\n",
    "                    \"instruction\": \"you are an expert in economics\", # 这是一个通用的指令\n",
    "                    \"input\": pair[\"question\"],\n",
    "                    \"output\": pair[\"answer\"]\n",
    "                }\n",
    "                f_out.write(json.dumps(output_record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Processed: {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa24dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ab0f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'LLaMA-Factory'...\n",
      "fatal: 无法访问 'https://github.com/hiyouga/LLaMA-Factory.git/'：GnuTLS recv error (-110): The TLS connection was non-properly terminated.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3c8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0ff2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 9 files:   0%|                                   | 0/9 [00:00<?, ?it/s]Downloading 'pytorch_model-00002-of-00002.bin' to 'models/.cache/huggingface/download/HnkwBfZ0kY-ttHuN02vuxl1p6V0=.e6cc31dd99c92be73064a38a661fae821dd5f437bac16202c014420c25a5cffd.incomplete'\n",
      "Downloading 'pytorch_model-00001-of-00002.bin' to 'models/.cache/huggingface/download/fPHULxv55kAe7RSfHmmL42LIc1I=.7c8e56ddd37b2c2df2ac23cfcf57b7924de43e531f66238725f58215dc9f03d3.incomplete'\n",
      "Downloading 'README.md' to 'models/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.9a238c25fdd40b5a4f174038873bfda3de863c2b.incomplete'\n",
      "Downloading 'generation_config.json' to 'models/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.85212c6f6424d4215a3d0b66b2b486e5cb177b53.incomplete'\n",
      "\n",
      "README.md: 3.23kB [00:00, 21.0MB/s]\n",
      "Download complete. Moving file to models/README.md\n",
      "\n",
      "generation_config.json: 181B [00:00, 2.26MB/s]\n",
      "Download complete. Moving file to models/generation_config.json\n",
      "Downloading 'tokenizer.json' to 'models/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.22a7dddf5732f937c53eb3bfc6127c6528d42cf8.incomplete'\n",
      "Downloading 'pytorch_model.bin.index.json' to 'models/.cache/huggingface/download/8TePU6wZ6PO52hlgkfCnTYnlMSI=.ce6094ab4c40400eca5963470784235f52b7e843.incomplete'\n",
      "Downloading '.gitattributes' to 'models/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "\n",
      "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\n",
      "\n",
      "pytorch_model.bin.index.json: 22.5kB [00:00, 144MB/s][A\n",
      "Download complete. Moving file to models/pytorch_model.bin.index.json\n",
      "Downloading 'config.json' to 'models/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.c7ec540a0bea43211102776010779525e60a36d0.incomplete'\n",
      "\n",
      "\n",
      ".gitattributes: 1.52kB [00:00, 14.0MB/s]A\n",
      "Download complete. Moving file to models/.gitattributes\n",
      "Fetching 9 files:  11%|███                        | 1/9 [00:00<00:07,  1.11it/s]\n",
      "tokenizer.json: 116kB [00:00, 769kB/s]\u001b[A\n",
      "\n",
      "config.json: 594B [00:00, 5.49MB/s]\u001b[A\n",
      "Download complete. Moving file to models/config.json\n",
      "Fetching 9 files:  33%|█████████                  | 3/9 [00:00<00:01,  3.70it/s]Downloading 'tokenizer_config.json' to 'models/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.08113300e3aec9f072ae8cfa6eaee71827f8a3b0.incomplete'\n",
      "\n",
      "tokenizer.json: 338kB [00:00, 1.29MB/s]\u001b[A\n",
      "\n",
      "tokenizer_config.json: 1.28kB [00:00, 922kB/s][A\n",
      "Download complete. Moving file to models/tokenizer_config.json\n",
      "\n",
      "tokenizer.json: 675kB [00:00, 2.05MB/s]\u001b[A\n",
      "tokenizer.json: 1.39MB [00:00, 3.72MB/s]\u001b[A\n",
      "tokenizer.json: 4.61MB [00:00, 6.76MB/s]\u001b[A\n",
      "Download complete. Moving file to models/tokenizer.json\n",
      "\n",
      "pytorch_model-00002-of-00002.bin:   0%|             | 0.00/3.85G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0%|             | 0.00/9.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   0%|    | 10.5M/3.85G [00:01<06:45, 9.47MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 21.0M/3.85G [00:02<06:39, 9.60MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 31.5M/3.85G [00:03<06:47, 9.38MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 41.9M/3.85G [00:04<06:57, 9.13MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0%|  | 10.5M/9.97G [00:04<1:12:30, 2.29MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   1%|    | 52.4M/3.85G [00:05<07:12, 8.79MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|    | 62.9M/3.85G [00:07<07:27, 8.47MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 21.0M/9.97G [00:07<55:45, 2.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|    | 73.4M/3.85G [00:08<07:45, 8.13MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|    | 83.9M/3.85G [00:09<07:55, 7.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 31.5M/9.97G [00:10<49:49, 3.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   0%|    | 41.9M/9.97G [00:11<40:22, 4.10MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   2%|    | 94.4M/3.85G [00:11<09:12, 6.81MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 52.4M/9.97G [00:13<34:21, 4.81MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   3%|▏    | 105M/3.85G [00:14<10:33, 5.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 62.9M/9.97G [00:14<31:14, 5.28MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   3%|▏    | 115M/3.85G [00:15<10:31, 5.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 73.4M/9.97G [00:16<29:36, 5.57MB/s]\u001b[A\u001b[A\n",
      "Fetching 9 files:  33%|█████████                  | 3/9 [00:19<00:01,  3.70it/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 83.9M/9.97G [00:18<28:56, 5.69MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▏    | 136M/3.85G [00:19<11:02, 5.61MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|    | 94.4M/9.97G [00:19<28:02, 5.87MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▏    | 147M/3.85G [00:21<10:19, 5.98MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 105M/9.97G [00:22<30:17, 5.43MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▏    | 157M/3.85G [00:22<09:53, 6.22MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   4%|▏    | 168M/3.85G [00:24<09:24, 6.52MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 115M/9.97G [00:24<31:49, 5.16MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▏    | 178M/3.85G [00:25<09:16, 6.61MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 126M/9.97G [00:26<32:27, 5.05MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▏    | 189M/3.85G [00:27<09:11, 6.65MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 136M/9.97G [00:28<32:35, 5.03MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▎    | 199M/3.85G [00:29<09:08, 6.66MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   5%|▎    | 210M/3.85G [00:30<09:05, 6.68MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   1%|     | 147M/9.97G [00:30<31:48, 5.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 157M/9.97G [00:31<28:13, 5.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 168M/9.97G [00:33<25:49, 6.32MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▎    | 220M/3.85G [00:33<11:39, 5.19MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 178M/9.97G [00:34<24:36, 6.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 189M/9.97G [00:35<23:29, 6.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▎    | 231M/3.85G [00:36<12:29, 4.84MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 199M/9.97G [00:37<23:27, 6.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   6%|▎    | 241M/3.85G [00:38<12:37, 4.77MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 210M/9.97G [00:38<23:19, 6.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▎    | 252M/3.85G [00:40<12:35, 4.76MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 220M/9.97G [00:40<23:29, 6.92MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▎    | 262M/3.85G [00:42<12:00, 4.98MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 231M/9.97G [00:42<27:17, 5.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▎    | 273M/3.85G [00:44<11:12, 5.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   2%|     | 241M/9.97G [00:44<28:22, 5.71MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   7%|▎    | 283M/3.85G [00:46<11:00, 5.41MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 252M/9.97G [00:46<27:47, 5.83MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▍    | 294M/3.85G [00:47<10:47, 5.50MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 262M/9.97G [00:48<26:48, 6.03MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▍    | 304M/3.85G [00:49<10:42, 5.52MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 273M/9.97G [00:50<27:15, 5.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▍    | 315M/3.85G [00:51<10:36, 5.56MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 283M/9.97G [00:52<28:59, 5.57MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   8%|▍    | 325M/3.85G [00:53<10:19, 5.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 294M/9.97G [00:54<28:41, 5.62MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▍    | 336M/3.85G [00:55<10:02, 5.84MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 304M/9.97G [00:55<28:23, 5.67MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▍    | 346M/3.85G [00:56<09:53, 5.91MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 315M/9.97G [00:57<28:17, 5.69MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:   9%|▍    | 357M/3.85G [00:58<09:48, 5.94MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 325M/9.97G [00:59<28:40, 5.61MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▍    | 367M/3.85G [01:00<09:35, 6.06MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 336M/9.97G [01:01<28:26, 5.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▍    | 377M/3.85G [01:02<10:21, 5.59MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   3%|▏    | 346M/9.97G [01:02<26:01, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 357M/9.97G [01:04<24:34, 6.52MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▌    | 388M/3.85G [01:04<11:19, 5.10MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 367M/9.97G [01:05<23:59, 6.67MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  10%|▌    | 398M/3.85G [01:06<11:19, 5.09MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 377M/9.97G [01:07<24:46, 6.45MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▌    | 409M/3.85G [01:08<10:42, 5.36MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 388M/9.97G [01:09<26:12, 6.09MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▌    | 419M/3.85G [01:10<10:02, 5.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 398M/9.97G [01:11<28:00, 5.70MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▌    | 430M/3.85G [01:11<09:26, 6.04MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  11%|▌    | 440M/3.85G [01:13<08:45, 6.49MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 409M/9.97G [01:14<31:25, 5.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|▌    | 451M/3.85G [01:14<08:27, 6.70MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|▌    | 461M/3.85G [01:15<08:16, 6.84MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 419M/9.97G [01:16<31:49, 5.00MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  12%|▌    | 472M/3.85G [01:17<08:14, 6.84MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 430M/9.97G [01:18<32:05, 4.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|▋    | 482M/3.85G [01:19<08:15, 6.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   4%|▏    | 440M/9.97G [01:20<31:54, 4.98MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|▋    | 493M/3.85G [01:20<08:15, 6.77MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|▋    | 503M/3.85G [01:22<08:57, 6.24MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▏    | 451M/9.97G [01:22<31:30, 5.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▏    | 461M/9.97G [01:23<28:52, 5.49MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  13%|▋    | 514M/3.85G [01:24<09:19, 5.97MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▏    | 472M/9.97G [01:25<27:21, 5.78MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▋    | 524M/3.85G [01:26<09:35, 5.78MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▏    | 482M/9.97G [01:27<26:55, 5.87MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▋    | 535M/3.85G [01:28<09:41, 5.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▏    | 493M/9.97G [01:28<26:07, 6.05MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▋    | 545M/3.85G [01:30<09:53, 5.57MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎    | 503M/9.97G [01:30<25:39, 6.15MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  14%|▋    | 556M/3.85G [01:32<10:23, 5.28MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎    | 514M/9.97G [01:32<26:55, 5.85MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎    | 524M/9.97G [01:33<25:22, 6.20MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|▋    | 566M/3.85G [01:34<10:31, 5.20MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎    | 535M/9.97G [01:35<24:39, 6.37MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|▋    | 577M/3.85G [01:36<10:44, 5.08MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   5%|▎    | 545M/9.97G [01:37<23:57, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 556M/9.97G [01:38<23:23, 6.71MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  15%|▊    | 587M/3.85G [01:39<11:01, 4.94MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 566M/9.97G [01:40<23:19, 6.72MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|▊    | 598M/3.85G [01:41<11:03, 4.90MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 577M/9.97G [01:41<23:14, 6.73MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|▊    | 608M/3.85G [01:43<10:49, 4.99MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 587M/9.97G [01:43<24:14, 6.45MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|▊    | 619M/3.85G [01:45<10:36, 5.08MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 598M/9.97G [01:45<24:40, 6.33MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 608M/9.97G [01:46<24:27, 6.38MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  16%|▊    | 629M/3.85G [01:47<10:17, 5.22MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 619M/9.97G [01:48<24:56, 6.25MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|▊    | 640M/3.85G [01:48<09:55, 5.40MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 629M/9.97G [01:50<25:30, 6.10MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|▊    | 650M/3.85G [01:50<09:34, 5.57MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   6%|▎    | 640M/9.97G [01:52<26:02, 5.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|▊    | 661M/3.85G [01:52<09:41, 5.49MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  17%|▊    | 671M/3.85G [01:54<09:00, 5.88MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 650M/9.97G [01:54<28:33, 5.44MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  18%|▉    | 682M/3.85G [01:55<08:33, 6.18MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 661M/9.97G [01:56<30:12, 5.14MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  18%|▉    | 692M/3.85G [01:57<08:08, 6.47MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  18%|▉    | 703M/3.85G [01:58<07:59, 6.58MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 671M/9.97G [01:58<30:45, 5.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|▉    | 713M/3.85G [02:00<07:54, 6.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 682M/9.97G [02:01<30:46, 5.03MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|▉    | 724M/3.85G [02:01<07:50, 6.66MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 692M/9.97G [02:03<30:42, 5.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|▉    | 734M/3.85G [02:03<07:47, 6.67MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  19%|▉    | 744M/3.85G [02:05<08:17, 6.25MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 703M/9.97G [02:05<30:18, 5.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 713M/9.97G [02:06<27:41, 5.57MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|▉    | 755M/3.85G [02:07<08:54, 5.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 724M/9.97G [02:08<26:12, 5.88MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|▉    | 765M/3.85G [02:09<09:17, 5.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 734M/9.97G [02:09<25:07, 6.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   7%|▎    | 744M/9.97G [02:11<24:09, 6.36MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|█    | 776M/3.85G [02:11<09:38, 5.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 755M/9.97G [02:12<24:00, 6.40MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  20%|█    | 786M/3.85G [02:13<09:45, 5.24MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 765M/9.97G [02:14<23:50, 6.44MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  21%|█    | 797M/3.85G [02:15<10:14, 4.97MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 776M/9.97G [02:16<24:18, 6.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 786M/9.97G [02:17<23:36, 6.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  21%|█    | 807M/3.85G [02:18<10:13, 4.96MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 797M/9.97G [02:19<23:00, 6.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  21%|█    | 818M/3.85G [02:20<10:21, 4.88MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 807M/9.97G [02:20<22:42, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 818M/9.97G [02:22<22:00, 6.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 828M/3.85G [02:22<10:25, 4.83MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 828M/9.97G [02:23<21:50, 6.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 839M/3.85G [02:24<10:38, 4.72MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   8%|▍    | 839M/9.97G [02:24<21:23, 7.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 849M/9.97G [02:26<23:21, 6.51MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 849M/3.85G [02:27<11:08, 4.50MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 860M/9.97G [02:28<23:32, 6.45MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  22%|█    | 860M/3.85G [02:29<10:00, 4.98MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 870M/9.97G [02:30<24:43, 6.13MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|█▏   | 870M/3.85G [02:30<09:27, 5.26MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|█▏   | 881M/3.85G [02:32<08:56, 5.54MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 881M/9.97G [02:32<25:19, 5.98MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|█▏   | 891M/3.85G [02:34<08:40, 5.69MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 891M/9.97G [02:34<26:13, 5.77MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  23%|█▏   | 902M/3.85G [02:36<08:44, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 902M/9.97G [02:36<27:34, 5.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 912M/3.85G [02:37<08:28, 5.78MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 912M/9.97G [02:38<27:37, 5.46MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 923M/3.85G [02:39<08:13, 5.94MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 923M/9.97G [02:40<27:35, 5.46MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 933M/3.85G [02:41<08:01, 6.06MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 933M/9.97G [02:42<28:14, 5.33MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  24%|█▏   | 944M/3.85G [02:42<07:44, 6.27MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|█▏   | 954M/3.85G [02:44<07:36, 6.34MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:   9%|▍    | 944M/9.97G [02:44<28:42, 5.24MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|█▎   | 965M/3.85G [02:45<07:21, 6.54MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍    | 954M/9.97G [02:46<30:36, 4.91MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  25%|█▎   | 975M/3.85G [02:47<07:29, 6.40MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|█▎   | 986M/3.85G [02:48<07:16, 6.57MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍    | 965M/9.97G [02:49<30:39, 4.89MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|█▎   | 996M/3.85G [02:50<07:25, 6.41MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍    | 975M/9.97G [02:50<29:28, 5.08MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|█   | 1.01G/3.85G [02:52<07:43, 6.14MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍    | 986M/9.97G [02:52<27:47, 5.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍    | 996M/9.97G [02:54<26:32, 5.63MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  26%|█   | 1.02G/3.85G [02:54<07:59, 5.91MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍   | 1.01G/9.97G [02:55<25:20, 5.89MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█   | 1.03G/3.85G [02:56<08:49, 5.34MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍   | 1.02G/9.97G [02:57<25:23, 5.87MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█   | 1.04G/3.85G [02:58<08:41, 5.40MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍   | 1.03G/9.97G [02:59<25:14, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█   | 1.05G/3.85G [03:00<08:19, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  10%|▍   | 1.04G/9.97G [03:01<25:14, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  27%|█   | 1.06G/3.85G [03:02<08:18, 5.61MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.05G/9.97G [03:02<24:57, 5.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|█   | 1.07G/3.85G [03:04<08:22, 5.54MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.06G/9.97G [03:04<24:11, 6.14MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|█   | 1.08G/3.85G [03:06<08:26, 5.48MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.07G/9.97G [03:06<24:19, 6.10MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  28%|█▏  | 1.09G/3.85G [03:08<08:36, 5.35MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.08G/9.97G [03:08<25:29, 5.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.09G/9.97G [03:09<24:37, 6.01MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|█▏  | 1.10G/3.85G [03:10<08:37, 5.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.10G/9.97G [03:11<24:06, 6.13MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|█▏  | 1.11G/3.85G [03:12<08:22, 5.45MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.11G/9.97G [03:13<24:41, 5.98MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|█▏  | 1.12G/3.85G [03:13<08:01, 5.67MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  29%|█▏  | 1.13G/3.85G [03:15<07:40, 5.90MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.12G/9.97G [03:15<25:52, 5.70MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▏  | 1.14G/3.85G [03:17<07:50, 5.76MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.13G/9.97G [03:17<27:33, 5.34MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▏  | 1.15G/3.85G [03:19<07:41, 5.85MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  11%|▍   | 1.14G/9.97G [03:19<26:15, 5.60MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▏  | 1.16G/3.85G [03:20<07:45, 5.78MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.15G/9.97G [03:21<25:46, 5.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.16G/9.97G [03:22<24:42, 5.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  30%|█▏  | 1.17G/3.85G [03:22<07:49, 5.71MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.17G/9.97G [03:24<24:41, 5.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  31%|█▏  | 1.18G/3.85G [03:24<07:50, 5.67MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.18G/9.97G [03:26<24:55, 5.87MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  31%|█▏  | 1.20G/3.85G [03:26<07:35, 5.84MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  31%|█▎  | 1.21G/3.85G [03:28<07:50, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.20G/9.97G [03:28<26:19, 5.55MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▎  | 1.22G/3.85G [03:30<07:36, 5.77MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.21G/9.97G [03:30<26:20, 5.54MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▎  | 1.23G/3.85G [03:31<07:25, 5.90MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.22G/9.97G [03:32<26:02, 5.60MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▎  | 1.24G/3.85G [03:33<07:30, 5.81MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.23G/9.97G [03:33<25:23, 5.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  12%|▍   | 1.24G/9.97G [03:35<24:31, 5.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  32%|█▎  | 1.25G/3.85G [03:35<07:38, 5.68MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.25G/9.97G [03:37<25:28, 5.71MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▎  | 1.26G/3.85G [03:37<07:51, 5.50MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▎  | 1.27G/3.85G [03:39<07:39, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.26G/9.97G [03:39<25:26, 5.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.27G/9.97G [03:41<25:08, 5.77MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▎  | 1.28G/3.85G [03:41<07:36, 5.63MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.28G/9.97G [03:42<24:06, 6.01MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  33%|█▎  | 1.29G/3.85G [03:43<07:35, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.29G/9.97G [03:44<24:13, 5.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|█▎  | 1.30G/3.85G [03:45<07:36, 5.59MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.30G/9.97G [03:46<24:29, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|█▎  | 1.31G/3.85G [03:46<07:24, 5.72MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.31G/9.97G [03:48<25:32, 5.65MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  34%|█▎  | 1.32G/3.85G [03:48<07:30, 5.61MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.32G/9.97G [03:49<24:46, 5.82MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▍  | 1.33G/3.85G [03:50<07:30, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.33G/9.97G [03:51<24:25, 5.89MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▍  | 1.34G/3.85G [03:52<07:34, 5.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  13%|▌   | 1.34G/9.97G [03:53<24:36, 5.84MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▍  | 1.35G/3.85G [03:54<07:16, 5.73MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.35G/9.97G [03:55<25:33, 5.62MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  35%|█▍  | 1.36G/3.85G [03:55<07:00, 5.92MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|█▍  | 1.37G/3.85G [03:57<06:44, 6.13MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.36G/9.97G [03:57<26:24, 5.43MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|█▍  | 1.38G/3.85G [03:59<06:41, 6.15MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.37G/9.97G [04:00<28:32, 5.02MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|█▍  | 1.39G/3.85G [04:00<06:31, 6.27MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  36%|█▍  | 1.41G/3.85G [04:02<06:15, 6.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.38G/9.97G [04:02<29:02, 4.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|█▍  | 1.42G/3.85G [04:03<06:03, 6.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.39G/9.97G [04:04<29:05, 4.91MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|█▍  | 1.43G/3.85G [04:05<05:59, 6.74MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  37%|█▍  | 1.44G/3.85G [04:06<05:54, 6.82MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.41G/9.97G [04:06<28:52, 4.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▌  | 1.45G/3.85G [04:08<06:08, 6.52MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.42G/9.97G [04:08<30:07, 4.73MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▌  | 1.46G/3.85G [04:10<06:11, 6.44MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.43G/9.97G [04:10<28:55, 4.92MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▌  | 1.47G/3.85G [04:11<06:22, 6.23MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  14%|▌   | 1.44G/9.97G [04:12<27:16, 5.21MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  38%|█▌  | 1.48G/3.85G [04:13<06:31, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.45G/9.97G [04:14<26:14, 5.41MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|█▌  | 1.49G/3.85G [04:15<06:44, 5.84MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.46G/9.97G [04:15<24:39, 5.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.47G/9.97G [04:17<23:23, 6.05MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|█▌  | 1.50G/3.85G [04:17<07:05, 5.54MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.48G/9.97G [04:19<23:31, 6.01MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|█▌  | 1.51G/3.85G [04:20<07:33, 5.16MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.49G/9.97G [04:20<22:52, 6.18MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  39%|█▌  | 1.52G/3.85G [04:22<07:42, 5.05MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.50G/9.97G [04:22<22:17, 6.33MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.51G/9.97G [04:23<21:28, 6.56MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|█▌  | 1.53G/3.85G [04:24<07:34, 5.10MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.52G/9.97G [04:25<21:59, 6.40MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|█▌  | 1.54G/3.85G [04:26<07:16, 5.29MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.53G/9.97G [04:27<22:29, 6.25MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  40%|█▌  | 1.55G/3.85G [04:27<07:02, 5.44MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  15%|▌   | 1.54G/9.97G [04:29<23:53, 5.88MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|█▌  | 1.56G/3.85G [04:30<07:11, 5.31MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▌   | 1.55G/9.97G [04:31<23:35, 5.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|█▋  | 1.57G/3.85G [04:31<06:57, 5.46MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.56G/9.97G [04:32<23:45, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|█▋  | 1.58G/3.85G [04:33<06:44, 5.61MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  41%|█▋  | 1.59G/3.85G [04:35<06:17, 5.98MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.57G/9.97G [04:35<26:53, 5.20MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|█▋  | 1.60G/3.85G [04:36<05:43, 6.55MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|█▋  | 1.61G/3.85G [04:37<05:35, 6.66MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.58G/9.97G [04:38<29:31, 4.73MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|█▋  | 1.63G/3.85G [04:39<05:29, 6.76MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.59G/9.97G [04:40<28:12, 4.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  42%|█▋  | 1.64G/3.85G [04:41<05:45, 6.43MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.60G/9.97G [04:41<26:39, 5.23MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|█▋  | 1.65G/3.85G [04:43<06:07, 6.00MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.61G/9.97G [04:43<24:36, 5.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.63G/9.97G [04:44<23:00, 6.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|█▋  | 1.66G/3.85G [04:45<07:01, 5.21MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  16%|▋   | 1.64G/9.97G [04:46<22:50, 6.08MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  43%|█▋  | 1.67G/3.85G [04:47<06:44, 5.41MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.65G/9.97G [04:48<25:01, 5.54MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|█▋  | 1.68G/3.85G [04:48<06:09, 5.89MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|█▊  | 1.69G/3.85G [04:50<05:48, 6.22MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.66G/9.97G [04:50<25:48, 5.37MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|█▊  | 1.70G/3.85G [04:52<05:42, 6.29MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.67G/9.97G [04:52<26:16, 5.26MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  44%|█▊  | 1.71G/3.85G [04:53<05:38, 6.34MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.68G/9.97G [04:54<26:23, 5.24MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|█▊  | 1.72G/3.85G [04:55<05:31, 6.44MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|█▊  | 1.73G/3.85G [04:57<05:48, 6.08MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.69G/9.97G [04:57<27:14, 5.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|█▊  | 1.74G/3.85G [04:58<05:38, 6.24MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.70G/9.97G [04:59<26:57, 5.11MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  45%|█▊  | 1.75G/3.85G [05:00<05:35, 6.26MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.71G/9.97G [05:01<26:43, 5.15MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|█▊  | 1.76G/3.85G [05:02<05:32, 6.28MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.72G/9.97G [05:03<26:20, 5.22MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|█▊  | 1.77G/3.85G [05:03<05:28, 6.33MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.73G/9.97G [05:05<25:49, 5.32MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  46%|█▊  | 1.78G/3.85G [05:05<05:28, 6.29MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  17%|▋   | 1.74G/9.97G [05:07<26:25, 5.19MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|█▊  | 1.79G/3.85G [05:07<05:51, 5.87MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.75G/9.97G [05:09<25:45, 5.32MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|█▊  | 1.80G/3.85G [05:09<05:44, 5.94MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|█▉  | 1.81G/3.85G [05:10<05:38, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.76G/9.97G [05:10<25:24, 5.38MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  47%|█▉  | 1.82G/3.85G [05:12<05:31, 6.12MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.77G/9.97G [05:12<25:41, 5.32MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 1.84G/3.85G [05:14<05:23, 6.23MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.78G/9.97G [05:15<26:08, 5.22MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 1.85G/3.85G [05:15<05:15, 6.36MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.79G/9.97G [05:17<26:52, 5.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 1.86G/3.85G [05:17<05:28, 6.09MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.80G/9.97G [05:18<25:15, 5.39MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  48%|█▉  | 1.87G/3.85G [05:19<05:50, 5.66MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.81G/9.97G [05:20<24:08, 5.63MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 1.88G/3.85G [05:21<06:01, 5.47MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.82G/9.97G [05:22<23:02, 5.89MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 1.89G/3.85G [05:23<05:58, 5.49MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  18%|▋   | 1.84G/9.97G [05:23<22:38, 5.98MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  49%|█▉  | 1.90G/3.85G [05:25<05:55, 5.50MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▋   | 1.85G/9.97G [05:25<22:20, 6.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▋   | 1.86G/9.97G [05:27<22:05, 6.12MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|█▉  | 1.91G/3.85G [05:28<06:35, 4.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▋   | 1.87G/9.97G [05:28<20:18, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.88G/9.97G [05:29<19:33, 6.89MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|█▉  | 1.92G/3.85G [05:30<06:54, 4.67MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.89G/9.97G [05:31<18:56, 7.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.90G/9.97G [05:32<18:49, 7.15MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|██  | 1.93G/3.85G [05:33<07:04, 4.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.91G/9.97G [05:34<18:27, 7.28MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  50%|██  | 1.94G/3.85G [05:35<06:58, 4.57MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.92G/9.97G [05:35<18:35, 7.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.93G/9.97G [05:37<18:58, 7.06MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 1.95G/3.85G [05:38<07:06, 4.46MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  19%|▊   | 1.94G/9.97G [05:38<19:30, 6.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 1.95G/9.97G [05:40<19:14, 6.95MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 1.96G/3.85G [05:40<07:07, 4.43MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 1.96G/9.97G [05:41<19:05, 6.99MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 1.97G/3.85G [05:42<07:01, 4.46MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 1.97G/9.97G [05:43<19:14, 6.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  51%|██  | 1.98G/3.85G [05:44<06:48, 4.58MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 1.98G/9.97G [05:44<19:22, 6.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 1.99G/9.97G [05:46<19:29, 6.82MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|██  | 1.99G/3.85G [05:46<06:34, 4.71MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 2.00G/9.97G [05:48<20:29, 6.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|██  | 2.00G/3.85G [05:49<06:35, 4.68MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 2.01G/9.97G [05:49<21:00, 6.31MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  52%|██  | 2.01G/3.85G [05:51<06:07, 5.00MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 2.02G/9.97G [05:51<22:11, 5.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|██  | 2.02G/3.85G [05:52<05:40, 5.36MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|██  | 2.03G/3.85G [05:54<05:15, 5.76MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  20%|▊   | 2.03G/9.97G [05:54<23:34, 5.61MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|██  | 2.04G/3.85G [05:55<04:55, 6.11MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.04G/9.97G [05:56<24:48, 5.32MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  53%|██▏ | 2.06G/3.85G [05:57<04:43, 6.34MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.06G/9.97G [05:58<26:46, 4.93MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▏ | 2.07G/3.85G [05:58<04:45, 6.26MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▏ | 2.08G/3.85G [06:00<04:41, 6.31MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.07G/9.97G [06:00<26:12, 5.02MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▏ | 2.09G/3.85G [06:02<04:36, 6.39MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.08G/9.97G [06:02<26:20, 4.99MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  54%|██▏ | 2.10G/3.85G [06:03<04:37, 6.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.09G/9.97G [06:04<25:47, 5.09MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  55%|██▏ | 2.11G/3.85G [06:05<04:29, 6.47MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  55%|██▏ | 2.12G/3.85G [06:06<04:29, 6.43MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.10G/9.97G [06:06<25:39, 5.11MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  55%|██▏ | 2.13G/3.85G [06:08<04:32, 6.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.11G/9.97G [06:08<25:34, 5.12MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▏ | 2.14G/3.85G [06:10<04:47, 5.97MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.12G/9.97G [06:10<24:34, 5.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.13G/9.97G [06:12<23:33, 5.55MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▏ | 2.15G/3.85G [06:12<04:56, 5.74MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  21%|▊   | 2.14G/9.97G [06:13<22:26, 5.81MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▏ | 2.16G/3.85G [06:14<04:57, 5.69MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▊   | 2.15G/9.97G [06:15<22:30, 5.79MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  56%|██▎ | 2.17G/3.85G [06:16<04:58, 5.64MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▊   | 2.16G/9.97G [06:17<22:21, 5.82MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▎ | 2.18G/3.85G [06:18<04:50, 5.74MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▊   | 2.17G/9.97G [06:19<22:54, 5.67MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▎ | 2.19G/3.85G [06:20<05:03, 5.47MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.18G/9.97G [06:21<22:35, 5.74MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▎ | 2.20G/3.85G [06:22<04:59, 5.51MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.19G/9.97G [06:22<21:50, 5.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  57%|██▎ | 2.21G/3.85G [06:24<04:59, 5.48MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.20G/9.97G [06:24<21:33, 6.00MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|██▎ | 2.22G/3.85G [06:25<04:56, 5.50MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.21G/9.97G [06:26<21:24, 6.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|██▎ | 2.23G/3.85G [06:27<04:52, 5.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.22G/9.97G [06:28<21:24, 6.03MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  58%|██▎ | 2.24G/3.85G [06:30<05:06, 5.25MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  22%|▉   | 2.23G/9.97G [06:30<22:22, 5.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.24G/9.97G [06:31<22:01, 5.84MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▎ | 2.25G/3.85G [06:32<05:04, 5.25MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.25G/9.97G [06:33<21:18, 6.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▎ | 2.26G/3.85G [06:34<05:02, 5.25MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.26G/9.97G [06:34<20:04, 6.39MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▎ | 2.28G/3.85G [06:36<05:03, 5.19MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.28G/9.97G [06:36<20:01, 6.40MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  59%|██▎ | 2.29G/3.85G [06:38<04:58, 5.25MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.29G/9.97G [06:38<20:03, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.30G/9.97G [06:40<21:07, 6.05MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██▍ | 2.30G/3.85G [06:40<05:08, 5.04MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.31G/9.97G [06:41<20:51, 6.12MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██▍ | 2.31G/3.85G [06:42<05:07, 5.03MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.32G/9.97G [06:43<19:58, 6.38MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██▍ | 2.32G/3.85G [06:44<04:59, 5.12MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.33G/9.97G [06:45<20:34, 6.19MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  60%|██▍ | 2.33G/3.85G [06:46<04:43, 5.39MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  23%|▉   | 2.34G/9.97G [06:46<21:17, 5.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|██▍ | 2.34G/3.85G [06:47<04:30, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.35G/9.97G [06:48<21:28, 5.91MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|██▍ | 2.35G/3.85G [06:49<04:36, 5.44MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.36G/9.97G [06:50<21:59, 5.76MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  61%|██▍ | 2.36G/3.85G [06:51<04:40, 5.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.37G/9.97G [06:52<21:24, 5.91MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|██▍ | 2.37G/3.85G [06:54<04:40, 5.28MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.38G/9.97G [06:54<21:01, 6.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.39G/9.97G [06:55<19:57, 6.33MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|██▍ | 2.38G/3.85G [06:56<04:42, 5.22MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.40G/9.97G [06:57<20:15, 6.23MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|██▍ | 2.39G/3.85G [06:57<04:35, 5.30MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.41G/9.97G [06:59<20:44, 6.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  62%|██▍ | 2.40G/3.85G [06:59<04:22, 5.52MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.42G/9.97G [07:01<21:59, 5.72MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|██▌ | 2.41G/3.85G [07:01<04:28, 5.37MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  24%|▉   | 2.43G/9.97G [07:02<21:52, 5.74MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|██▌ | 2.42G/3.85G [07:03<04:19, 5.51MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.44G/9.97G [07:04<21:47, 5.75MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|██▌ | 2.43G/3.85G [07:05<04:11, 5.66MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.45G/9.97G [07:06<21:48, 5.74MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  63%|██▌ | 2.44G/3.85G [07:07<04:07, 5.69MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.46G/9.97G [07:08<21:57, 5.69MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|██▌ | 2.45G/3.85G [07:08<04:01, 5.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.47G/9.97G [07:10<22:04, 5.66MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|██▌ | 2.46G/3.85G [07:10<04:07, 5.61MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|▉   | 2.49G/9.97G [07:12<22:45, 5.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  64%|██▌ | 2.47G/3.85G [07:12<04:02, 5.69MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|█   | 2.50G/9.97G [07:14<22:26, 5.55MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██▌ | 2.49G/3.85G [07:14<03:59, 5.70MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██▌ | 2.50G/3.85G [07:16<03:49, 5.91MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|█   | 2.51G/9.97G [07:16<22:14, 5.59MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██▌ | 2.51G/3.85G [07:17<03:44, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|█   | 2.52G/9.97G [07:18<22:30, 5.52MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  65%|██▌ | 2.52G/3.85G [07:19<03:44, 5.96MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|█   | 2.53G/9.97G [07:19<22:17, 5.56MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|██▌ | 2.53G/3.85G [07:21<03:51, 5.72MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  25%|█   | 2.54G/9.97G [07:21<22:49, 5.43MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|██▋ | 2.54G/3.85G [07:23<03:44, 5.87MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.55G/9.97G [07:23<22:50, 5.42MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|██▋ | 2.55G/3.85G [07:24<03:33, 6.11MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.56G/9.97G [07:25<23:03, 5.35MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  66%|██▋ | 2.56G/3.85G [07:26<03:33, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.57G/9.97G [07:27<22:46, 5.41MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██▋ | 2.57G/3.85G [07:28<03:34, 5.97MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.58G/9.97G [07:29<21:57, 5.61MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██▋ | 2.58G/3.85G [07:30<03:32, 5.99MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.59G/9.97G [07:31<22:40, 5.42MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██▋ | 2.59G/3.85G [07:32<03:43, 5.64MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.60G/9.97G [07:33<21:41, 5.66MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  67%|██▋ | 2.60G/3.85G [07:34<03:46, 5.53MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.61G/9.97G [07:35<21:45, 5.63MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██▋ | 2.61G/3.85G [07:35<03:39, 5.65MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.62G/9.97G [07:36<21:05, 5.80MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██▋ | 2.62G/3.85G [07:37<03:33, 5.78MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  26%|█   | 2.63G/9.97G [07:38<21:15, 5.75MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  68%|██▋ | 2.63G/3.85G [07:39<03:34, 5.68MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.64G/9.97G [07:40<20:19, 6.01MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██▋ | 2.64G/3.85G [07:41<03:45, 5.36MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.65G/9.97G [07:42<21:03, 5.79MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██▊ | 2.65G/3.85G [07:43<03:39, 5.47MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.66G/9.97G [07:44<21:20, 5.70MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██▊ | 2.66G/3.85G [07:45<03:24, 5.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.67G/9.97G [07:46<22:00, 5.52MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  69%|██▊ | 2.67G/3.85G [07:46<03:15, 6.03MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|██▊ | 2.68G/3.85G [07:48<03:02, 6.39MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.68G/9.97G [07:48<23:01, 5.27MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|██▊ | 2.69G/3.85G [07:49<02:58, 6.49MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.69G/9.97G [07:50<24:04, 5.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|██▊ | 2.71G/3.85G [07:51<02:52, 6.66MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  70%|██▊ | 2.72G/3.85G [07:52<02:53, 6.57MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.71G/9.97G [07:53<25:27, 4.75MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|██▊ | 2.73G/3.85G [07:54<02:47, 6.73MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.72G/9.97G [07:55<25:38, 4.71MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|██▊ | 2.74G/3.85G [07:55<02:43, 6.81MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  71%|██▊ | 2.75G/3.85G [07:57<02:44, 6.72MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.73G/9.97G [07:57<25:07, 4.80MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|██▊ | 2.76G/3.85G [07:59<02:46, 6.59MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  27%|█   | 2.74G/9.97G [07:59<23:37, 5.10MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|██▊ | 2.77G/3.85G [08:00<02:51, 6.32MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.75G/9.97G [08:01<23:17, 5.17MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|██▉ | 2.78G/3.85G [08:02<02:56, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.76G/9.97G [08:03<23:14, 5.17MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  72%|██▉ | 2.79G/3.85G [08:04<02:57, 5.98MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.77G/9.97G [08:04<22:05, 5.43MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|██▉ | 2.80G/3.85G [08:06<03:01, 5.81MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.78G/9.97G [08:06<21:44, 5.51MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|██▉ | 2.81G/3.85G [08:08<02:51, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.79G/9.97G [08:08<21:40, 5.52MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|██▉ | 2.82G/3.85G [08:09<02:50, 6.04MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█   | 2.80G/9.97G [08:10<21:25, 5.58MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  73%|██▉ | 2.83G/3.85G [08:11<02:49, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▏  | 2.81G/9.97G [08:12<22:03, 5.41MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|██▉ | 2.84G/3.85G [08:13<02:59, 5.63MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▏  | 2.82G/9.97G [08:14<21:13, 5.61MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|██▉ | 2.85G/3.85G [08:15<02:58, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  28%|█▏  | 2.83G/9.97G [08:15<20:09, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  74%|██▉ | 2.86G/3.85G [08:17<03:00, 5.47MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.84G/9.97G [08:17<19:55, 5.96MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|██▉ | 2.87G/3.85G [08:19<02:52, 5.67MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.85G/9.97G [08:19<20:30, 5.78MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|██▉ | 2.88G/3.85G [08:21<02:46, 5.83MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.86G/9.97G [08:21<20:23, 5.81MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███ | 2.89G/3.85G [08:23<02:50, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.87G/9.97G [08:23<21:16, 5.56MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  75%|███ | 2.90G/3.85G [08:24<02:46, 5.69MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.88G/9.97G [08:25<21:13, 5.56MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███ | 2.92G/3.85G [08:26<02:37, 5.93MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.89G/9.97G [08:27<21:57, 5.37MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███ | 2.93G/3.85G [08:27<02:28, 6.24MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███ | 2.94G/3.85G [08:29<02:23, 6.38MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.90G/9.97G [08:29<22:34, 5.22MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  76%|███ | 2.95G/3.85G [08:30<02:16, 6.63MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.92G/9.97G [08:31<23:40, 4.96MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███ | 2.96G/3.85G [08:32<02:11, 6.83MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███ | 2.97G/3.85G [08:34<02:15, 6.55MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.93G/9.97G [08:34<24:41, 4.75MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  77%|███ | 2.98G/3.85G [08:35<02:10, 6.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  29%|█▏  | 2.94G/9.97G [08:36<24:38, 4.76MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███ | 2.99G/3.85G [08:37<02:06, 6.82MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███ | 3.00G/3.85G [08:38<02:02, 6.95MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 2.95G/9.97G [08:38<25:01, 4.68MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███ | 3.01G/3.85G [08:39<02:00, 6.99MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 2.96G/9.97G [08:41<25:18, 4.62MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  78%|███▏| 3.02G/3.85G [08:41<01:57, 7.11MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███▏| 3.03G/3.85G [08:42<01:56, 7.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 2.97G/9.97G [08:43<26:31, 4.40MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███▏| 3.04G/3.85G [08:44<01:58, 6.83MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 2.98G/9.97G [08:45<25:54, 4.50MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███▏| 3.05G/3.85G [08:46<01:58, 6.76MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  79%|███▏| 3.06G/3.85G [08:47<02:01, 6.51MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 2.99G/9.97G [08:47<24:05, 4.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 3.00G/9.97G [08:49<22:42, 5.11MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|███▏| 3.07G/3.85G [08:49<02:05, 6.24MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 3.01G/9.97G [08:51<21:13, 5.46MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|███▏| 3.08G/3.85G [08:51<02:08, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 3.02G/9.97G [08:52<20:26, 5.67MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  80%|███▏| 3.09G/3.85G [08:53<02:15, 5.62MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  30%|█▏  | 3.03G/9.97G [08:54<20:44, 5.58MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|███▏| 3.10G/3.85G [08:55<02:14, 5.55MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.04G/9.97G [08:56<20:48, 5.55MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|███▏| 3.11G/3.85G [08:57<02:06, 5.86MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.05G/9.97G [08:58<20:57, 5.50MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|███▏| 3.12G/3.85G [08:58<02:01, 5.99MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  81%|███▎| 3.14G/3.85G [09:00<01:59, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.06G/9.97G [09:00<20:49, 5.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.07G/9.97G [09:02<20:24, 5.63MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|███▎| 3.15G/3.85G [09:02<01:59, 5.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.08G/9.97G [09:04<20:30, 5.60MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|███▎| 3.16G/3.85G [09:04<02:03, 5.65MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.09G/9.97G [09:06<20:19, 5.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|███▎| 3.17G/3.85G [09:06<02:00, 5.68MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  82%|███▎| 3.18G/3.85G [09:08<01:54, 5.88MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.10G/9.97G [09:07<20:29, 5.58MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|███▎| 3.19G/3.85G [09:09<01:50, 6.03MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▏  | 3.11G/9.97G [09:09<20:49, 5.49MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|███▎| 3.20G/3.85G [09:11<01:46, 6.13MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▎  | 3.12G/9.97G [09:12<21:18, 5.35MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  83%|███▎| 3.21G/3.85G [09:12<01:41, 6.34MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  31%|█▎  | 3.14G/9.97G [09:14<22:26, 5.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|███▎| 3.22G/3.85G [09:14<01:44, 6.09MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.15G/9.97G [09:16<21:42, 5.24MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|███▎| 3.23G/3.85G [09:16<01:40, 6.18MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.16G/9.97G [09:17<20:43, 5.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|███▎| 3.24G/3.85G [09:18<01:41, 6.04MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  84%|███▎| 3.25G/3.85G [09:19<01:40, 6.01MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.17G/9.97G [09:19<20:37, 5.49MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|███▍| 3.26G/3.85G [09:21<01:36, 6.13MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.18G/9.97G [09:21<20:36, 5.49MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|███▍| 3.27G/3.85G [09:23<01:34, 6.16MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.19G/9.97G [09:23<20:31, 5.50MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|███▍| 3.28G/3.85G [09:25<01:38, 5.79MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.20G/9.97G [09:25<20:41, 5.45MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  85%|███▍| 3.29G/3.85G [09:27<01:37, 5.77MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.21G/9.97G [09:27<19:50, 5.68MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  86%|███▍| 3.30G/3.85G [09:28<01:35, 5.73MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.22G/9.97G [09:29<19:39, 5.72MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  86%|███▍| 3.31G/3.85G [09:30<01:36, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  32%|█▎  | 3.23G/9.97G [09:30<19:55, 5.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  86%|███▍| 3.32G/3.85G [09:32<01:34, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.24G/9.97G [09:32<19:33, 5.73MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|███▍| 3.33G/3.85G [09:34<01:30, 5.71MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.25G/9.97G [09:34<20:50, 5.37MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|███▍| 3.34G/3.85G [09:36<01:23, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.26G/9.97G [09:37<21:32, 5.19MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|███▍| 3.36G/3.85G [09:37<01:18, 6.35MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  87%|███▍| 3.37G/3.85G [09:38<01:13, 6.66MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.27G/9.97G [09:39<21:39, 5.15MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|███▌| 3.38G/3.85G [09:40<01:11, 6.65MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.28G/9.97G [09:41<22:07, 5.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|███▌| 3.39G/3.85G [09:42<01:09, 6.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.29G/9.97G [09:43<22:21, 4.97MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|███▌| 3.40G/3.85G [09:43<01:09, 6.55MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  88%|███▌| 3.41G/3.85G [09:45<01:11, 6.21MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.30G/9.97G [09:45<21:50, 5.08MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  89%|███▌| 3.42G/3.85G [09:47<01:08, 6.39MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.31G/9.97G [09:47<21:57, 5.05MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  89%|███▌| 3.43G/3.85G [09:49<01:10, 6.03MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.32G/9.97G [09:50<23:52, 4.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  89%|███▌| 3.44G/3.85G [09:51<01:16, 5.41MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  33%|█▎  | 3.33G/9.97G [09:52<22:40, 4.87MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|███▌| 3.45G/3.85G [09:53<01:19, 5.10MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.34G/9.97G [09:53<20:49, 5.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.36G/9.97G [09:55<20:41, 5.33MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|███▌| 3.46G/3.85G [09:55<01:17, 5.07MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|███▌| 3.47G/3.85G [09:57<01:10, 5.40MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.37G/9.97G [09:58<24:33, 4.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  90%|███▌| 3.48G/3.85G [09:59<01:05, 5.70MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|███▋| 3.49G/3.85G [10:00<01:01, 5.89MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.38G/9.97G [10:02<27:05, 4.06MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|███▋| 3.50G/3.85G [10:02<00:59, 5.92MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|███▋| 3.51G/3.85G [10:04<00:57, 5.94MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.39G/9.97G [10:05<28:01, 3.91MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  91%|███▋| 3.52G/3.85G [10:06<00:58, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.40G/9.97G [10:07<28:49, 3.80MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  92%|███▋| 3.53G/3.85G [10:08<00:56, 5.61MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  92%|███▋| 3.54G/3.85G [10:10<00:54, 5.70MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.41G/9.97G [10:10<26:51, 4.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  92%|███▋| 3.55G/3.85G [10:11<00:51, 5.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▎  | 3.42G/9.97G [10:11<24:25, 4.47MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|███▋| 3.57G/3.85G [10:13<00:51, 5.60MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  34%|█▍  | 3.43G/9.97G [10:13<22:47, 4.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.44G/9.97G [10:15<21:28, 5.07MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|███▋| 3.58G/3.85G [10:15<00:49, 5.59MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.45G/9.97G [10:16<19:05, 5.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.46G/9.97G [10:18<17:46, 6.10MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|███▋| 3.59G/3.85G [10:19<01:00, 4.44MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.47G/9.97G [10:19<16:37, 6.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.48G/9.97G [10:21<16:26, 6.58MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  93%|███▋| 3.60G/3.85G [10:22<01:02, 4.09MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.49G/9.97G [10:22<16:30, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.50G/9.97G [10:24<17:07, 6.29MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|███▋| 3.61G/3.85G [10:25<01:02, 3.92MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.51G/9.97G [10:26<18:13, 5.90MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|███▊| 3.62G/3.85G [10:27<00:58, 4.02MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.52G/9.97G [10:28<19:33, 5.49MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|███▊| 3.63G/3.85G [10:29<00:52, 4.24MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  35%|█▍  | 3.53G/9.97G [10:30<20:02, 5.35MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  94%|███▊| 3.64G/3.85G [10:32<00:49, 4.35MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.54G/9.97G [10:33<20:27, 5.23MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|███▊| 3.65G/3.85G [10:34<00:45, 4.49MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.55G/9.97G [10:35<21:13, 5.04MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|███▊| 3.66G/3.85G [10:36<00:40, 4.80MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.57G/9.97G [10:38<23:00, 4.64MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  95%|███▊| 3.67G/3.85G [10:38<00:40, 4.48MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.58G/9.97G [10:40<24:14, 4.40MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|███▊| 3.68G/3.85G [10:41<00:39, 4.35MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.59G/9.97G [10:43<24:46, 4.29MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|███▊| 3.69G/3.85G [10:44<00:38, 4.21MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.60G/9.97G [10:45<25:37, 4.14MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|███▊| 3.70G/3.85G [10:46<00:34, 4.38MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  96%|███▊| 3.71G/3.85G [10:48<00:30, 4.66MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|███▊| 3.72G/3.85G [10:50<00:26, 4.88MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.61G/9.97G [10:50<30:10, 3.51MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|███▉| 3.73G/3.85G [10:51<00:23, 5.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.62G/9.97G [10:52<29:45, 3.56MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|███▉| 3.74G/3.85G [10:53<00:19, 5.52MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  97%|███▉| 3.75G/3.85G [10:54<00:16, 6.07MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  36%|█▍  | 3.63G/9.97G [10:55<28:41, 3.68MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|███▉| 3.76G/3.85G [10:56<00:14, 6.23MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.64G/9.97G [10:57<26:45, 3.94MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|███▉| 3.77G/3.85G [10:58<00:12, 6.09MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.65G/9.97G [10:59<23:07, 4.55MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  98%|███▉| 3.79G/3.85G [11:00<00:12, 5.42MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.66G/9.97G [11:00<21:01, 5.00MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|███▉| 3.80G/3.85G [11:02<00:10, 5.44MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.67G/9.97G [11:02<20:01, 5.24MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|███▉| 3.81G/3.85G [11:04<00:08, 5.43MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.68G/9.97G [11:04<19:36, 5.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.69G/9.97G [11:06<19:04, 5.48MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|███▉| 3.82G/3.85G [11:06<00:06, 5.30MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.70G/9.97G [11:08<20:03, 5.21MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin:  99%|███▉| 3.83G/3.85G [11:08<00:05, 4.99MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.71G/9.97G [11:10<21:01, 4.96MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|███▉| 3.84G/3.85G [11:11<00:02, 4.96MB/s]\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|███▉| 3.85G/3.85G [11:13<00:00, 5.02MB/s]\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.72G/9.97G [11:13<21:59, 4.74MB/s]\u001b[A\u001b[A\n",
      "pytorch_model-00002-of-00002.bin: 100%|████| 3.85G/3.85G [11:13<00:00, 5.72MB/s]\u001b[A\n",
      "Download complete. Moving file to models/pytorch_model-00002-of-00002.bin\n",
      "\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  37%|█▍  | 3.73G/9.97G [11:14<20:10, 5.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.74G/9.97G [11:16<17:55, 5.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.75G/9.97G [11:17<16:31, 6.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.76G/9.97G [11:19<16:01, 6.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.77G/9.97G [11:20<14:03, 7.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.79G/9.97G [11:21<12:58, 7.95MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.80G/9.97G [11:22<13:18, 7.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.81G/9.97G [11:23<13:15, 7.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.82G/9.97G [11:25<12:52, 7.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  38%|█▌  | 3.83G/9.97G [11:26<13:31, 7.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.84G/9.97G [11:28<13:47, 7.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.85G/9.97G [11:30<15:00, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.86G/9.97G [11:31<15:33, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.87G/9.97G [11:33<15:30, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.88G/9.97G [11:34<15:26, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.89G/9.97G [11:36<15:14, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.90G/9.97G [11:38<15:22, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.91G/9.97G [11:39<15:25, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.92G/9.97G [11:40<14:26, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  39%|█▌  | 3.93G/9.97G [11:42<14:11, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 3.94G/9.97G [11:43<13:53, 7.23MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 3.95G/9.97G [11:45<13:46, 7.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 3.96G/9.97G [11:46<13:49, 7.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 3.97G/9.97G [11:48<13:56, 7.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 3.98G/9.97G [11:49<13:44, 7.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 4.00G/9.97G [11:51<15:07, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 4.01G/9.97G [11:53<15:07, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 4.02G/9.97G [11:54<15:22, 6.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 4.03G/9.97G [11:56<15:21, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  40%|█▌  | 4.04G/9.97G [11:58<15:13, 6.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▌  | 4.05G/9.97G [11:59<14:58, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.06G/9.97G [12:00<13:59, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.07G/9.97G [12:02<14:13, 6.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.08G/9.97G [12:03<13:36, 7.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.09G/9.97G [12:05<13:28, 7.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.10G/9.97G [12:06<13:43, 7.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.11G/9.97G [12:08<13:58, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.12G/9.97G [12:09<13:59, 6.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  41%|█▋  | 4.13G/9.97G [12:11<15:17, 6.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.14G/9.97G [12:13<15:46, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.15G/9.97G [12:15<15:42, 6.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.16G/9.97G [12:16<15:29, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.17G/9.97G [12:18<15:26, 6.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.18G/9.97G [12:20<15:06, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.19G/9.97G [12:21<15:37, 6.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.20G/9.97G [12:23<15:04, 6.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.22G/9.97G [12:25<14:46, 6.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.23G/9.97G [12:26<14:43, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  42%|█▋  | 4.24G/9.97G [12:28<14:19, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.25G/9.97G [12:29<14:06, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.26G/9.97G [12:31<14:40, 6.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.27G/9.97G [12:32<14:30, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.28G/9.97G [12:34<14:14, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.29G/9.97G [12:35<13:58, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.30G/9.97G [12:37<13:40, 6.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.31G/9.97G [12:38<13:30, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.32G/9.97G [12:40<13:26, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  43%|█▋  | 4.33G/9.97G [12:42<14:11, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▋  | 4.34G/9.97G [12:43<14:07, 6.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▋  | 4.35G/9.97G [12:45<13:45, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.36G/9.97G [12:46<13:46, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.37G/9.97G [12:48<13:29, 6.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.38G/9.97G [12:49<13:16, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.39G/9.97G [12:51<13:09, 7.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.40G/9.97G [12:52<14:04, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.41G/9.97G [12:54<14:03, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.42G/9.97G [12:55<13:34, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  44%|█▊  | 4.44G/9.97G [12:57<13:43, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.45G/9.97G [12:59<13:40, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.46G/9.97G [13:00<13:37, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.47G/9.97G [13:02<15:05, 6.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.48G/9.97G [13:04<14:52, 6.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.49G/9.97G [13:05<14:31, 6.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.50G/9.97G [13:07<14:19, 6.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.51G/9.97G [13:08<13:40, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.52G/9.97G [13:10<13:32, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  45%|█▊  | 4.53G/9.97G [13:11<13:16, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.54G/9.97G [13:13<13:32, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.55G/9.97G [13:15<13:30, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.56G/9.97G [13:16<13:04, 6.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.57G/9.97G [13:18<12:58, 6.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.58G/9.97G [13:19<13:01, 6.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.59G/9.97G [13:21<12:56, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.60G/9.97G [13:22<13:45, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.61G/9.97G [13:24<13:34, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.62G/9.97G [13:26<13:21, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  46%|█▊  | 4.63G/9.97G [13:27<13:04, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▊  | 4.65G/9.97G [13:29<13:10, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▊  | 4.66G/9.97G [13:30<13:05, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▊  | 4.67G/9.97G [13:32<13:05, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.68G/9.97G [13:34<13:44, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.69G/9.97G [13:35<13:57, 6.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.70G/9.97G [13:37<13:40, 6.43MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.71G/9.97G [13:38<13:25, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.72G/9.97G [13:40<13:50, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  47%|█▉  | 4.73G/9.97G [13:42<13:19, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.74G/9.97G [13:43<13:42, 6.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.75G/9.97G [13:45<13:10, 6.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.76G/9.97G [13:46<13:04, 6.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.77G/9.97G [13:48<12:50, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.78G/9.97G [13:49<12:48, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.79G/9.97G [13:51<12:22, 6.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.80G/9.97G [13:52<12:20, 6.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.81G/9.97G [13:54<13:11, 6.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.82G/9.97G [13:56<13:00, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  48%|█▉  | 4.83G/9.97G [13:57<12:34, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.84G/9.97G [13:59<12:44, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.85G/9.97G [14:00<12:27, 6.84MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.87G/9.97G [14:02<12:24, 6.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.88G/9.97G [14:03<12:50, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.89G/9.97G [14:05<12:48, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.90G/9.97G [14:07<12:26, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.91G/9.97G [14:08<12:13, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.92G/9.97G [14:09<11:50, 7.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  49%|█▉  | 4.93G/9.97G [14:11<11:39, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|█▉  | 4.94G/9.97G [14:12<11:28, 7.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|█▉  | 4.95G/9.97G [14:14<12:08, 6.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|█▉  | 4.96G/9.97G [14:15<12:01, 6.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|█▉  | 4.97G/9.97G [14:17<11:54, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|█▉  | 4.98G/9.97G [14:18<11:47, 7.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|██  | 4.99G/9.97G [14:20<11:35, 7.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|██  | 5.00G/9.97G [14:21<11:30, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|██  | 5.01G/9.97G [14:23<11:33, 7.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|██  | 5.02G/9.97G [14:24<11:11, 7.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  50%|██  | 5.03G/9.97G [14:25<11:17, 7.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.04G/9.97G [14:27<11:22, 7.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.05G/9.97G [14:28<11:16, 7.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.06G/9.97G [14:29<09:57, 8.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.08G/9.97G [14:30<09:02, 9.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.09G/9.97G [14:31<08:23, 9.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.10G/9.97G [14:32<07:56, 10.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.11G/9.97G [14:33<07:37, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.12G/9.97G [14:34<07:29, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  51%|██  | 5.13G/9.97G [14:35<07:11, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.14G/9.97G [14:35<07:06, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.15G/9.97G [14:36<07:07, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.16G/9.97G [14:37<06:54, 11.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.17G/9.97G [14:38<06:52, 11.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.18G/9.97G [14:39<07:24, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.19G/9.97G [14:40<07:13, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.20G/9.97G [14:41<07:06, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.21G/9.97G [14:42<06:59, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.22G/9.97G [14:43<07:22, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  52%|██  | 5.23G/9.97G [14:45<08:24, 9.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.24G/9.97G [14:46<09:03, 8.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.25G/9.97G [14:47<09:33, 8.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.26G/9.97G [14:49<10:43, 7.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.27G/9.97G [14:51<11:55, 6.56MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.28G/9.97G [14:52<10:45, 7.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██  | 5.30G/9.97G [14:53<09:29, 8.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██▏ | 5.31G/9.97G [14:54<08:45, 8.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██▏ | 5.32G/9.97G [14:55<08:08, 9.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  53%|██▏ | 5.33G/9.97G [14:56<07:41, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.34G/9.97G [14:57<07:20, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.35G/9.97G [14:58<07:06, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.36G/9.97G [14:59<06:55, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.37G/9.97G [15:00<06:48, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.38G/9.97G [15:01<06:54, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.39G/9.97G [15:01<06:44, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.40G/9.97G [15:03<07:16, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.41G/9.97G [15:04<07:09, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.42G/9.97G [15:04<06:59, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  54%|██▏ | 5.43G/9.97G [15:05<07:02, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.44G/9.97G [15:06<06:56, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.45G/9.97G [15:07<07:06, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.46G/9.97G [15:08<07:01, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.47G/9.97G [15:09<07:10, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.48G/9.97G [15:10<07:01, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.49G/9.97G [15:11<06:49, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.51G/9.97G [15:12<06:46, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.52G/9.97G [15:13<07:06, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  55%|██▏ | 5.53G/9.97G [15:14<06:58, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.54G/9.97G [15:15<07:00, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.55G/9.97G [15:17<07:23, 9.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.56G/9.97G [15:18<08:05, 9.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.57G/9.97G [15:19<08:15, 8.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.58G/9.97G [15:20<07:38, 9.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.59G/9.97G [15:21<07:18, 9.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▏ | 5.60G/9.97G [15:22<06:56, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▎ | 5.61G/9.97G [15:23<06:42, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▎ | 5.62G/9.97G [15:24<06:38, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  56%|██▎ | 5.63G/9.97G [15:25<06:29, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.64G/9.97G [15:26<06:25, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.65G/9.97G [15:26<06:24, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.66G/9.97G [15:27<06:21, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.67G/9.97G [15:28<06:24, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.68G/9.97G [15:29<06:28, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.69G/9.97G [15:31<07:03, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.70G/9.97G [15:32<07:02, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.71G/9.97G [15:33<07:04, 10.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  57%|██▎ | 5.73G/9.97G [15:34<07:01, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.74G/9.97G [15:35<06:43, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.75G/9.97G [15:36<06:37, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.76G/9.97G [15:36<06:28, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.77G/9.97G [15:37<06:22, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.78G/9.97G [15:38<06:30, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.79G/9.97G [15:39<06:36, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.80G/9.97G [15:40<06:40, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.81G/9.97G [15:41<06:29, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.82G/9.97G [15:42<06:32, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  58%|██▎ | 5.83G/9.97G [15:43<06:39, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.84G/9.97G [15:45<06:41, 10.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.85G/9.97G [15:45<06:25, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.86G/9.97G [15:46<06:14, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.87G/9.97G [15:47<06:15, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.88G/9.97G [15:48<06:01, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.89G/9.97G [15:49<06:02, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.90G/9.97G [15:50<05:53, 11.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▎ | 5.91G/9.97G [15:51<06:00, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  59%|██▍ | 5.92G/9.97G [15:52<06:08, 11.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.93G/9.97G [15:53<06:19, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.95G/9.97G [15:54<06:14, 10.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.96G/9.97G [15:55<06:56, 9.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.97G/9.97G [15:57<07:09, 9.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.98G/9.97G [15:58<07:10, 9.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 5.99G/9.97G [15:59<07:20, 9.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 6.00G/9.97G [16:00<06:52, 9.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 6.01G/9.97G [16:01<06:44, 9.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 6.02G/9.97G [16:02<06:49, 9.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  60%|██▍ | 6.03G/9.97G [16:03<06:47, 9.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.04G/9.97G [16:04<06:40, 9.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.05G/9.97G [16:05<06:39, 9.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.06G/9.97G [16:06<06:41, 9.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.07G/9.97G [16:07<06:36, 9.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.08G/9.97G [16:09<06:59, 9.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.09G/9.97G [16:10<07:49, 8.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.10G/9.97G [16:12<08:24, 7.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.11G/9.97G [16:13<08:48, 7.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  61%|██▍ | 6.12G/9.97G [16:15<08:48, 7.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.13G/9.97G [16:16<09:06, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.14G/9.97G [16:17<08:12, 7.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.16G/9.97G [16:19<07:41, 8.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.17G/9.97G [16:20<07:53, 8.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.18G/9.97G [16:21<07:37, 8.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.19G/9.97G [16:22<07:08, 8.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.20G/9.97G [16:23<07:00, 8.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.21G/9.97G [16:24<07:09, 8.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.22G/9.97G [16:26<07:16, 8.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  62%|██▍ | 6.23G/9.97G [16:27<07:32, 8.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.24G/9.97G [16:29<07:42, 8.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.25G/9.97G [16:30<07:36, 8.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.26G/9.97G [16:31<07:38, 8.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.27G/9.97G [16:32<07:35, 8.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.28G/9.97G [16:34<07:38, 8.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.29G/9.97G [16:35<07:55, 7.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.30G/9.97G [16:37<07:53, 7.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.31G/9.97G [16:38<08:07, 7.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  63%|██▌ | 6.32G/9.97G [16:39<08:02, 7.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.33G/9.97G [16:42<09:24, 6.44MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.34G/9.97G [16:44<09:56, 6.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.35G/9.97G [16:44<08:33, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.36G/9.97G [16:45<07:31, 7.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.38G/9.97G [16:46<06:55, 8.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.39G/9.97G [16:47<06:21, 9.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.40G/9.97G [16:48<06:27, 9.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.41G/9.97G [16:50<06:46, 8.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.42G/9.97G [16:51<06:42, 8.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  64%|██▌ | 6.43G/9.97G [16:52<06:40, 8.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.44G/9.97G [16:54<07:00, 8.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.45G/9.97G [16:55<07:04, 8.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.46G/9.97G [16:56<07:12, 8.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.47G/9.97G [16:58<07:18, 7.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.48G/9.97G [16:59<07:14, 8.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.49G/9.97G [17:00<07:19, 7.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.50G/9.97G [17:02<07:26, 7.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.51G/9.97G [17:03<07:39, 7.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  65%|██▌ | 6.52G/9.97G [17:05<08:13, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▌ | 6.53G/9.97G [17:06<08:02, 7.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.54G/9.97G [17:08<08:05, 7.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.55G/9.97G [17:09<08:06, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.56G/9.97G [17:11<08:17, 6.84MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.57G/9.97G [17:12<08:20, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.59G/9.97G [17:14<08:26, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.60G/9.97G [17:16<08:10, 6.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.61G/9.97G [17:17<08:06, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.62G/9.97G [17:19<08:07, 6.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  66%|██▋ | 6.63G/9.97G [17:20<07:55, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.64G/9.97G [17:21<07:53, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.65G/9.97G [17:23<07:53, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.66G/9.97G [17:25<08:27, 6.52MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.67G/9.97G [17:26<08:04, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.68G/9.97G [17:28<07:55, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.69G/9.97G [17:29<08:00, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.70G/9.97G [17:31<07:53, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.71G/9.97G [17:32<08:01, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  67%|██▋ | 6.72G/9.97G [17:34<07:48, 6.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.73G/9.97G [17:35<07:54, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.74G/9.97G [17:37<07:49, 6.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.75G/9.97G [17:38<07:46, 6.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.76G/9.97G [17:40<07:44, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.77G/9.97G [17:42<07:48, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.78G/9.97G [17:43<07:47, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.79G/9.97G [17:45<07:50, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.81G/9.97G [17:47<09:05, 5.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.82G/9.97G [17:49<09:38, 5.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  68%|██▋ | 6.83G/9.97G [17:51<09:51, 5.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▋ | 6.84G/9.97G [17:53<09:33, 5.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▋ | 6.85G/9.97G [17:55<09:15, 5.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.86G/9.97G [17:57<09:34, 5.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.87G/9.97G [17:59<09:15, 5.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.88G/9.97G [18:00<08:55, 5.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.89G/9.97G [18:02<08:45, 5.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.90G/9.97G [18:04<08:27, 6.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.91G/9.97G [18:05<08:01, 6.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  69%|██▊ | 6.92G/9.97G [18:06<07:32, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.93G/9.97G [18:08<07:11, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.94G/9.97G [18:09<06:56, 7.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.95G/9.97G [18:11<06:56, 7.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.96G/9.97G [18:12<06:45, 7.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.97G/9.97G [18:13<06:50, 7.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.98G/9.97G [18:15<06:54, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 6.99G/9.97G [18:17<07:47, 6.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 7.00G/9.97G [18:19<07:32, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 7.01G/9.97G [18:20<07:33, 6.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  70%|██▊ | 7.03G/9.97G [18:22<07:35, 6.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.04G/9.97G [18:24<07:40, 6.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.05G/9.97G [18:25<07:46, 6.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.06G/9.97G [18:27<07:33, 6.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.07G/9.97G [18:28<07:01, 6.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.08G/9.97G [18:30<06:53, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.09G/9.97G [18:31<06:51, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.10G/9.97G [18:33<06:50, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.11G/9.97G [18:34<06:50, 6.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  71%|██▊ | 7.12G/9.97G [18:36<06:46, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▊ | 7.13G/9.97G [18:37<07:21, 6.42MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▊ | 7.14G/9.97G [18:39<07:22, 6.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▊ | 7.15G/9.97G [18:41<07:27, 6.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▊ | 7.16G/9.97G [18:43<07:29, 6.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.17G/9.97G [18:44<07:24, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.18G/9.97G [18:46<07:05, 6.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.19G/9.97G [18:47<07:12, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.20G/9.97G [18:49<06:59, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.21G/9.97G [18:50<06:44, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  72%|██▉ | 7.22G/9.97G [18:52<06:42, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.24G/9.97G [18:53<06:34, 6.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.25G/9.97G [18:55<06:29, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.26G/9.97G [18:56<06:23, 7.07MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.27G/9.97G [18:58<06:40, 6.74MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.28G/9.97G [18:59<06:34, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.29G/9.97G [19:01<06:20, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.30G/9.97G [19:02<06:19, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.31G/9.97G [19:04<06:17, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  73%|██▉ | 7.32G/9.97G [19:05<06:11, 7.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.33G/9.97G [19:07<06:12, 7.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.34G/9.97G [19:09<06:54, 6.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.35G/9.97G [19:10<06:42, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.36G/9.97G [19:12<06:33, 6.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.37G/9.97G [19:13<06:30, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.38G/9.97G [19:15<06:27, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.39G/9.97G [19:16<06:20, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.40G/9.97G [19:18<06:40, 6.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.41G/9.97G [19:20<06:23, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  74%|██▉ | 7.42G/9.97G [19:21<05:32, 7.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|██▉ | 7.43G/9.97G [19:21<04:58, 8.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|██▉ | 7.44G/9.97G [19:22<04:30, 9.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|██▉ | 7.46G/9.97G [19:23<04:13, 9.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|██▉ | 7.47G/9.97G [19:24<04:02, 10.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|███ | 7.48G/9.97G [19:25<03:50, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|███ | 7.49G/9.97G [19:26<03:44, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|███ | 7.50G/9.97G [19:27<03:39, 11.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|███ | 7.51G/9.97G [19:28<03:35, 11.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  75%|███ | 7.52G/9.97G [19:29<03:35, 11.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.53G/9.97G [19:30<03:56, 10.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.54G/9.97G [19:32<04:44, 8.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.55G/9.97G [19:33<05:20, 7.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.56G/9.97G [19:35<05:50, 6.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.57G/9.97G [19:37<06:00, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.58G/9.97G [19:38<05:58, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.59G/9.97G [19:40<06:06, 6.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.60G/9.97G [19:42<05:59, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.61G/9.97G [19:43<05:58, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  76%|███ | 7.62G/9.97G [19:45<06:00, 6.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.63G/9.97G [19:46<05:43, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.64G/9.97G [19:48<05:45, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.65G/9.97G [19:50<05:47, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.67G/9.97G [19:51<06:02, 6.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.68G/9.97G [19:53<05:54, 6.46MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.69G/9.97G [19:54<05:46, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.70G/9.97G [19:56<05:38, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.71G/9.97G [19:57<05:32, 6.79MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  77%|███ | 7.72G/9.97G [19:59<05:32, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.73G/9.97G [20:01<05:46, 6.47MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.74G/9.97G [20:02<05:38, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.75G/9.97G [20:04<05:38, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.76G/9.97G [20:05<05:30, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.77G/9.97G [20:07<05:21, 6.84MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███ | 7.78G/9.97G [20:08<05:12, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▏| 7.79G/9.97G [20:10<05:11, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▏| 7.80G/9.97G [20:11<05:13, 6.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▏| 7.81G/9.97G [20:13<05:04, 7.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  78%|███▏| 7.82G/9.97G [20:14<04:57, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.83G/9.97G [20:16<04:56, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.84G/9.97G [20:17<04:57, 7.15MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.85G/9.97G [20:19<04:54, 7.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.86G/9.97G [20:20<04:59, 7.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.87G/9.97G [20:22<05:05, 6.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.89G/9.97G [20:23<04:58, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.90G/9.97G [20:25<04:54, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.91G/9.97G [20:26<04:51, 7.07MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  79%|███▏| 7.92G/9.97G [20:28<04:46, 7.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.93G/9.97G [20:29<04:47, 7.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.94G/9.97G [20:30<04:37, 7.33MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.95G/9.97G [20:32<04:56, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.96G/9.97G [20:34<04:49, 6.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.97G/9.97G [20:35<04:44, 7.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.98G/9.97G [20:37<04:48, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 7.99G/9.97G [20:38<04:44, 6.95MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 8.00G/9.97G [20:40<04:44, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 8.01G/9.97G [20:41<04:38, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  80%|███▏| 8.02G/9.97G [20:43<05:14, 6.19MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.03G/9.97G [20:45<05:12, 6.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.04G/9.97G [20:47<05:11, 6.18MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.05G/9.97G [20:48<04:59, 6.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.06G/9.97G [20:50<04:48, 6.60MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.07G/9.97G [20:51<04:47, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.08G/9.97G [20:53<04:32, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▏| 8.10G/9.97G [20:54<04:23, 7.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▎| 8.11G/9.97G [20:55<04:23, 7.08MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  81%|███▎| 8.12G/9.97G [20:57<04:20, 7.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.13G/9.97G [20:58<04:18, 7.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.14G/9.97G [21:00<04:18, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.15G/9.97G [21:01<04:15, 7.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.16G/9.97G [21:03<04:33, 6.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.17G/9.97G [21:05<04:28, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.18G/9.97G [21:06<04:21, 6.85MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.19G/9.97G [21:08<04:18, 6.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.20G/9.97G [21:09<04:19, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.21G/9.97G [21:11<04:13, 6.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  82%|███▎| 8.22G/9.97G [21:12<04:10, 6.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.23G/9.97G [21:14<04:18, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.24G/9.97G [21:15<04:18, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.25G/9.97G [21:17<04:08, 6.90MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.26G/9.97G [21:18<04:08, 6.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.27G/9.97G [21:20<04:08, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.28G/9.97G [21:21<04:04, 6.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.29G/9.97G [21:23<04:16, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.30G/9.97G [21:25<04:06, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  83%|███▎| 8.32G/9.97G [21:26<04:01, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.33G/9.97G [21:28<03:56, 6.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.34G/9.97G [21:29<03:51, 7.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.35G/9.97G [21:31<03:50, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.36G/9.97G [21:32<03:47, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.37G/9.97G [21:34<03:57, 6.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.38G/9.97G [21:35<03:53, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.39G/9.97G [21:37<03:46, 6.96MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.40G/9.97G [21:38<03:19, 7.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▎| 8.41G/9.97G [21:38<02:58, 8.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  84%|███▍| 8.42G/9.97G [21:39<02:45, 9.33MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.43G/9.97G [21:40<02:32, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.44G/9.97G [21:41<02:25, 10.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.45G/9.97G [21:42<02:19, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.46G/9.97G [21:43<02:15, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.47G/9.97G [21:44<02:23, 10.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.48G/9.97G [21:45<02:17, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.49G/9.97G [21:46<02:25, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.50G/9.97G [21:48<02:43, 8.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  85%|███▍| 8.51G/9.97G [21:49<02:57, 8.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.52G/9.97G [21:51<02:59, 8.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.54G/9.97G [21:52<03:07, 7.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.55G/9.97G [21:54<03:28, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.56G/9.97G [21:56<03:47, 6.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.57G/9.97G [21:58<04:08, 5.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.58G/9.97G [22:00<04:13, 5.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.59G/9.97G [22:02<04:09, 5.54MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.60G/9.97G [22:04<03:48, 5.99MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.61G/9.97G [22:05<03:34, 6.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  86%|███▍| 8.62G/9.97G [22:06<03:23, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.63G/9.97G [22:08<03:18, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.64G/9.97G [22:09<03:14, 6.84MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.65G/9.97G [22:11<03:14, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.66G/9.97G [22:13<03:14, 6.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.67G/9.97G [22:14<03:21, 6.43MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.68G/9.97G [22:16<03:15, 6.58MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.69G/9.97G [22:17<03:12, 6.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.70G/9.97G [22:19<03:03, 6.89MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  87%|███▍| 8.71G/9.97G [22:20<03:01, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.72G/9.97G [22:22<02:58, 6.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.73G/9.97G [22:23<02:54, 7.07MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.75G/9.97G [22:24<02:35, 7.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.76G/9.97G [22:25<02:18, 8.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.77G/9.97G [22:26<02:06, 9.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.78G/9.97G [22:27<02:10, 9.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.79G/9.97G [22:29<02:24, 8.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.80G/9.97G [22:30<02:30, 7.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.81G/9.97G [22:32<02:37, 7.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  88%|███▌| 8.82G/9.97G [22:33<02:38, 7.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.83G/9.97G [22:35<02:53, 6.56MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.84G/9.97G [22:37<02:57, 6.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.85G/9.97G [22:39<02:52, 6.47MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.86G/9.97G [22:40<02:30, 7.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.87G/9.97G [22:41<02:25, 7.56MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.88G/9.97G [22:42<02:26, 7.41MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.89G/9.97G [22:44<02:26, 7.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.90G/9.97G [22:45<02:22, 7.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  89%|███▌| 8.91G/9.97G [22:47<02:20, 7.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.92G/9.97G [22:48<02:22, 7.35MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.93G/9.97G [22:50<02:31, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.94G/9.97G [22:52<02:33, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.95G/9.97G [22:53<02:29, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.97G/9.97G [22:55<02:24, 6.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.98G/9.97G [22:56<02:22, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 8.99G/9.97G [22:57<02:19, 7.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 9.00G/9.97G [22:59<02:17, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 9.01G/9.97G [23:01<02:24, 6.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  90%|███▌| 9.02G/9.97G [23:02<02:25, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▌| 9.03G/9.97G [23:04<02:21, 6.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.04G/9.97G [23:05<02:17, 6.75MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.05G/9.97G [23:07<02:16, 6.71MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.06G/9.97G [23:08<02:13, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.07G/9.97G [23:10<02:17, 6.53MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.08G/9.97G [23:12<02:13, 6.67MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.09G/9.97G [23:13<02:08, 6.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.10G/9.97G [23:15<02:07, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  91%|███▋| 9.11G/9.97G [23:16<02:02, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.12G/9.97G [23:18<02:04, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.13G/9.97G [23:19<01:59, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.14G/9.97G [23:21<02:05, 6.59MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.15G/9.97G [23:23<02:02, 6.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.16G/9.97G [23:24<01:59, 6.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.18G/9.97G [23:26<01:56, 6.80MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.19G/9.97G [23:27<01:52, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.20G/9.97G [23:29<01:52, 6.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.21G/9.97G [23:30<01:54, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  92%|███▋| 9.22G/9.97G [23:32<01:52, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.23G/9.97G [23:33<01:50, 6.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.24G/9.97G [23:35<01:47, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.25G/9.97G [23:36<01:44, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.26G/9.97G [23:38<01:39, 7.16MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.27G/9.97G [23:39<01:26, 8.10MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.28G/9.97G [23:39<01:17, 8.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.29G/9.97G [23:40<01:10, 9.62MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.30G/9.97G [23:41<01:05, 10.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  93%|███▋| 9.31G/9.97G [23:42<01:05, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▋| 9.32G/9.97G [23:44<01:10, 9.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▋| 9.33G/9.97G [23:45<01:12, 8.72MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▋| 9.34G/9.97G [23:47<01:17, 8.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.35G/9.97G [23:48<01:20, 7.63MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.36G/9.97G [23:50<01:21, 7.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.37G/9.97G [23:51<01:22, 7.20MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.38G/9.97G [23:53<01:22, 7.07MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.40G/9.97G [23:54<01:21, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.41G/9.97G [23:56<01:19, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  94%|███▊| 9.42G/9.97G [23:57<01:19, 6.92MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.43G/9.97G [23:59<01:21, 6.65MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.44G/9.97G [24:01<01:19, 6.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.45G/9.97G [24:02<01:16, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.46G/9.97G [24:04<01:15, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.47G/9.97G [24:05<01:15, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.48G/9.97G [24:07<01:12, 6.76MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.49G/9.97G [24:08<01:07, 7.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.50G/9.97G [24:10<01:09, 6.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  95%|███▊| 9.51G/9.97G [24:11<01:06, 6.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.52G/9.97G [24:13<01:03, 7.09MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.53G/9.97G [24:14<01:00, 7.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.54G/9.97G [24:15<00:59, 7.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.55G/9.97G [24:17<00:58, 7.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.56G/9.97G [24:18<00:56, 7.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.57G/9.97G [24:20<00:57, 6.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.58G/9.97G [24:21<00:55, 6.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.59G/9.97G [24:23<00:53, 7.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.60G/9.97G [24:24<00:51, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  96%|███▊| 9.62G/9.97G [24:26<00:49, 7.13MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▊| 9.63G/9.97G [24:27<00:48, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▊| 9.64G/9.97G [24:29<00:49, 6.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▊| 9.65G/9.97G [24:31<00:48, 6.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.66G/9.97G [24:32<00:46, 6.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.67G/9.97G [24:34<00:43, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.68G/9.97G [24:35<00:42, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.69G/9.97G [24:37<00:40, 6.83MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.70G/9.97G [24:38<00:38, 6.98MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  97%|███▉| 9.71G/9.97G [24:40<00:39, 6.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.72G/9.97G [24:41<00:36, 6.82MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.73G/9.97G [24:43<00:34, 6.85MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.74G/9.97G [24:44<00:32, 7.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.75G/9.97G [24:46<00:29, 7.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.76G/9.97G [24:47<00:25, 8.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.77G/9.97G [24:48<00:21, 9.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.78G/9.97G [24:48<00:19, 9.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.79G/9.97G [24:49<00:17, 10.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.80G/9.97G [24:50<00:16, 9.93MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  98%|███▉| 9.81G/9.97G [24:52<00:16, 9.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.83G/9.97G [24:53<00:16, 8.86MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.84G/9.97G [24:54<00:15, 8.36MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.85G/9.97G [24:56<00:16, 7.39MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.86G/9.97G [24:58<00:16, 6.78MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.87G/9.97G [25:00<00:16, 6.31MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.88G/9.97G [25:02<00:14, 6.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.89G/9.97G [25:03<00:12, 6.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.90G/9.97G [25:05<00:11, 6.21MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin:  99%|███▉| 9.91G/9.97G [25:07<00:09, 6.17MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.92G/9.97G [25:08<00:07, 6.25MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.93G/9.97G [25:10<00:06, 6.32MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.94G/9.97G [25:12<00:04, 6.29MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.95G/9.97G [25:13<00:02, 6.56MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.96G/9.97G [25:15<00:01, 6.57MB/s]\u001b[A\u001b[A\n",
      "\n",
      "pytorch_model-00001-of-00002.bin: 100%|████| 9.97G/9.97G [25:16<00:00, 6.57MB/s]\u001b[A\u001b[A\n",
      "Download complete. Moving file to models/pytorch_model-00001-of-00002.bin\n",
      "Fetching 9 files: 100%|██████████████████████████| 9/9 [25:18<00:00, 168.69s/it]\n",
      "/home/byh/gpt/models\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli download deepseek-ai/deepseek-llm-7b-chat --local-dir ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c169a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting huggingface_hub\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/31/a0/651f93d154cb72323358bf2bbae3e642bdb5d2f1bfc874d096f7cb159fa0/huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from huggingface_hub)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/42/14/42b2651a2f46b022ccd948bca9f2d5af0fd8929c4eec235b8d6d844fbe67/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/71/70db47e4f6ce3e5c37a607355f80da8860a33226be640226ac52cb05ef2e/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /home/byh/miniconda3/lib/python3.9/site-packages (from huggingface_hub) (23.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/b9/793686b2d54b531203c160ef12bec60228a0109c79bae6c1277961026770/pyyaml-6.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (750 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.8/750.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/byh/miniconda3/lib/python3.9/site-packages (from huggingface_hub) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/byh/miniconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/byh/miniconda3/lib/python3.9/site-packages (from huggingface_hub) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/15/07/86397573efefff941e100367bbda0b21496ffcdb34db7ab51912994c32a2/hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/byh/miniconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/byh/miniconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/byh/miniconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/byh/miniconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2025.8.3)\n",
      "Installing collected packages: pyyaml, hf-xet, fsspec, filelock, huggingface_hub\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 pyyaml-6.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a4fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-10-14 21:45:27] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,648 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2337] 2025-10-14 21:45:27,782 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:763] 2025-10-14 21:45:27,785 >> loading configuration file /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-10-14 21:45:27,786 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2066] 2025-10-14 21:45:27,786 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2337] 2025-10-14 21:45:27,914 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-10-14 21:45:27] llamafactory.data.loader:143 >> Loading dataset deepseek_data.jsonl...\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'json' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/byh/miniconda3/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
      "    launcher.launch()\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/launcher.py\", line 152, in launch\n",
      "    run_exp()\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
      "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
      "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/data/loader.py\", line 304, in get_dataset\n",
      "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/data/loader.py\", line 182, in _get_merged_dataset\n",
      "    datasets[dataset_name] = _load_single_dataset(dataset_attr, model_args, data_args, training_args)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/data/loader.py\", line 131, in _load_single_dataset\n",
      "    dataset = load_dataset(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 1392, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 1132, in load_dataset_builder\n",
      "    dataset_module = dataset_module_factory(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 906, in dataset_module_factory\n",
      "    return PackagedDatasetModuleFactory(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 517, in __init__\n",
      "    increase_load_count(name)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/datasets/load.py\", line 192, in increase_load_count\n",
      "    get_session().head(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/requests/sessions.py\", line 624, in head\n",
      "    return self.request(\"HEAD\", url, **kwargs)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_http.py\", line 95, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/requests/adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/site-packages/urllib3/connection.py\", line 565, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/http/client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/http/client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/http/client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/byh/miniconda3/lib/python3.9/ssl.py\", line 1100, in read\n",
      "^C\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train /home/byh/gpt/LLaMA-Factory/examples/train_lora/deep_seek_r1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aaaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"./output/lora_weights\")\n",
    "model = model.merge_and_unload()  # 合并到原始模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d268f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: api.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "              [--adapter_name_or_path ADAPTER_NAME_OR_PATH]\n",
      "              [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR]\n",
      "              [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
      "              [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]]\n",
      "              [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]\n",
      "              [--add_tokens ADD_TOKENS]\n",
      "              [--add_special_tokens ADD_SPECIAL_TOKENS]\n",
      "              [--new_special_tokens_config NEW_SPECIAL_TOKENS_CONFIG]\n",
      "              [--init_special_tokens {noise_init,desc_init,desc_init_w_noise}]\n",
      "              [--model_revision MODEL_REVISION]\n",
      "              [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]\n",
      "              [--no_low_cpu_mem_usage]\n",
      "              [--rope_scaling {linear,dynamic,yarn,llama3}]\n",
      "              [--flash_attn {auto,disabled,sdpa,fa2}]\n",
      "              [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}]\n",
      "              [--use_unsloth [USE_UNSLOTH]]\n",
      "              [--use_unsloth_gc [USE_UNSLOTH_GC]]\n",
      "              [--enable_liger_kernel [ENABLE_LIGER_KERNEL]]\n",
      "              [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]\n",
      "              [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]\n",
      "              [--use_reentrant_gc [USE_REENTRANT_GC]] [--no_use_reentrant_gc]\n",
      "              [--upcast_layernorm [UPCAST_LAYERNORM]]\n",
      "              [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]\n",
      "              [--train_from_scratch [TRAIN_FROM_SCRATCH]]\n",
      "              [--infer_backend {huggingface,vllm,sglang}]\n",
      "              [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]]\n",
      "              [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}]\n",
      "              [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]\n",
      "              [--om_hub_token OM_HUB_TOKEN]\n",
      "              [--print_param_status [PRINT_PARAM_STATUS]]\n",
      "              [--trust_remote_code [TRUST_REMOTE_CODE]]\n",
      "              [--quantization_method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}]\n",
      "              [--quantization_bit QUANTIZATION_BIT]\n",
      "              [--quantization_type {fp4,nf4}]\n",
      "              [--double_quantization [DOUBLE_QUANTIZATION]]\n",
      "              [--no_double_quantization] [--quantization_device_map {auto}]\n",
      "              [--fp8 [FP8]] [--fp8_backend FP8_BACKEND]\n",
      "              [--fp8_enable_fsdp_float8_all_gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER]]\n",
      "              [--image_max_pixels IMAGE_MAX_PIXELS]\n",
      "              [--image_min_pixels IMAGE_MIN_PIXELS]\n",
      "              [--image_do_pan_and_scan [IMAGE_DO_PAN_AND_SCAN]]\n",
      "              [--crop_to_patches [CROP_TO_PATCHES]]\n",
      "              [--video_max_pixels VIDEO_MAX_PIXELS]\n",
      "              [--video_min_pixels VIDEO_MIN_PIXELS] [--video_fps VIDEO_FPS]\n",
      "              [--video_maxlen VIDEO_MAXLEN]\n",
      "              [--use_audio_in_video [USE_AUDIO_IN_VIDEO]]\n",
      "              [--audio_sampling_rate AUDIO_SAMPLING_RATE]\n",
      "              [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE]\n",
      "              [--export_device {cpu,auto}]\n",
      "              [--export_quantization_bit EXPORT_QUANTIZATION_BIT]\n",
      "              [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]\n",
      "              [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]\n",
      "              [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]\n",
      "              [--export_legacy_format [EXPORT_LEGACY_FORMAT]]\n",
      "              [--export_hub_model_id EXPORT_HUB_MODEL_ID]\n",
      "              [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL]\n",
      "              [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]\n",
      "              [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]\n",
      "              [--vllm_config VLLM_CONFIG] [--sglang_maxlen SGLANG_MAXLEN]\n",
      "              [--sglang_mem_fraction SGLANG_MEM_FRACTION]\n",
      "              [--sglang_tp_size SGLANG_TP_SIZE]\n",
      "              [--sglang_config SGLANG_CONFIG]\n",
      "              [--sglang_lora_backend {triton,flashinfer}]\n",
      "              [--template TEMPLATE] [--dataset DATASET]\n",
      "              [--eval_dataset EVAL_DATASET] [--dataset_dir DATASET_DIR]\n",
      "              [--media_dir MEDIA_DIR] [--cutoff_len CUTOFF_LEN]\n",
      "              [--train_on_prompt [TRAIN_ON_PROMPT]]\n",
      "              [--mask_history [MASK_HISTORY]] [--streaming [STREAMING]]\n",
      "              [--buffer_size BUFFER_SIZE]\n",
      "              [--mix_strategy {concat,interleave_under,interleave_over}]\n",
      "              [--interleave_probs INTERLEAVE_PROBS]\n",
      "              [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "              [--preprocessing_batch_size PREPROCESSING_BATCH_SIZE]\n",
      "              [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "              [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]\n",
      "              [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n",
      "              [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]\n",
      "              [--eval_on_each_dataset [EVAL_ON_EACH_DATASET]]\n",
      "              [--packing PACKING] [--neat_packing [NEAT_PACKING]]\n",
      "              [--tool_format TOOL_FORMAT] [--default_system DEFAULT_SYSTEM]\n",
      "              [--enable_thinking [ENABLE_THINKING]] [--no_enable_thinking]\n",
      "              [--tokenized_path TOKENIZED_PATH]\n",
      "              [--data_shared_file_system [DATA_SHARED_FILE_SYSTEM]]\n",
      "              [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]\n",
      "              [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]\n",
      "              [--freeze_extra_modules FREEZE_EXTRA_MODULES]\n",
      "              [--additional_target ADDITIONAL_TARGET]\n",
      "              [--module_dropout MODULE_DROPOUT] [--oft_rank OFT_RANK]\n",
      "              [--oft_block_size OFT_BLOCK_SIZE] [--oft_target OFT_TARGET]\n",
      "              [--create_new_adapter [CREATE_NEW_ADAPTER]]\n",
      "              [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n",
      "              [--lora_rank LORA_RANK] [--lora_target LORA_TARGET]\n",
      "              [--loraplus_lr_ratio LORAPLUS_LR_RATIO]\n",
      "              [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]\n",
      "              [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]\n",
      "              [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]\n",
      "              [--pissa_convert [PISSA_CONVERT]] [--pref_beta PREF_BETA]\n",
      "              [--pref_ftx PREF_FTX] [--pref_bco_weight PREF_BCO_WEIGHT]\n",
      "              [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]\n",
      "              [--dpo_label_smoothing DPO_LABEL_SMOOTHING]\n",
      "              [--kto_chosen_weight KTO_CHOSEN_WEIGHT]\n",
      "              [--kto_rejected_weight KTO_REJECTED_WEIGHT]\n",
      "              [--simpo_gamma SIMPO_GAMMA] [--ppo_buffer_size PPO_BUFFER_SIZE]\n",
      "              [--ppo_epochs PPO_EPOCHS] [--ppo_score_norm [PPO_SCORE_NORM]]\n",
      "              [--ppo_target PPO_TARGET]\n",
      "              [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]\n",
      "              [--ref_model REF_MODEL]\n",
      "              [--ref_model_adapters REF_MODEL_ADAPTERS]\n",
      "              [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]\n",
      "              [--reward_model REWARD_MODEL]\n",
      "              [--reward_model_adapters REWARD_MODEL_ADAPTERS]\n",
      "              [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]\n",
      "              [--reward_model_type {lora,full,api}] [--ld_alpha LD_ALPHA]\n",
      "              [--use_galore [USE_GALORE]] [--galore_target GALORE_TARGET]\n",
      "              [--galore_rank GALORE_RANK]\n",
      "              [--galore_update_interval GALORE_UPDATE_INTERVAL]\n",
      "              [--galore_scale GALORE_SCALE]\n",
      "              [--galore_proj_type {std,reverse_std,right,left,full}]\n",
      "              [--galore_layerwise [GALORE_LAYERWISE]]\n",
      "              [--use_apollo [USE_APOLLO]] [--apollo_target APOLLO_TARGET]\n",
      "              [--apollo_rank APOLLO_RANK]\n",
      "              [--apollo_update_interval APOLLO_UPDATE_INTERVAL]\n",
      "              [--apollo_scale APOLLO_SCALE] [--apollo_proj {svd,random}]\n",
      "              [--apollo_proj_type {std,right,left}]\n",
      "              [--apollo_scale_type {channel,tensor}]\n",
      "              [--apollo_layerwise [APOLLO_LAYERWISE]]\n",
      "              [--apollo_scale_front [APOLLO_SCALE_FRONT]]\n",
      "              [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]\n",
      "              [--badam_start_block BADAM_START_BLOCK]\n",
      "              [--badam_switch_mode {ascending,descending,random,fixed}]\n",
      "              [--badam_switch_interval BADAM_SWITCH_INTERVAL]\n",
      "              [--badam_update_ratio BADAM_UPDATE_RATIO]\n",
      "              [--badam_mask_mode {adjacent,scatter}]\n",
      "              [--badam_verbose BADAM_VERBOSE] [--use_swanlab [USE_SWANLAB]]\n",
      "              [--swanlab_project SWANLAB_PROJECT]\n",
      "              [--swanlab_workspace SWANLAB_WORKSPACE]\n",
      "              [--swanlab_run_name SWANLAB_RUN_NAME]\n",
      "              [--swanlab_mode {cloud,local}]\n",
      "              [--swanlab_api_key SWANLAB_API_KEY]\n",
      "              [--swanlab_logdir SWANLAB_LOGDIR]\n",
      "              [--swanlab_lark_webhook_url SWANLAB_LARK_WEBHOOK_URL]\n",
      "              [--swanlab_lark_secret SWANLAB_LARK_SECRET]\n",
      "              [--pure_bf16 [PURE_BF16]] [--stage {pt,sft,rm,ppo,dpo,kto}]\n",
      "              [--finetuning_type {lora,freeze,full}]\n",
      "              [--use_llama_pro [USE_LLAMA_PRO]]\n",
      "              [--use_adam_mini [USE_ADAM_MINI]] [--use_muon [USE_MUON]]\n",
      "              [--use_dft_loss [USE_DFT_LOSS]]\n",
      "              [--freeze_vision_tower [FREEZE_VISION_TOWER]]\n",
      "              [--no_freeze_vision_tower]\n",
      "              [--freeze_multi_modal_projector [FREEZE_MULTI_MODAL_PROJECTOR]]\n",
      "              [--no_freeze_multi_modal_projector]\n",
      "              [--freeze_language_model [FREEZE_LANGUAGE_MODEL]]\n",
      "              [--compute_accuracy [COMPUTE_ACCURACY]]\n",
      "              [--disable_shuffling [DISABLE_SHUFFLING]]\n",
      "              [--early_stopping_steps EARLY_STOPPING_STEPS]\n",
      "              [--plot_loss [PLOT_LOSS]]\n",
      "              [--include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]]\n",
      "              [--do_sample [DO_SAMPLE]] [--no_do_sample]\n",
      "              [--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K]\n",
      "              [--num_beams NUM_BEAMS] [--max_length MAX_LENGTH]\n",
      "              [--max_new_tokens MAX_NEW_TOKENS]\n",
      "              [--repetition_penalty REPETITION_PENALTY]\n",
      "              [--length_penalty LENGTH_PENALTY]\n",
      "              [--skip_special_tokens [SKIP_SPECIAL_TOKENS]]\n",
      "              [--no_skip_special_tokens]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH, --model-name-or-path MODEL_NAME_OR_PATH\n",
      "                        Path to the model weight or identifier from\n",
      "                        huggingface.co/models or modelscope.cn/models.\n",
      "                        (default: None)\n",
      "  --adapter_name_or_path ADAPTER_NAME_OR_PATH, --adapter-name-or-path ADAPTER_NAME_OR_PATH\n",
      "                        Path to the adapter weight or identifier from\n",
      "                        huggingface.co/models. Use commas to separate multiple\n",
      "                        adapters. (default: None)\n",
      "  --adapter_folder ADAPTER_FOLDER, --adapter-folder ADAPTER_FOLDER\n",
      "                        The folder containing the adapter weights to load.\n",
      "                        (default: None)\n",
      "  --cache_dir CACHE_DIR, --cache-dir CACHE_DIR\n",
      "                        Where to store the pre-trained models downloaded from\n",
      "                        huggingface.co or modelscope.cn. (default: None)\n",
      "  --use_fast_tokenizer [USE_FAST_TOKENIZER], --use-fast-tokenizer [USE_FAST_TOKENIZER]\n",
      "                        Whether or not to use one of the fast tokenizer\n",
      "                        (backed by the tokenizers library). (default: True)\n",
      "  --no_use_fast_tokenizer, --no-use-fast-tokenizer\n",
      "                        Whether or not to use one of the fast tokenizer\n",
      "                        (backed by the tokenizers library). (default: False)\n",
      "  --resize_vocab [RESIZE_VOCAB], --resize-vocab [RESIZE_VOCAB]\n",
      "                        Whether or not to resize the tokenizer vocab and the\n",
      "                        embedding layers. (default: False)\n",
      "  --split_special_tokens [SPLIT_SPECIAL_TOKENS], --split-special-tokens [SPLIT_SPECIAL_TOKENS]\n",
      "                        Whether or not the special tokens should be split\n",
      "                        during the tokenization process. (default: False)\n",
      "  --add_tokens ADD_TOKENS, --add-tokens ADD_TOKENS\n",
      "                        Non-special tokens to be added into the tokenizer. Use\n",
      "                        commas to separate multiple tokens. (default: None)\n",
      "  --add_special_tokens ADD_SPECIAL_TOKENS, --add-special-tokens ADD_SPECIAL_TOKENS\n",
      "                        Special tokens to be added into the tokenizer. Use\n",
      "                        commas to separate multiple tokens. (default: None)\n",
      "  --new_special_tokens_config NEW_SPECIAL_TOKENS_CONFIG, --new-special-tokens-config NEW_SPECIAL_TOKENS_CONFIG\n",
      "                        Path to YAML config with special token descriptions\n",
      "                        for semantic initialization. If set, this takes\n",
      "                        precedence over add_special_tokens. YAML format:\n",
      "                        {'<token>': 'description text', ...} (default: None)\n",
      "  --init_special_tokens {noise_init,desc_init,desc_init_w_noise}, --init-special-tokens {noise_init,desc_init,desc_init_w_noise}\n",
      "                        Initialization method for new special tokens:\n",
      "                        'noise_init' (default, random noise around mean),\n",
      "                        'desc_init' (semantic initialization from\n",
      "                        descriptions), 'desc_init_w_noise' (semantic + random\n",
      "                        noise). Note: 'desc_init' methods require\n",
      "                        new_special_tokens_config. (default: noise_init)\n",
      "  --model_revision MODEL_REVISION, --model-revision MODEL_REVISION\n",
      "                        The specific model version to use (can be a branch\n",
      "                        name, tag name or commit id). (default: main)\n",
      "  --low_cpu_mem_usage [LOW_CPU_MEM_USAGE], --low-cpu-mem-usage [LOW_CPU_MEM_USAGE]\n",
      "                        Whether or not to use memory-efficient model loading.\n",
      "                        (default: True)\n",
      "  --no_low_cpu_mem_usage, --no-low-cpu-mem-usage\n",
      "                        Whether or not to use memory-efficient model loading.\n",
      "                        (default: False)\n",
      "  --rope_scaling {linear,dynamic,yarn,llama3}, --rope-scaling {linear,dynamic,yarn,llama3}\n",
      "                        Which scaling strategy should be adopted for the RoPE\n",
      "                        embeddings. (default: None)\n",
      "  --flash_attn {auto,disabled,sdpa,fa2}, --flash-attn {auto,disabled,sdpa,fa2}\n",
      "                        Enable FlashAttention for faster training and\n",
      "                        inference. (default: AttentionFunction.AUTO)\n",
      "  --shift_attn [SHIFT_ATTN], --shift-attn [SHIFT_ATTN]\n",
      "                        Enable shift short attention (S^2-Attn) proposed by\n",
      "                        LongLoRA. (default: False)\n",
      "  --mixture_of_depths {convert,load}, --mixture-of-depths {convert,load}\n",
      "                        Convert the model to mixture-of-depths (MoD) or load\n",
      "                        the MoD model. (default: None)\n",
      "  --use_unsloth [USE_UNSLOTH], --use-unsloth [USE_UNSLOTH]\n",
      "                        Whether or not to use unsloth's optimization for the\n",
      "                        LoRA training. (default: False)\n",
      "  --use_unsloth_gc [USE_UNSLOTH_GC], --use-unsloth-gc [USE_UNSLOTH_GC]\n",
      "                        Whether or not to use unsloth's gradient checkpointing\n",
      "                        (no need to install unsloth). (default: False)\n",
      "  --enable_liger_kernel [ENABLE_LIGER_KERNEL], --enable-liger-kernel [ENABLE_LIGER_KERNEL]\n",
      "                        Whether or not to enable liger kernel for faster\n",
      "                        training. (default: False)\n",
      "  --moe_aux_loss_coef MOE_AUX_LOSS_COEF, --moe-aux-loss-coef MOE_AUX_LOSS_COEF\n",
      "                        Coefficient of the auxiliary router loss in mixture-\n",
      "                        of-experts model. (default: None)\n",
      "  --disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING], --disable-gradient-checkpointing [DISABLE_GRADIENT_CHECKPOINTING]\n",
      "                        Whether or not to disable gradient checkpointing.\n",
      "                        (default: False)\n",
      "  --use_reentrant_gc [USE_REENTRANT_GC], --use-reentrant-gc [USE_REENTRANT_GC]\n",
      "                        Whether or not to use reentrant gradient\n",
      "                        checkpointing. (default: True)\n",
      "  --no_use_reentrant_gc, --no-use-reentrant-gc\n",
      "                        Whether or not to use reentrant gradient\n",
      "                        checkpointing. (default: False)\n",
      "  --upcast_layernorm [UPCAST_LAYERNORM], --upcast-layernorm [UPCAST_LAYERNORM]\n",
      "                        Whether or not to upcast the layernorm weights in\n",
      "                        fp32. (default: False)\n",
      "  --upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT], --upcast-lmhead-output [UPCAST_LMHEAD_OUTPUT]\n",
      "                        Whether or not to upcast the output of lm_head in\n",
      "                        fp32. (default: False)\n",
      "  --train_from_scratch [TRAIN_FROM_SCRATCH], --train-from-scratch [TRAIN_FROM_SCRATCH]\n",
      "                        Whether or not to randomly initialize the model\n",
      "                        weights. (default: False)\n",
      "  --infer_backend {huggingface,vllm,sglang}, --infer-backend {huggingface,vllm,sglang}\n",
      "                        Backend engine used at inference. (default:\n",
      "                        EngineName.HF)\n",
      "  --offload_folder OFFLOAD_FOLDER, --offload-folder OFFLOAD_FOLDER\n",
      "                        Path to offload model weights. (default: offload)\n",
      "  --use_cache [USE_CACHE], --use-cache [USE_CACHE]\n",
      "                        Whether or not to use KV cache in generation.\n",
      "                        (default: True)\n",
      "  --no_use_cache, --no-use-cache\n",
      "                        Whether or not to use KV cache in generation.\n",
      "                        (default: False)\n",
      "  --infer_dtype {auto,float16,bfloat16,float32}, --infer-dtype {auto,float16,bfloat16,float32}\n",
      "                        Data type for model weights and activations at\n",
      "                        inference. (default: auto)\n",
      "  --hf_hub_token HF_HUB_TOKEN, --hf-hub-token HF_HUB_TOKEN\n",
      "                        Auth token to log in with Hugging Face Hub. (default:\n",
      "                        None)\n",
      "  --ms_hub_token MS_HUB_TOKEN, --ms-hub-token MS_HUB_TOKEN\n",
      "                        Auth token to log in with ModelScope Hub. (default:\n",
      "                        None)\n",
      "  --om_hub_token OM_HUB_TOKEN, --om-hub-token OM_HUB_TOKEN\n",
      "                        Auth token to log in with Modelers Hub. (default:\n",
      "                        None)\n",
      "  --print_param_status [PRINT_PARAM_STATUS], --print-param-status [PRINT_PARAM_STATUS]\n",
      "                        For debugging purposes, print the status of the\n",
      "                        parameters in the model. (default: False)\n",
      "  --trust_remote_code [TRUST_REMOTE_CODE], --trust-remote-code [TRUST_REMOTE_CODE]\n",
      "                        Whether to trust the execution of code from\n",
      "                        datasets/models defined on the Hub or not. (default:\n",
      "                        False)\n",
      "  --quantization_method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}, --quantization-method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}\n",
      "                        Quantization method to use for on-the-fly\n",
      "                        quantization. (default: QuantizationMethod.BNB)\n",
      "  --quantization_bit QUANTIZATION_BIT, --quantization-bit QUANTIZATION_BIT\n",
      "                        The number of bits to quantize the model using on-the-\n",
      "                        fly quantization. (default: None)\n",
      "  --quantization_type {fp4,nf4}, --quantization-type {fp4,nf4}\n",
      "                        Quantization data type to use in bitsandbytes int4\n",
      "                        training. (default: nf4)\n",
      "  --double_quantization [DOUBLE_QUANTIZATION], --double-quantization [DOUBLE_QUANTIZATION]\n",
      "                        Whether or not to use double quantization in\n",
      "                        bitsandbytes int4 training. (default: True)\n",
      "  --no_double_quantization, --no-double-quantization\n",
      "                        Whether or not to use double quantization in\n",
      "                        bitsandbytes int4 training. (default: False)\n",
      "  --quantization_device_map {auto}, --quantization-device-map {auto}\n",
      "                        Device map used to infer the 4-bit quantized model,\n",
      "                        needs bitsandbytes>=0.43.0. (default: None)\n",
      "  --fp8 [FP8]           Enable FP8 mixed precision training via HuggingFace\n",
      "                        Accelerate. Requires PyTorch 2.7+ and Hopper\n",
      "                        architecture GPUs. (default: False)\n",
      "  --fp8_backend FP8_BACKEND, --fp8-backend FP8_BACKEND\n",
      "                        FP8 backend to use ('auto', 'torchao', 'te', 'msamp').\n",
      "                        'auto' selects best available backend. (default: auto)\n",
      "  --fp8_enable_fsdp_float8_all_gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER], --fp8-enable-fsdp-float8-all-gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER]\n",
      "                        Enable FP8 optimizations for FSDP2 all-gather\n",
      "                        operations. (default: False)\n",
      "  --image_max_pixels IMAGE_MAX_PIXELS, --image-max-pixels IMAGE_MAX_PIXELS\n",
      "                        The maximum number of pixels of image inputs.\n",
      "                        (default: 589824)\n",
      "  --image_min_pixels IMAGE_MIN_PIXELS, --image-min-pixels IMAGE_MIN_PIXELS\n",
      "                        The minimum number of pixels of image inputs.\n",
      "                        (default: 1024)\n",
      "  --image_do_pan_and_scan [IMAGE_DO_PAN_AND_SCAN], --image-do-pan-and-scan [IMAGE_DO_PAN_AND_SCAN]\n",
      "                        Use pan and scan to process image for gemma3.\n",
      "                        (default: False)\n",
      "  --crop_to_patches [CROP_TO_PATCHES], --crop-to-patches [CROP_TO_PATCHES]\n",
      "                        Whether to crop the image to patches for internvl.\n",
      "                        (default: False)\n",
      "  --video_max_pixels VIDEO_MAX_PIXELS, --video-max-pixels VIDEO_MAX_PIXELS\n",
      "                        The maximum number of pixels of video inputs.\n",
      "                        (default: 65536)\n",
      "  --video_min_pixels VIDEO_MIN_PIXELS, --video-min-pixels VIDEO_MIN_PIXELS\n",
      "                        The minimum number of pixels of video inputs.\n",
      "                        (default: 256)\n",
      "  --video_fps VIDEO_FPS, --video-fps VIDEO_FPS\n",
      "                        The frames to sample per second for video inputs.\n",
      "                        (default: 2.0)\n",
      "  --video_maxlen VIDEO_MAXLEN, --video-maxlen VIDEO_MAXLEN\n",
      "                        The maximum number of sampled frames for video inputs.\n",
      "                        (default: 128)\n",
      "  --use_audio_in_video [USE_AUDIO_IN_VIDEO], --use-audio-in-video [USE_AUDIO_IN_VIDEO]\n",
      "                        Whether or not to use audio in video inputs. (default:\n",
      "                        False)\n",
      "  --audio_sampling_rate AUDIO_SAMPLING_RATE, --audio-sampling-rate AUDIO_SAMPLING_RATE\n",
      "                        The sampling rate of audio inputs. (default: 16000)\n",
      "  --export_dir EXPORT_DIR, --export-dir EXPORT_DIR\n",
      "                        Path to the directory to save the exported model.\n",
      "                        (default: None)\n",
      "  --export_size EXPORT_SIZE, --export-size EXPORT_SIZE\n",
      "                        The file shard size (in GB) of the exported model.\n",
      "                        (default: 5)\n",
      "  --export_device {cpu,auto}, --export-device {cpu,auto}\n",
      "                        The device used in model export, use `auto` to\n",
      "                        accelerate exporting. (default: cpu)\n",
      "  --export_quantization_bit EXPORT_QUANTIZATION_BIT, --export-quantization-bit EXPORT_QUANTIZATION_BIT\n",
      "                        The number of bits to quantize the exported model.\n",
      "                        (default: None)\n",
      "  --export_quantization_dataset EXPORT_QUANTIZATION_DATASET, --export-quantization-dataset EXPORT_QUANTIZATION_DATASET\n",
      "                        Path to the dataset or dataset name to use in\n",
      "                        quantizing the exported model. (default: None)\n",
      "  --export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES, --export-quantization-nsamples EXPORT_QUANTIZATION_NSAMPLES\n",
      "                        The number of samples used for quantization. (default:\n",
      "                        128)\n",
      "  --export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN, --export-quantization-maxlen EXPORT_QUANTIZATION_MAXLEN\n",
      "                        The maximum length of the model inputs used for\n",
      "                        quantization. (default: 1024)\n",
      "  --export_legacy_format [EXPORT_LEGACY_FORMAT], --export-legacy-format [EXPORT_LEGACY_FORMAT]\n",
      "                        Whether or not to save the `.bin` files instead of\n",
      "                        `.safetensors`. (default: False)\n",
      "  --export_hub_model_id EXPORT_HUB_MODEL_ID, --export-hub-model-id EXPORT_HUB_MODEL_ID\n",
      "                        The name of the repository if push the model to the\n",
      "                        Hugging Face hub. (default: None)\n",
      "  --vllm_maxlen VLLM_MAXLEN, --vllm-maxlen VLLM_MAXLEN\n",
      "                        Maximum sequence (prompt + response) length of the\n",
      "                        vLLM engine. (default: 4096)\n",
      "  --vllm_gpu_util VLLM_GPU_UTIL, --vllm-gpu-util VLLM_GPU_UTIL\n",
      "                        The fraction of GPU memory in (0,1) to be used for the\n",
      "                        vLLM engine. (default: 0.7)\n",
      "  --vllm_enforce_eager [VLLM_ENFORCE_EAGER], --vllm-enforce-eager [VLLM_ENFORCE_EAGER]\n",
      "                        Whether or not to disable CUDA graph in the vLLM\n",
      "                        engine. (default: False)\n",
      "  --vllm_max_lora_rank VLLM_MAX_LORA_RANK, --vllm-max-lora-rank VLLM_MAX_LORA_RANK\n",
      "                        Maximum rank of all LoRAs in the vLLM engine.\n",
      "                        (default: 32)\n",
      "  --vllm_config VLLM_CONFIG, --vllm-config VLLM_CONFIG\n",
      "                        Config to initialize the vllm engine. Please use JSON\n",
      "                        strings. (default: None)\n",
      "  --sglang_maxlen SGLANG_MAXLEN, --sglang-maxlen SGLANG_MAXLEN\n",
      "                        Maximum sequence (prompt + response) length of the\n",
      "                        SGLang engine. (default: 4096)\n",
      "  --sglang_mem_fraction SGLANG_MEM_FRACTION, --sglang-mem-fraction SGLANG_MEM_FRACTION\n",
      "                        The memory fraction (0-1) to be used for the SGLang\n",
      "                        engine. (default: 0.7)\n",
      "  --sglang_tp_size SGLANG_TP_SIZE, --sglang-tp-size SGLANG_TP_SIZE\n",
      "                        Tensor parallel size for the SGLang engine. (default:\n",
      "                        -1)\n",
      "  --sglang_config SGLANG_CONFIG, --sglang-config SGLANG_CONFIG\n",
      "                        Config to initialize the SGLang engine. Please use\n",
      "                        JSON strings. (default: None)\n",
      "  --sglang_lora_backend {triton,flashinfer}, --sglang-lora-backend {triton,flashinfer}\n",
      "                        The backend of running GEMM kernels for Lora modules.\n",
      "                        Recommend using the Triton LoRA backend for better\n",
      "                        performance and stability. (default: triton)\n",
      "  --template TEMPLATE   Which template to use for constructing prompts in\n",
      "                        training and inference. (default: None)\n",
      "  --dataset DATASET     The name of dataset(s) to use for training. Use commas\n",
      "                        to separate multiple datasets. (default: None)\n",
      "  --eval_dataset EVAL_DATASET, --eval-dataset EVAL_DATASET\n",
      "                        The name of dataset(s) to use for evaluation. Use\n",
      "                        commas to separate multiple datasets. (default: None)\n",
      "  --dataset_dir DATASET_DIR, --dataset-dir DATASET_DIR\n",
      "                        Path to the folder containing the datasets. (default:\n",
      "                        data)\n",
      "  --media_dir MEDIA_DIR, --media-dir MEDIA_DIR\n",
      "                        Path to the folder containing the images, videos or\n",
      "                        audios. Defaults to `dataset_dir`. (default: None)\n",
      "  --cutoff_len CUTOFF_LEN, --cutoff-len CUTOFF_LEN\n",
      "                        The cutoff length of the tokenized inputs in the\n",
      "                        dataset. (default: 2048)\n",
      "  --train_on_prompt [TRAIN_ON_PROMPT], --train-on-prompt [TRAIN_ON_PROMPT]\n",
      "                        Whether or not to disable the mask on the prompt.\n",
      "                        (default: False)\n",
      "  --mask_history [MASK_HISTORY], --mask-history [MASK_HISTORY]\n",
      "                        Whether or not to mask the history and train on the\n",
      "                        last turn only. (default: False)\n",
      "  --streaming [STREAMING]\n",
      "                        Enable dataset streaming. (default: False)\n",
      "  --buffer_size BUFFER_SIZE, --buffer-size BUFFER_SIZE\n",
      "                        Size of the buffer to randomly sample examples from in\n",
      "                        dataset streaming. (default: 16384)\n",
      "  --mix_strategy {concat,interleave_under,interleave_over}, --mix-strategy {concat,interleave_under,interleave_over}\n",
      "                        Strategy to use in dataset mixing (concat/interleave)\n",
      "                        (undersampling/oversampling). (default: concat)\n",
      "  --interleave_probs INTERLEAVE_PROBS, --interleave-probs INTERLEAVE_PROBS\n",
      "                        Probabilities to sample data from datasets. Use commas\n",
      "                        to separate multiple datasets. (default: None)\n",
      "  --overwrite_cache [OVERWRITE_CACHE], --overwrite-cache [OVERWRITE_CACHE]\n",
      "                        Overwrite the cached training and evaluation sets.\n",
      "                        (default: False)\n",
      "  --preprocessing_batch_size PREPROCESSING_BATCH_SIZE, --preprocessing-batch-size PREPROCESSING_BATCH_SIZE\n",
      "                        The number of examples in one group in pre-processing.\n",
      "                        (default: 1000)\n",
      "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS, --preprocessing-num-workers PREPROCESSING_NUM_WORKERS\n",
      "                        The number of processes to use for the pre-processing.\n",
      "                        (default: None)\n",
      "  --max_samples MAX_SAMPLES, --max-samples MAX_SAMPLES\n",
      "                        For debugging purposes, truncate the number of\n",
      "                        examples for each dataset. (default: None)\n",
      "  --eval_num_beams EVAL_NUM_BEAMS, --eval-num-beams EVAL_NUM_BEAMS\n",
      "                        Number of beams to use for evaluation. This argument\n",
      "                        will be passed to `model.generate` (default: None)\n",
      "  --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS], --ignore-pad-token-for-loss [IGNORE_PAD_TOKEN_FOR_LOSS]\n",
      "                        Whether or not to ignore the tokens corresponding to\n",
      "                        the pad label in loss computation. (default: True)\n",
      "  --no_ignore_pad_token_for_loss, --no-ignore-pad-token-for-loss\n",
      "                        Whether or not to ignore the tokens corresponding to\n",
      "                        the pad label in loss computation. (default: False)\n",
      "  --val_size VAL_SIZE, --val-size VAL_SIZE\n",
      "                        Size of the validation set, should be an integer or a\n",
      "                        float in range `[0,1)`. (default: 0.0)\n",
      "  --eval_on_each_dataset [EVAL_ON_EACH_DATASET], --eval-on-each-dataset [EVAL_ON_EACH_DATASET]\n",
      "                        Whether or not to evaluate on each dataset separately.\n",
      "                        (default: False)\n",
      "  --packing PACKING     Enable sequences packing in training. Will\n",
      "                        automatically enable in pre-training. (default: None)\n",
      "  --neat_packing [NEAT_PACKING], --neat-packing [NEAT_PACKING]\n",
      "                        Enable sequence packing without cross-attention.\n",
      "                        (default: False)\n",
      "  --tool_format TOOL_FORMAT, --tool-format TOOL_FORMAT\n",
      "                        Tool format to use for constructing function calling\n",
      "                        examples. (default: None)\n",
      "  --default_system DEFAULT_SYSTEM, --default-system DEFAULT_SYSTEM\n",
      "                        Override the default system message in the template.\n",
      "                        (default: None)\n",
      "  --enable_thinking [ENABLE_THINKING], --enable-thinking [ENABLE_THINKING]\n",
      "                        Whether or not to enable thinking mode for reasoning\n",
      "                        models. (default: True)\n",
      "  --no_enable_thinking, --no-enable-thinking\n",
      "                        Whether or not to enable thinking mode for reasoning\n",
      "                        models. (default: False)\n",
      "  --tokenized_path TOKENIZED_PATH, --tokenized-path TOKENIZED_PATH\n",
      "                        Path to save or load the tokenized datasets. If\n",
      "                        tokenized_path not exists, it will save the tokenized\n",
      "                        datasets. If tokenized_path exists, it will load the\n",
      "                        tokenized datasets. (default: None)\n",
      "  --data_shared_file_system [DATA_SHARED_FILE_SYSTEM], --data-shared-file-system [DATA_SHARED_FILE_SYSTEM]\n",
      "                        Whether or not to use a shared file system for the\n",
      "                        datasets. (default: False)\n",
      "  --freeze_trainable_layers FREEZE_TRAINABLE_LAYERS, --freeze-trainable-layers FREEZE_TRAINABLE_LAYERS\n",
      "                        The number of trainable layers for freeze (partial-\n",
      "                        parameter) fine-tuning. Positive numbers mean the last\n",
      "                        n layers are set as trainable, negative numbers mean\n",
      "                        the first n layers are set as trainable. (default: 2)\n",
      "  --freeze_trainable_modules FREEZE_TRAINABLE_MODULES, --freeze-trainable-modules FREEZE_TRAINABLE_MODULES\n",
      "                        Name(s) of trainable modules for freeze (partial-\n",
      "                        parameter) fine-tuning. Use commas to separate\n",
      "                        multiple modules. Use `all` to specify all the\n",
      "                        available modules. (default: all)\n",
      "  --freeze_extra_modules FREEZE_EXTRA_MODULES, --freeze-extra-modules FREEZE_EXTRA_MODULES\n",
      "                        Name(s) of modules apart from hidden layers to be set\n",
      "                        as trainable for freeze (partial-parameter) fine-\n",
      "                        tuning. Use commas to separate multiple modules.\n",
      "                        (default: None)\n",
      "  --additional_target ADDITIONAL_TARGET, --additional-target ADDITIONAL_TARGET\n",
      "                        Name(s) of modules apart from LoRA layers to be set as\n",
      "                        trainable and saved in the final checkpoint. Use\n",
      "                        commas to separate multiple modules. (default: None)\n",
      "  --module_dropout MODULE_DROPOUT, --module-dropout MODULE_DROPOUT\n",
      "                        Dropout rate for the OFT fine-tuning. (default: 0.0)\n",
      "  --oft_rank OFT_RANK, --oft-rank OFT_RANK\n",
      "                        The intrinsic dimension for OFT fine-tuning. (default:\n",
      "                        0)\n",
      "  --oft_block_size OFT_BLOCK_SIZE, --oft-block-size OFT_BLOCK_SIZE\n",
      "                        The intrinsic dimension for OFT fine-tuning. (default:\n",
      "                        32)\n",
      "  --oft_target OFT_TARGET, --oft-target OFT_TARGET\n",
      "                        Name(s) of target modules to apply OFT. Use commas to\n",
      "                        separate multiple modules. Use `all` to specify all\n",
      "                        the linear modules. (default: all)\n",
      "  --create_new_adapter [CREATE_NEW_ADAPTER], --create-new-adapter [CREATE_NEW_ADAPTER]\n",
      "                        Whether or not to create a new adapter with randomly\n",
      "                        initialized weight. (default: False)\n",
      "  --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n",
      "                        The scale factor for LoRA fine-tuning (default:\n",
      "                        lora_rank * 2). (default: None)\n",
      "  --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n",
      "                        Dropout rate for the LoRA fine-tuning. (default: 0.0)\n",
      "  --lora_rank LORA_RANK, --lora-rank LORA_RANK\n",
      "                        The intrinsic dimension for LoRA fine-tuning.\n",
      "                        (default: 8)\n",
      "  --lora_target LORA_TARGET, --lora-target LORA_TARGET\n",
      "                        Name(s) of target modules to apply LoRA. Use commas to\n",
      "                        separate multiple modules. Use `all` to specify all\n",
      "                        the linear modules. (default: all)\n",
      "  --loraplus_lr_ratio LORAPLUS_LR_RATIO, --loraplus-lr-ratio LORAPLUS_LR_RATIO\n",
      "                        LoRA plus learning rate ratio (lr_B / lr_A). (default:\n",
      "                        None)\n",
      "  --loraplus_lr_embedding LORAPLUS_LR_EMBEDDING, --loraplus-lr-embedding LORAPLUS_LR_EMBEDDING\n",
      "                        LoRA plus learning rate for lora embedding layers.\n",
      "                        (default: 1e-06)\n",
      "  --use_rslora [USE_RSLORA], --use-rslora [USE_RSLORA]\n",
      "                        Whether or not to use the rank stabilization scaling\n",
      "                        factor for LoRA layer. (default: False)\n",
      "  --use_dora [USE_DORA], --use-dora [USE_DORA]\n",
      "                        Whether or not to use the weight-decomposed lora\n",
      "                        method (DoRA). (default: False)\n",
      "  --pissa_init [PISSA_INIT], --pissa-init [PISSA_INIT]\n",
      "                        Whether or not to initialize a PiSSA adapter.\n",
      "                        (default: False)\n",
      "  --pissa_iter PISSA_ITER, --pissa-iter PISSA_ITER\n",
      "                        The number of iteration steps performed by FSVD in\n",
      "                        PiSSA. Use -1 to disable it. (default: 16)\n",
      "  --pissa_convert [PISSA_CONVERT], --pissa-convert [PISSA_CONVERT]\n",
      "                        Whether or not to convert the PiSSA adapter to a\n",
      "                        normal LoRA adapter. (default: False)\n",
      "  --pref_beta PREF_BETA, --pref-beta PREF_BETA\n",
      "                        The beta parameter in the preference loss. (default:\n",
      "                        0.1)\n",
      "  --pref_ftx PREF_FTX, --pref-ftx PREF_FTX\n",
      "                        The supervised fine-tuning loss coefficient in DPO\n",
      "                        training. (default: 0.0)\n",
      "  --pref_bco_weight PREF_BCO_WEIGHT, --pref-bco-weight PREF_BCO_WEIGHT\n",
      "                        The Binary Classifier Optimization coefficient in DPO\n",
      "                        training. (default: 0.0)\n",
      "  --pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}, --pref-loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}\n",
      "                        The type of DPO loss to use. (default: sigmoid)\n",
      "  --dpo_label_smoothing DPO_LABEL_SMOOTHING, --dpo-label-smoothing DPO_LABEL_SMOOTHING\n",
      "                        The robust DPO label smoothing parameter in cDPO that\n",
      "                        should be between 0 and 0.5. (default: 0.0)\n",
      "  --kto_chosen_weight KTO_CHOSEN_WEIGHT, --kto-chosen-weight KTO_CHOSEN_WEIGHT\n",
      "                        The weight factor of the desirable losses in KTO\n",
      "                        training. (default: 1.0)\n",
      "  --kto_rejected_weight KTO_REJECTED_WEIGHT, --kto-rejected-weight KTO_REJECTED_WEIGHT\n",
      "                        The weight factor of the undesirable losses in KTO\n",
      "                        training. (default: 1.0)\n",
      "  --simpo_gamma SIMPO_GAMMA, --simpo-gamma SIMPO_GAMMA\n",
      "                        The target reward margin term in SimPO loss. (default:\n",
      "                        0.5)\n",
      "  --ppo_buffer_size PPO_BUFFER_SIZE, --ppo-buffer-size PPO_BUFFER_SIZE\n",
      "                        The number of mini-batches to make experience buffer\n",
      "                        in a PPO optimization step. (default: 1)\n",
      "  --ppo_epochs PPO_EPOCHS, --ppo-epochs PPO_EPOCHS\n",
      "                        The number of epochs to perform in a PPO optimization\n",
      "                        step. (default: 4)\n",
      "  --ppo_score_norm [PPO_SCORE_NORM], --ppo-score-norm [PPO_SCORE_NORM]\n",
      "                        Use score normalization in PPO training. (default:\n",
      "                        False)\n",
      "  --ppo_target PPO_TARGET, --ppo-target PPO_TARGET\n",
      "                        Target KL value for adaptive KL control in PPO\n",
      "                        training. (default: 6.0)\n",
      "  --ppo_whiten_rewards [PPO_WHITEN_REWARDS], --ppo-whiten-rewards [PPO_WHITEN_REWARDS]\n",
      "                        Whiten the rewards before compute advantages in PPO\n",
      "                        training. (default: False)\n",
      "  --ref_model REF_MODEL, --ref-model REF_MODEL\n",
      "                        Path to the reference model used for the PPO or DPO\n",
      "                        training. (default: None)\n",
      "  --ref_model_adapters REF_MODEL_ADAPTERS, --ref-model-adapters REF_MODEL_ADAPTERS\n",
      "                        Path to the adapters of the reference model. (default:\n",
      "                        None)\n",
      "  --ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT, --ref-model-quantization-bit REF_MODEL_QUANTIZATION_BIT\n",
      "                        The number of bits to quantize the reference model.\n",
      "                        (default: None)\n",
      "  --reward_model REWARD_MODEL, --reward-model REWARD_MODEL\n",
      "                        Path to the reward model used for the PPO training.\n",
      "                        (default: None)\n",
      "  --reward_model_adapters REWARD_MODEL_ADAPTERS, --reward-model-adapters REWARD_MODEL_ADAPTERS\n",
      "                        Path to the adapters of the reward model. (default:\n",
      "                        None)\n",
      "  --reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT, --reward-model-quantization-bit REWARD_MODEL_QUANTIZATION_BIT\n",
      "                        The number of bits to quantize the reward model.\n",
      "                        (default: None)\n",
      "  --reward_model_type {lora,full,api}, --reward-model-type {lora,full,api}\n",
      "                        The type of the reward model in PPO training. Lora\n",
      "                        model only supports lora training. (default: lora)\n",
      "  --ld_alpha LD_ALPHA, --ld-alpha LD_ALPHA\n",
      "                        Alpha parameter from the LD-DPO paper, which controls\n",
      "                        the weighting of the verbose token log-probabilities\n",
      "                        in responses. (default: None)\n",
      "  --use_galore [USE_GALORE], --use-galore [USE_GALORE]\n",
      "                        Whether or not to use the gradient low-Rank projection\n",
      "                        (GaLore). (default: False)\n",
      "  --galore_target GALORE_TARGET, --galore-target GALORE_TARGET\n",
      "                        Name(s) of modules to apply GaLore. Use commas to\n",
      "                        separate multiple modules. Use `all` to specify all\n",
      "                        the linear modules. (default: all)\n",
      "  --galore_rank GALORE_RANK, --galore-rank GALORE_RANK\n",
      "                        The rank of GaLore gradients. (default: 16)\n",
      "  --galore_update_interval GALORE_UPDATE_INTERVAL, --galore-update-interval GALORE_UPDATE_INTERVAL\n",
      "                        Number of steps to update the GaLore projection.\n",
      "                        (default: 200)\n",
      "  --galore_scale GALORE_SCALE, --galore-scale GALORE_SCALE\n",
      "                        GaLore scaling coefficient. (default: 2.0)\n",
      "  --galore_proj_type {std,reverse_std,right,left,full}, --galore-proj-type {std,reverse_std,right,left,full}\n",
      "                        Type of GaLore projection. (default: std)\n",
      "  --galore_layerwise [GALORE_LAYERWISE], --galore-layerwise [GALORE_LAYERWISE]\n",
      "                        Whether or not to enable layer-wise update to further\n",
      "                        save memory. (default: False)\n",
      "  --use_apollo [USE_APOLLO], --use-apollo [USE_APOLLO]\n",
      "                        Whether or not to use the APOLLO optimizer. (default:\n",
      "                        False)\n",
      "  --apollo_target APOLLO_TARGET, --apollo-target APOLLO_TARGET\n",
      "                        Name(s) of modules to apply APOLLO. Use commas to\n",
      "                        separate multiple modules. Use `all` to specify all\n",
      "                        the linear modules. (default: all)\n",
      "  --apollo_rank APOLLO_RANK, --apollo-rank APOLLO_RANK\n",
      "                        The rank of APOLLO gradients. (default: 16)\n",
      "  --apollo_update_interval APOLLO_UPDATE_INTERVAL, --apollo-update-interval APOLLO_UPDATE_INTERVAL\n",
      "                        Number of steps to update the APOLLO projection.\n",
      "                        (default: 200)\n",
      "  --apollo_scale APOLLO_SCALE, --apollo-scale APOLLO_SCALE\n",
      "                        APOLLO scaling coefficient. (default: 32.0)\n",
      "  --apollo_proj {svd,random}, --apollo-proj {svd,random}\n",
      "                        Type of APOLLO low-rank projection algorithm (svd or\n",
      "                        random). (default: random)\n",
      "  --apollo_proj_type {std,right,left}, --apollo-proj-type {std,right,left}\n",
      "                        Type of APOLLO projection. (default: std)\n",
      "  --apollo_scale_type {channel,tensor}, --apollo-scale-type {channel,tensor}\n",
      "                        Type of APOLLO scaling (channel or tensor). (default:\n",
      "                        channel)\n",
      "  --apollo_layerwise [APOLLO_LAYERWISE], --apollo-layerwise [APOLLO_LAYERWISE]\n",
      "                        Whether or not to enable layer-wise update to further\n",
      "                        save memory. (default: False)\n",
      "  --apollo_scale_front [APOLLO_SCALE_FRONT], --apollo-scale-front [APOLLO_SCALE_FRONT]\n",
      "                        Whether or not to use the norm-growth limiter in front\n",
      "                        of gradient scaling. (default: False)\n",
      "  --use_badam [USE_BADAM], --use-badam [USE_BADAM]\n",
      "                        Whether or not to use the BAdam optimizer. (default:\n",
      "                        False)\n",
      "  --badam_mode {layer,ratio}, --badam-mode {layer,ratio}\n",
      "                        Whether to use layer-wise or ratio-wise BAdam\n",
      "                        optimizer. (default: layer)\n",
      "  --badam_start_block BADAM_START_BLOCK, --badam-start-block BADAM_START_BLOCK\n",
      "                        The starting block index for layer-wise BAdam.\n",
      "                        (default: None)\n",
      "  --badam_switch_mode {ascending,descending,random,fixed}, --badam-switch-mode {ascending,descending,random,fixed}\n",
      "                        the strategy of picking block to update for layer-wise\n",
      "                        BAdam. (default: ascending)\n",
      "  --badam_switch_interval BADAM_SWITCH_INTERVAL, --badam-switch-interval BADAM_SWITCH_INTERVAL\n",
      "                        Number of steps to update the block for layer-wise\n",
      "                        BAdam. Use -1 to disable the block update. (default:\n",
      "                        50)\n",
      "  --badam_update_ratio BADAM_UPDATE_RATIO, --badam-update-ratio BADAM_UPDATE_RATIO\n",
      "                        The ratio of the update for ratio-wise BAdam.\n",
      "                        (default: 0.05)\n",
      "  --badam_mask_mode {adjacent,scatter}, --badam-mask-mode {adjacent,scatter}\n",
      "                        The mode of the mask for BAdam optimizer. `adjacent`\n",
      "                        means that the trainable parameters are adjacent to\n",
      "                        each other, `scatter` means that trainable parameters\n",
      "                        are randomly choosed from the weight. (default:\n",
      "                        adjacent)\n",
      "  --badam_verbose BADAM_VERBOSE, --badam-verbose BADAM_VERBOSE\n",
      "                        The verbosity level of BAdam optimizer. 0 for no\n",
      "                        print, 1 for print the block prefix, 2 for print\n",
      "                        trainable parameters. (default: 0)\n",
      "  --use_swanlab [USE_SWANLAB], --use-swanlab [USE_SWANLAB]\n",
      "                        Whether or not to use the SwanLab (an experiment\n",
      "                        tracking and visualization tool). (default: False)\n",
      "  --swanlab_project SWANLAB_PROJECT, --swanlab-project SWANLAB_PROJECT\n",
      "                        The project name in SwanLab. (default: llamafactory)\n",
      "  --swanlab_workspace SWANLAB_WORKSPACE, --swanlab-workspace SWANLAB_WORKSPACE\n",
      "                        The workspace name in SwanLab. (default: None)\n",
      "  --swanlab_run_name SWANLAB_RUN_NAME, --swanlab-run-name SWANLAB_RUN_NAME\n",
      "                        The experiment name in SwanLab. (default: None)\n",
      "  --swanlab_mode {cloud,local}, --swanlab-mode {cloud,local}\n",
      "                        The mode of SwanLab. (default: cloud)\n",
      "  --swanlab_api_key SWANLAB_API_KEY, --swanlab-api-key SWANLAB_API_KEY\n",
      "                        The API key for SwanLab. (default: None)\n",
      "  --swanlab_logdir SWANLAB_LOGDIR, --swanlab-logdir SWANLAB_LOGDIR\n",
      "                        The log directory for SwanLab. (default: None)\n",
      "  --swanlab_lark_webhook_url SWANLAB_LARK_WEBHOOK_URL, --swanlab-lark-webhook-url SWANLAB_LARK_WEBHOOK_URL\n",
      "                        The Lark(飞书) webhook URL for SwanLab. (default: None)\n",
      "  --swanlab_lark_secret SWANLAB_LARK_SECRET, --swanlab-lark-secret SWANLAB_LARK_SECRET\n",
      "                        The Lark(飞书) secret for SwanLab. (default: None)\n",
      "  --pure_bf16 [PURE_BF16], --pure-bf16 [PURE_BF16]\n",
      "                        Whether or not to train model in purely bf16 precision\n",
      "                        (without AMP). (default: False)\n",
      "  --stage {pt,sft,rm,ppo,dpo,kto}\n",
      "                        Which stage will be performed in training. (default:\n",
      "                        sft)\n",
      "  --finetuning_type {lora,freeze,full}, --finetuning-type {lora,freeze,full}\n",
      "                        Which fine-tuning method to use. (default: lora)\n",
      "  --use_llama_pro [USE_LLAMA_PRO], --use-llama-pro [USE_LLAMA_PRO]\n",
      "                        Whether or not to make only the parameters in the\n",
      "                        expanded blocks trainable. (default: False)\n",
      "  --use_adam_mini [USE_ADAM_MINI], --use-adam-mini [USE_ADAM_MINI]\n",
      "                        Whether or not to use the Adam-mini optimizer.\n",
      "                        (default: False)\n",
      "  --use_muon [USE_MUON], --use-muon [USE_MUON]\n",
      "                        Whether or not to use the Muon optimizer. (default:\n",
      "                        False)\n",
      "  --use_dft_loss [USE_DFT_LOSS], --use-dft-loss [USE_DFT_LOSS]\n",
      "                        Whether to use the DFT loss. (default: False)\n",
      "  --freeze_vision_tower [FREEZE_VISION_TOWER], --freeze-vision-tower [FREEZE_VISION_TOWER]\n",
      "                        Whether ot not to freeze the vision tower in MLLM\n",
      "                        training. (default: True)\n",
      "  --no_freeze_vision_tower, --no-freeze-vision-tower\n",
      "                        Whether ot not to freeze the vision tower in MLLM\n",
      "                        training. (default: False)\n",
      "  --freeze_multi_modal_projector [FREEZE_MULTI_MODAL_PROJECTOR], --freeze-multi-modal-projector [FREEZE_MULTI_MODAL_PROJECTOR]\n",
      "                        Whether or not to freeze the multi modal projector in\n",
      "                        MLLM training. (default: True)\n",
      "  --no_freeze_multi_modal_projector, --no-freeze-multi-modal-projector\n",
      "                        Whether or not to freeze the multi modal projector in\n",
      "                        MLLM training. (default: False)\n",
      "  --freeze_language_model [FREEZE_LANGUAGE_MODEL], --freeze-language-model [FREEZE_LANGUAGE_MODEL]\n",
      "                        Whether or not to freeze the language model in MLLM\n",
      "                        training. (default: False)\n",
      "  --compute_accuracy [COMPUTE_ACCURACY], --compute-accuracy [COMPUTE_ACCURACY]\n",
      "                        Whether or not to compute the token-level accuracy at\n",
      "                        evaluation. (default: False)\n",
      "  --disable_shuffling [DISABLE_SHUFFLING], --disable-shuffling [DISABLE_SHUFFLING]\n",
      "                        Whether or not to disable the shuffling of the\n",
      "                        training set. (default: False)\n",
      "  --early_stopping_steps EARLY_STOPPING_STEPS, --early-stopping-steps EARLY_STOPPING_STEPS\n",
      "                        Number of steps to stop training if the\n",
      "                        `metric_for_best_model` does not improve. (default:\n",
      "                        None)\n",
      "  --plot_loss [PLOT_LOSS], --plot-loss [PLOT_LOSS]\n",
      "                        Whether or not to save the training loss curves.\n",
      "                        (default: False)\n",
      "  --include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND], --include-effective-tokens-per-second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]\n",
      "                        Whether or not to compute effective tokens per second.\n",
      "                        (default: False)\n",
      "  --do_sample [DO_SAMPLE], --do-sample [DO_SAMPLE]\n",
      "                        Whether or not to use sampling, use greedy decoding\n",
      "                        otherwise. (default: True)\n",
      "  --no_do_sample, --no-do-sample\n",
      "                        Whether or not to use sampling, use greedy decoding\n",
      "                        otherwise. (default: False)\n",
      "  --temperature TEMPERATURE\n",
      "                        The value used to modulate the next token\n",
      "                        probabilities. (default: 0.95)\n",
      "  --top_p TOP_P, --top-p TOP_P\n",
      "                        The smallest set of most probable tokens with\n",
      "                        probabilities that add up to top_p or higher are kept.\n",
      "                        (default: 0.7)\n",
      "  --top_k TOP_K, --top-k TOP_K\n",
      "                        The number of highest probability vocabulary tokens to\n",
      "                        keep for top-k filtering. (default: 50)\n",
      "  --num_beams NUM_BEAMS, --num-beams NUM_BEAMS\n",
      "                        Number of beams for beam search. 1 means no beam\n",
      "                        search. (default: 1)\n",
      "  --max_length MAX_LENGTH, --max-length MAX_LENGTH\n",
      "                        The maximum length the generated tokens can have. It\n",
      "                        can be overridden by max_new_tokens. (default: 1024)\n",
      "  --max_new_tokens MAX_NEW_TOKENS, --max-new-tokens MAX_NEW_TOKENS\n",
      "                        The maximum numbers of tokens to generate, ignoring\n",
      "                        the number of tokens in the prompt. (default: 1024)\n",
      "  --repetition_penalty REPETITION_PENALTY, --repetition-penalty REPETITION_PENALTY\n",
      "                        The parameter for repetition penalty. 1.0 means no\n",
      "                        penalty. (default: 1.0)\n",
      "  --length_penalty LENGTH_PENALTY, --length-penalty LENGTH_PENALTY\n",
      "                        Exponential penalty to the length that is used with\n",
      "                        beam-based generation. (default: 1.0)\n",
      "  --skip_special_tokens [SKIP_SPECIAL_TOKENS], --skip-special-tokens [SKIP_SPECIAL_TOKENS]\n",
      "                        Whether or not to remove special tokens in the\n",
      "                        decoding. (default: True)\n",
      "  --no_skip_special_tokens, --no-skip-special-tokens\n",
      "                        Whether or not to remove special tokens in the\n",
      "                        decoding. (default: False)\n",
      "\n",
      "Got unknown args, potentially deprecated arguments: ['--checkpoint_dir', '/home/byh/gpt/models/sft/DeepSeek-R1-Distill-Qwen-7B']\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/api.py\", line 33, in <module>\n",
      "    main()\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/api.py\", line 24, in main\n",
      "    chat_model = ChatModel()\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 48, in __init__\n",
      "    model_args, data_args, finetuning_args, generating_args = get_infer_args(args)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 435, in get_infer_args\n",
      "    model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 203, in _parse_infer_args\n",
      "    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)\n",
      "  File \"/home/byh/gpt/LLaMA-Factory/src/llamafactory/hparams/parser.py\", line 86, in _parse_args\n",
      "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {unknown_args}\")\n",
      "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--checkpoint_dir', '/home/byh/gpt/models/sft/DeepSeek-R1-Distill-Qwen-7B']\n"
     ]
    }
   ],
   "source": [
    "!python /home/byh/gpt/LLaMA-Factory/src/api.py \\\n",
    "    --model_name_or_path /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "    --checkpoint_dir /home/byh/gpt/models/sft/DeepSeek-R1-Distill-Qwen-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e2821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-19 11:39:23 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m INFO 10-19 11:39:24 [api_server.py:1839] vLLM API server version 0.11.0\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m INFO 10-19 11:39:24 [utils.py:233] non-default args: {'host': '0.0.0.0', 'model': '/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', 'served_model_name': ['qwen3'], 'block_size': 16}\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m INFO 10-19 11:39:24 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m INFO 10-19 11:39:24 [model.py:1510] Using max model len 4096\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m INFO 10-19 11:39:26 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 10-19 11:39:27 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:29 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:29 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:31 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m WARNING 10-19 11:39:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:31 [gpu_model_runner.py:2602] Starting to load model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:31 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:31 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.94s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.52s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.73s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:37 [default_loader.py:267] Loading weights took 5.48 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:37 [gpu_model_runner.py:2653] Model loading took 12.8726 GiB and 5.579782 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:42 [backends.py:548] Using cache directory: /home/byh/.cache/vllm/torch_compile_cache/31c6ca6bbd/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:42 [backends.py:559] Dynamo bytecode transform time: 4.83 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:43 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.853 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:43 [monitor.py:34] torch.compile takes 4.83 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:44 [gpu_worker.py:298] Available KV cache memory: 7.42 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:44 [kv_cache_utils.py:1087] GPU KV cache size: 16,192 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:44 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 3.95x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:02<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:01<00:00, 32.62it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m INFO 10-19 11:39:48 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 2.26 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3217, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 100, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 180, in sample\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     random_sampled, processed_logprobs = self.topk_topp_sampler(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 90, in forward_native\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     logits = self.apply_top_k_top_p(logits, k, p)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 183, in apply_top_k_top_p\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 61.12 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, with 59.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 207, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in initialize_from_config\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 413, in compile_or_warm_up_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     self.model_runner._dummy_sampler_run(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3221, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708]     raise RuntimeError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m ERROR 10-19 11:39:48 [core.py:708] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3217, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 100, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     sampled, processed_logprobs = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/sampler.py\", line 180, in sample\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     random_sampled, processed_logprobs = self.topk_topp_sampler(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 90, in forward_native\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     logits = self.apply_top_k_top_p(logits, k, p)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 183, in apply_top_k_top_p\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 61.12 MiB is free. Including non-PyTorch memory, this process has 23.52 GiB memory in use. Of the allocated memory 20.86 GiB is allocated by PyTorch, with 59.88 MiB allocated in private pools (e.g., CUDA Graphs), and 21.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 207, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 75, in initialize_from_config\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 413, in compile_or_warm_up_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     self.model_runner._dummy_sampler_run(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3221, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m     raise RuntimeError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2489672)\u001b[0;0m RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n",
      "[rank0]:[W1019 11:39:49.137448046 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return _run_code(code, main_globals, None,\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     exec(code, run_globals)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 1953, in <module>\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     uvloop.run(run_server(args))\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/uvloop/__init__.py\", line 69, in run\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return loop.run_until_complete(wrapper())\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/uvloop/__init__.py\", line 48, in wrapper\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return await main\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 1884, in run_server\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 1902, in run_server_worker\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     async with build_async_engine_client(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return await anext(self.gen)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 180, in build_async_engine_client\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     async with build_async_engine_client_from_engine_args(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/contextlib.py\", line 199, in __aenter__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return await anext(self.gen)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 225, in build_async_engine_client_from_engine_args\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     async_llm = AsyncLLM.from_vllm_config(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 1572, in inner\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return fn(*args, **kwargs)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 207, in from_vllm_config\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return cls(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py\", line 134, in __init__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 102, in make_async_mp_client\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     return AsyncMPClient(*client_args)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 769, in __init__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     super().__init__(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\", line 448, in __init__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     with launch_core_engines(vllm_config, executor_class,\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     next(self.gen)\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/utils.py\", line 732, in launch_core_engines\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     wait_for_engine_startup(\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m   File \"/home/byh/miniconda3/envs/deepseek/lib/python3.10/site-packages/vllm/v1/engine/utils.py\", line 785, in wait_for_engine_startup\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m     raise RuntimeError(\"Engine core initialization failed. \"\n",
      "\u001b[1;36m(APIServer pid=2489522)\u001b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n"
     ]
    }
   ],
   "source": [
    "# run vlm\n",
    "!python3 -m vllm.entrypoints.openai.api_server --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --port 8000 --served-model-name qwen3 --block-size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1012270d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您遇到的是显存不足的问题。有几种方法可以解决：\\n\\n## 方法1：调整推理参数（推荐）\\n\\n```bash\\npython3 -m vllm.entrypoints.openai.api_server \\\\\\n    --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B \\\\\\n    --host 0.0.0.0 \\\\\\n    --port 8000 \\\\\\n    --served-model-name qwen3 \\\\\\n    --max-model-len 4096 \\\\\\n    --gpu-memory-utilization 0.8 \\\\\\n    --swap-space 4\\n```\\n\\n关键参数说明：\\n- `--max-model-len 4096`：限制最大序列长度\\n- `--gpu-memory-utilization 0.8`：限制GPU内存使用率\\n- `--swap-space 4`：设置交换空间（GB）\\n\\n## 方法2：使用量化（如果显存严重不足）\\n\\n```bash\\npython3 -m vllm.entrypoints.openai.api_server \\\\\\n    --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B \\\\\\n    --host 0.0.0.0 \\\\\\n    --port 8000 \\\\\\n    --served-model-name qwen3 \\\\\\n    --quantization awq \\\\\\n    --max-model-len 2048\\n```\\n\\n## 方法3：检查并优化系统\\n\\n1. **检查可用显存**：\\n```bash\\nnvidia-smi\\n```\\n\\n2. **如果显存确实很小**，考虑使用更小的模型或CPU推理：\\n```bash\\npython3 -m vllm.entrypoints.openai.api_server \\\\\\n    --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B \\\\\\n    --host 0.0.0.0 \\\\\\n    --port 8000 \\\\\\n    --served-model-name qwen3 \\\\\\n    --device cpu\\n```\\n\\n## 方法4：分批加载\\n\\n如果模型太大，可以尝试：\\n```bash\\npython3 -m vllm.entrypoints.openai.api_server \\\\\\n    --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B \\\\\\n    --host 0.0.0.0 \\\\\\n    --port 8000 \\\\\\n    --served-model-name qwen3 \\\\\\n    --block-size 16 \\\\\\n    --max-parallel-loading 1\\n```\\n\\n## 建议的执行顺序：\\n\\n1. 先用**方法1**调整参数\\n2. 如果还是OOM，尝试**方法2**量化\\n3. 如果显存确实很小（<8GB），考虑使用CPU或换更小的模型\\n\\n您当前的GPU显存是多少？这样我可以给出更精确的建议。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "import os\n",
    "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-b6118335f5c34520abffbe6fa324257a\" #my key\n",
    "\n",
    "# --- 设置 DeepSeek API Key ---\n",
    "api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set the environment variable DEEPSEEK_API_KEY\")\n",
    "\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
    "        # --- 调用 DeepSeek GPT ---\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"我使用vllm显存超出了!python3 -m vllm.entrypoints.openai.api_server --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --port 8000 --served-model-name qwen3 \"}\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "gpt_output = response.choices[0].message.content\n",
    "gpt_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a167f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e530ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-19 13:22:56 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:22:57 [api_server.py:1839] vLLM API server version 0.11.0\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:22:57 [utils.py:233] non-default args: {'host': '0.0.0.0', 'model': '/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', 'served_model_name': ['qwen3'], 'max_num_seqs': 64}\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:22:57 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:22:57 [model.py:1510] Using max model len 4096\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:22:58 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 10-19 13:23:00 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:01 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:01 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='/home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=qwen3, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:02 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m WARNING 10-19 13:23:02 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:02 [gpu_model_runner.py:2602] Starting to load model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:02 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:02 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.66s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.34s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.54s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:08 [default_loader.py:267] Loading weights took 5.09 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:08 [gpu_model_runner.py:2653] Model loading took 12.8726 GiB and 5.199408 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:10 [backends.py:548] Using cache directory: /home/byh/.cache/vllm/torch_compile_cache/31c6ca6bbd/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:10 [backends.py:559] Dynamo bytecode transform time: 2.44 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:11 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.760 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:12 [monitor.py:34] torch.compile takes 2.44 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:13 [gpu_worker.py:298] Available KV cache memory: 8.04 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:13 [kv_cache_utils.py:1087] GPU KV cache size: 17,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:13 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 4.29x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 19/19 [00:00<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 11/11 [00:00<00:00, 45.99it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:14 [gpu_model_runner.py:3480] Graph capturing finished in 1 secs, took 0.71 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2679547)\u001b[0;0m INFO 10-19 13:23:14 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.45 seconds\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1097\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [api_server.py:1634] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m WARNING 10-19 13:23:15 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.7, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:34] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:23:15 [launcher.py:42] Route: /metrics, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m2679432\u001b[0m]\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m INFO 10-19 13:26:08 [launcher.py:99] Shutting down FastAPI HTTP server.\n",
      "[rank0]:[W1019 13:26:08.404735864 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "^C\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[1;36m(APIServer pid=2679432)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# run vlm\n",
    "!python3 -m vllm.entrypoints.openai.api_server --model /home/byh/gpt/models/DeepSeek-R1-Distill-Qwen-7B --host 0.0.0.0 --port 8000 --served-model-name qwen3  --max-num-seqs 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f7ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
